[
  {
    "objectID": "workbooks.html",
    "href": "workbooks.html",
    "title": "Workbooks",
    "section": "",
    "text": "Published Workbooks\n\nProject 0\n\nProject 1\n\nProject 2\n\nProject 3\nProject 4\nProject 5\n\nProject 6\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Workbooks",
      "Course Workbooks"
    ]
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "About this site\n\n\n\n Back to top"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Course Setup",
    "section": "",
    "text": "How to get your computer setup?\nThe left navigation bar is sequenced for the setup process:\nIf you want to use a AWS Cloud Virtual Machine (VM) instead of your laptop and install Python and VS Code on that VM, start here:\n\nCreate an AWS EC2 VM (optional)\n\nThis course uses Slack as our main tool for communication and collaboration. You will need to install Slack on your laptop and mobile device. You will also need to create a Slack account using your BYU-Idaho email. If you have an existing account, you can either change your email to the BYU-Idaho email or you can create a new account.\n\nInstall Slack on your laptop:\n\nWindows\n\nMac\n\n\nInstall Slack on your Cell:\n\nAndroid\n\niPhone\n\nCreate a Slack Account using your BYU-Idaho email:\n\nSlack\n\n\nIf you want to use Python and VS code on your own laptop, start here:\n\nInstall Python\nInstall Python Libraries\n\nInstall VS Code\n\nInstall Quarto\n\nInstall Git & GitHub  \n\nYou will need to use SQL in Week 6:\n\nSQL setup and test\n\nInterested in using Copilot all Semester? &gt; Copilot is a micro-assistant that helps you write better code. It is a VS Code extension that uses AI built off Chat GPT-4. With your student GitHub account, you can use Copilot for free. It helps you write code faster and with fewer errors. It is not perfect, but it is a great tool to help you learn to code.\n\nInstall GitHub Copilot (optional)\n\nPrefer an Open Source AI tool try Llama? &gt; Llama is an AI-powered code editor that helps you write better code. It is a VS Code extension that uses AI built off Chat GPT-4. With your student GitHub account, you can use Llama for free. It helps you write code faster and with fewer errors. It is not perfect either, but it is a great tool to help you learn to code.\n\nInstall Llama (optional)\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Course Material",
    "section": "",
    "text": "We will be relying on a few resources for this course. You will find the pertinant readings attached to each of the projects. Those readings will be culled from:\n\nPython for Data Science: A port of R for Data Science using the Python packages pandas and Altair.\npandas User Guide\nAltair User Guide\nPlotly Express\nscikit-learn learn User Guide\nscikit-learn tutorials\nPython Data Science Handbook\nA Whirlwind Tour of Python\nSQL\n\nWes McKinney’s pandas code for his book Python for Data Analysis is a useful reference as well: https://github.com/wesm/pydata-book\n\n\n\n Back to top",
    "crumbs": [
      "Materials",
      "Course Materials"
    ]
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "“What do you mean by data science programming?”\nMost likely, you have had 1-2 courses of programming before you have taken DS 250. Unlike traditional computer science courses, DS 250 uses Python in an interactive mode instead of building programs. The data provider usually has some big questions that need answering; However, there are hundreds of little issues and responses along the way. We use programming to facilitate this investigation.\nThere are similarities with User Experience Designers. In our case, we don’t get to ask users about their experience. We use programming to ask data about its background, and each data set has its own history. We want our analysis to mold to that experience. You can think of data science programming like a first date with your data. You can’t write one long program nieve of the issues and nuances each living data set provides.\n\n\n\n“How does DS 250 compare to DS 350?”\nThe two courses have similarities. You could think of DS 250 as an introduction to data wrangling and visualization. Both classes use real-world data and are built around data science projects. There are some critical differences between the two courses.\n\nIn this course, we use Python, and DS 350 uses R.\nWe are introducing the principles of data science programming in DS 250.\nThe course is only 2-credits.\nDS 250 is intended to introduce visualization, wrangling, and modeling.\n\n\n\n\nfaq “How does DS 250 prepare me for DS 350 and CSE 450?”\nYou will be comfortable with interactive programming and have an introduction to the principles of data formats for data science applications. You will be introduced to principles related to machine learning, data wrangling, and data visualization.\n\n\n\n“What programming languages do we use in this course?”\nThe course is done using Python. We focus on the pandas and Altair packages.\n\n\n\n“What are the prerequisites for this course?”\nUsing the new courses at BYU-I, the prerequisite is CSE 110. However, if you have experience programming from other classes, you most likely are prepared for this course.\n\n\n\n“Why Python instead of R?”\nThe computer science and software engineering programs at BYU-I use Python as their foundational courses. The standard student will have some experience with Python before DS 250. Python is an essential programming language for data scientists, and we already have DS350, which is taught in R.\n\n\n\n“What is pandas?”\npandas is the foundational data science package in Python. If you are using tabular data you will be in pandas.\n\n\n\n“Why are we using Altair instead of Seaborn or Matplotlib?”\nMatplotlib was the first visualization package to gain a following in Python. Seaborn is built on top of Matplotlib. Many data scientists use both in their work—neither leverage the grammar of graphics as developed by Leland Wilkinson. Altair is built on Vega-Lite, which uses the Vega visualization grammar. It is declarative and actively developed. We expect that it will become the predominant visualization package in Python https://youtu.be/FytuB8nFHPQ and https://youtu.be/vTingdk_pVM.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Workbooks/wb6.html",
    "href": "Workbooks/wb6.html",
    "title": "Project 6 Workbook",
    "section": "",
    "text": "Project 6 WorkBook\nUnder Construction\n\n\n\n\n Back to top",
    "crumbs": [
      "Workbooks",
      "Project 6"
    ]
  },
  {
    "objectID": "Workbooks/wb4.html",
    "href": "Workbooks/wb4.html",
    "title": "Project 4 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack.",
    "crumbs": [
      "Workbooks",
      "Project 4"
    ]
  },
  {
    "objectID": "Workbooks/wb4.html#tutoring-lab-info",
    "href": "Workbooks/wb4.html#tutoring-lab-info",
    "title": "Project 4 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack.",
    "crumbs": [
      "Workbooks",
      "Project 4"
    ]
  },
  {
    "objectID": "Workbooks/wb2.html",
    "href": "Workbooks/wb2.html",
    "title": "Project 2 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack.",
    "crumbs": [
      "Workbooks",
      "Project 2"
    ]
  },
  {
    "objectID": "Workbooks/wb2.html#tutoring-lab-info",
    "href": "Workbooks/wb2.html#tutoring-lab-info",
    "title": "Project 2 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack.",
    "crumbs": [
      "Workbooks",
      "Project 2"
    ]
  },
  {
    "objectID": "Workbooks/wb2.html#plotly-chart-structure",
    "href": "Workbooks/wb2.html#plotly-chart-structure",
    "title": "Project 2 Workbook",
    "section": "Plotly Chart Structure",
    "text": "Plotly Chart Structure\n\n\nShow the code\nfig = px.scatter(data, x='displ', y='hwy')\nfig.show()",
    "crumbs": [
      "Workbooks",
      "Project 2"
    ]
  },
  {
    "objectID": "Workbooks/wb2.html#size-of-chart",
    "href": "Workbooks/wb2.html#size-of-chart",
    "title": "Project 2 Workbook",
    "section": "Size of Chart",
    "text": "Size of Chart\nWidth and Height\n\n\nShow the code\nfig = px.scatter(data, x='displ', y='hwy')\nfig.update_layout(width=600, height=600)\n\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\n\nWidth and Height Explanation\n\n\n\n\n\nThe width and height parameters in the update_layout method are used to set the width and height of the plot in pixels, respectively. In this example, the width is set to 600 pixels, and the height is also set to 600 pixels. Adjust these values according to your desired dimensions for the scatter plot.",
    "crumbs": [
      "Workbooks",
      "Project 2"
    ]
  },
  {
    "objectID": "Workbooks/wb2.html#title-and-subtitle",
    "href": "Workbooks/wb2.html#title-and-subtitle",
    "title": "Project 2 Workbook",
    "section": "Title and Subtitle",
    "text": "Title and Subtitle\nTitle\n\n\nShow the code\nfig = px.bar(data, x='cty', y='hwy')\nfig.update_layout(title_text=\"Bar Chart Example\")\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\n\nTitle Explanation\n\n\n\n\n\nThe title_text parameter in the update_layout method is used to set the title of the plot. In this example, the title is set to “Bar Chart Example”. You can customize the title by changing the value assigned to title_text to better describe the content or purpose of your bar chart.\n\n\n\nTitle and subtitle - Title w/ Subtitle 1\n\n\nShow the code\nfig = px.bar(data, x='cty', y='hwy')\n\nfig.update_layout(\n    title_text=\"City vs Highway MPG Bar Chart\",\n    title_font=dict(color=\"red\"),\n    title_font_size=18,\n    title_y=0.95,\n    title_x=0.5\n)\n\nfig.add_annotation(\n    text=\"Your Annotation Text\",\n    font=dict(color=\"blue\"),  # Choose your desired font color\n    x=0.5,\n    y=0.9,\n    showarrow=False\n)\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\n\nTitle and subtitle - Title w/ Subtitle 1 Explanation\n\n\n\n\n\nThe title_text parameter in the update_layout method is used to set the main title of the plot, and additional parameters like title_font, title_font_size, title_y, and title_x are used to customize the appearance and position of the title.\nThe add_annotation method is used to add an annotation or subtitle to the plot. In this example, it adds a blue text annotation with the content “Your Annotation Text” at a specified position (x=0.5, y=0.9) relative to the plot.\nAdjust the values of these parameters to customize the appearance and position of the title and annotation according to your preferences.\n\n\n\nTitle and subtitle - Title w/ Subtitle 2\n\n\nShow the code\nfig = px.bar(data, x='cty', y='hwy')\n\nfig.update_layout(\n    title_text=\"City vs Highway MPG Bar Chart\",\n    title_font_size=18,\n    title_y=0.95,\n    title_x=0.5\n)\n\nfig.add_annotation(\n    text=\"Your Annotation Text\",\n    x=0.5,\n    y=0.9,\n    showarrow=False\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\n\nTitle and subtitle - Title w/ Subtitle 2 Explanation\n\n\n\n\n\nThe title_text parameter in the update_layout method is used to set the main title of the plot, and additional parameters like title_font_size, title_y, and title_x are used to customize the appearance and position of the title.\nThe add_annotation method is used to add an annotation or subtitle to the plot. In this example, it adds a text annotation with the content “Your Annotation Text” at a specified position (x=0.5, y=0.9) relative to the plot.\nAdjust the values of these parameters to customize the appearance and position of the title and annotation according to your preferences.\n\n\n\nGroup Variable\n\n\nShow the code\nfig = px.bar(data, x='manufacturer', y=['cty', 'hwy'], barmode='group')\n\nfig.update_layout(\n    xaxis_title=\"Manufacturer\",\n    yaxis_title=\"Mileage\",\n    title_text=\"Average City and Highway Mileage by Manufacturer\"\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\n\nGroup Variable Explanation\n\n\n\n\n\nThe x parameter in the px.bar function is set to ‘manufacturer’, which means the bars will be grouped by the ‘manufacturer’ variable on the x-axis. The y parameter is set to a list [‘cty’, ‘hwy’], indicating that two sets of bars will be plotted for each manufacturer, one for ‘cty’ and another for ‘hwy’.\nThe barmode='group' parameter ensures that the bars are grouped for each ‘manufacturer’.\nThe update_layout method is used to set the titles for the x-axis (xaxis_title), y-axis (yaxis_title), and the overall plot (title_text). In this example, the plot represents the average city and highway mileage by manufacturer.\n\n\n\nAxis formatting - Axis Scale removing Zero\n\n\nShow the code\nfig = px.bar(data, x='manufacturer', y=['cty', 'hwy'], barmode='group')\n\nfig.update_layout(\n    xaxis_title=\"Manufacturer\",\n    yaxis_title=\"Mileage\",\n    title_text=\"Average City and Highway Mileage by Manufacturer\",\n    xaxis=dict(showline=True, showgrid=False),\n    yaxis=dict(zeroline=False, showline=True, showgrid=False),\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\n\nAxis formatting - Axis Scale removing Zero Explanation\n\n\n\n\n\nThe xaxis_title and yaxis_title parameters in the update_layout method are used to set the titles for the x-axis and y-axis, respectively.\nThe xaxis and yaxis dictionaries in the update_layout method provide additional formatting options for the x-axis and y-axis. In this example:\nxaxis=dict(showline=True, showgrid=False) ensures that the x-axis has a visible line but no grid lines. yaxis=dict(zeroline=False, showline=True, showgrid=False) ensures that the y-axis has no zero line (zeroline=False), a visible line, and no grid lines. These settings help customize the appearance of the plot by controlling the visibility of axis lines and grid lines.\n\n\n\nAxis formatting - Axis Domain Sizing\n\n\nShow the code\nfig = px.bar(data, x='manufacturer', y=['cty', 'hwy'], barmode='group')\n\nfig.update_layout(\n    xaxis_title=\"Manufacturer\",\n    yaxis_title=\"Mileage\",\n    title_text=\"Average City and Highway Mileage by Manufacturer\",\n    xaxis=dict(domain=[0.1, 0.9]),  # Adjust the domain as needed\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\n\nAxis formatting - Axis Domain Sizing Explanation\n\n\n\n\n\nThe xaxis_title and yaxis_title parameters in the update_layout method are used to set the titles for the x-axis and y-axis, respectively.\nThe xaxis dictionary in the update_layout method includes the domain parameter, which is set to [0.1, 0.9]. This parameter controls the size of the x-axis domain, determining the portion of the total width of the plot that the x-axis occupies. In this example, the x-axis is set to span from 10% to 90% of the total width.\nAdjust the values of the domain parameter as needed to control the sizing of the x-axis in your plot.\n\n\n\nReference marks - Verticle Reference Line with Color\n\n\nShow the code\nfig = px.bar(data, x='manufacturer', y=['cty', 'hwy'], barmode='group')\n\nfig.update_layout(\n    xaxis_title=\"Manufacturer\",\n    yaxis_title=\"Mileage\",\n    title_text=\"Average City and Highway Mileage by Manufacturer\"\n)\n\nfig.add_shape(\n    dict(\n        type=\"line\",\n        x0=\"Your_X_Value\",  # Specify the x-coordinate of the line\n        x1=\"Your_X_Value\",  # Specify the x-coordinate of the line\n        y0=0,\n        y1=1,\n        line=dict(color=\"red\"),  # Specify the color of the line\n    )\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\n\nReference marks - Verticle Reference Line with Color Explanation\n\n\n\n\n\nThe add_shape method is used to add a reference line to the plot. In this example, a vertical reference line is added to the x-axis at the specified x-coordinate value.\nThe type=\"line\" parameter specifies that the added shape is a line. The x0 and x1 parameters are set to “Your_X_Value” to specify the x-coordinates where the line starts and ends. Replace “Your_X_Value” with the actual x-coordinate value where you want the reference line.\nThe y0 and y1 parameters set the starting and ending points on the y-axis. In this example, they are set to 0 and 1, respectively.\nThe line dictionary inside the shape specifies the attributes of the line, such as the color. In this case, the line color is set to red. Adjust the values as needed to customize the appearance of the reference line.\n\n\n\nReference marks - Verticle Reference Line with Text\n\n\nShow the code\nfig = px.bar(data, x='manufacturer', y=['cty', 'hwy'], barmode='group')\n\nfig.update_layout(\n    xaxis_title=\"Manufacturer\",\n    yaxis_title=\"Mileage\",\n    title_text=\"Average City and Highway Mileage by Manufacturer\"\n)\n\nfig.add_shape(\n    dict(\n        type=\"line\",\n        x0=\"Your_X_Value\",  # Specify the x-coordinate of the line\n        x1=\"Your_X_Value\",  # Specify the x-coordinate of the line\n        y0=0,\n        y1=1,\n        line=dict(color=\"red\"),  # Specify the color of the line\n    )\n)\n\nfig.add_annotation(\n    text=\"Your Text\",\n    x=\"Your_X_Value\",  # Specify the x-coordinate of the text\n    y=500,  # Adjust the y-coordinate of the text as needed\n    showarrow=False\n)\n\nfig.show()\n\n\n                                                \n\n\n\n\n\n\n\n\nReference marks - Verticle Reference Line with Text Explanation\n\n\n\n\n\nThe add_shape method is used to add a reference line to the plot. In this example, a vertical reference line is added to the x-axis at the specified x-coordinate value.\nThe type=\"line\" parameter specifies that the added shape is a line. The x0 and x1 parameters are set to “Your_X_Value” to specify the x-coordinates where the line starts and ends. Replace “Your_X_Value” with the actual x-coordinate value where you want the reference line.\nThe y0 and y1 parameters set the starting and ending points on the y-axis. In this example, they are set to 0 and 1, respectively.\nThe line dictionary inside the shape specifies the attributes of the line, such as the color. In this case, the line color is set to red.\nThe add_annotation method is used to add text annotation to the plot. The text parameter is set to “Your Text”, and the x and y parameters specify the coordinates where the text will be placed. Adjust the values of these parameters as needed to customize the appearance of the reference line and text.",
    "crumbs": [
      "Workbooks",
      "Project 2"
    ]
  },
  {
    "objectID": "Workbooks/wb1.html",
    "href": "Workbooks/wb1.html",
    "title": "Project 1 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack.",
    "crumbs": [
      "Workbooks",
      "Project 1"
    ]
  },
  {
    "objectID": "Workbooks/wb1.html#tutoring-lab-info",
    "href": "Workbooks/wb1.html#tutoring-lab-info",
    "title": "Project 1 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack.",
    "crumbs": [
      "Workbooks",
      "Project 1"
    ]
  },
  {
    "objectID": "Workbooks/wb1.html#text-basics",
    "href": "Workbooks/wb1.html#text-basics",
    "title": "Project 1 Workbook",
    "section": "Text Basics",
    "text": "Text Basics\n\nHorizontal Lines\nAdd horizontal lines with either three ---, ***, or ___ But you also need blank lines above and below them\n\n\n\n\n\n\nExpand To See The Results\n\n\n\n\n\n\n\n\n\n\n\n\nHeaders\n# Level 1 Header\n## Level 2 Header\n### Level 3 Header\n#### Level 4 Header\n##### Level 5 Header\n###### Level 6 Header\nNote: only top 3 Levels of Headers will automatically generate a table of contents. Also Level 2 will automatically add a line underneath it.\n\n\n\n\n\n\nExpand To See The Results\n\n\n\n\n\nLevel 1 Header\n\nLevel 2 Header\n\nLevel 3 Header\n\nLevel 4 Header\n\nLevel 5 Header\n\nLevel 6 Header\n\n\n\n\n\n\n\n\n\n\nItalics and Bold\n_italics_ use one `_`\nyou can also use _mid_ sentence\n\n__bold__ use two `__`\nyou can also use __mid__ sentence\n\n\n\n\n\n\nExpand To See The Results\n\n\n\n\n\nitalics use one _ you can also use mid sentence\nbold use two __ you can also use mid sentence\n\n\n\n\n\nBullet Items\n- Bulleted items\n  - Indented bulleted items\n  - You can have as many as you want\n    - Really as many as you want\n      - I knew you wanted one more\n\n\n\n\n\n\nExpand To See The Results\n\n\n\n\n\n\nBulleted items\n\nIndented bulleted items\nYou can have as many as you want\n\nReally as many as you want\n\nI knew you wanted one more bullet\n\n\n\n\n\n\n\n\n\nNumbered Items\n1. Numbered items\n1. Numbered items continued\n1. Dont worry these will iterate\n1. Keep using 1. each time\n\n\n\n\n\n\nExpand To See The Results\n\n\n\n\n\n\nNumbered items\nNumbered items continued\nDont worry these will iterate\nKeep using 1. each time",
    "crumbs": [
      "Workbooks",
      "Project 1"
    ]
  },
  {
    "objectID": "Workbooks/wb1.html#level-2-header",
    "href": "Workbooks/wb1.html#level-2-header",
    "title": "Project 1 Workbook",
    "section": "Level 2 Header",
    "text": "Level 2 Header\n\nLevel 3 Header\n\nLevel 4 Header\n\nLevel 5 Header\n\nLevel 6 Header",
    "crumbs": [
      "Workbooks",
      "Project 1"
    ]
  },
  {
    "objectID": "Workbooks/wb1.html#pandas-dataframe-df",
    "href": "Workbooks/wb1.html#pandas-dataframe-df",
    "title": "Project 1 Workbook",
    "section": "Pandas DataFrame (df)",
    "text": "Pandas DataFrame (df)\n\n\n\n\n\n\nExpand To See Links to Chapter in the book Python for DataScience\n\n\n\n\n\n\nChapter on Pandas: DataFrames\n\n\n\n\nWhat is a pandas dataFrame? We can read the official documentation. I also like the video in this tutorial.\nUse the Import Packages and Load df for the Code that follows.",
    "crumbs": [
      "Workbooks",
      "Project 1"
    ]
  },
  {
    "objectID": "Workbooks/wb1.html#import-packages",
    "href": "Workbooks/wb1.html#import-packages",
    "title": "Project 1 Workbook",
    "section": "Import Packages",
    "text": "Import Packages\nimport `library` as `alias`\n\n\nImport Libraries\n\n#| label: libraries\n#| include: false\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\n\nfrom IPython.display import Markdown\nfrom IPython.display import display",
    "crumbs": [
      "Workbooks",
      "Project 1"
    ]
  },
  {
    "objectID": "Workbooks/wb1.html#load-data",
    "href": "Workbooks/wb1.html#load-data",
    "title": "Project 1 Workbook",
    "section": "Load Data",
    "text": "Load Data\ndf = pd.read_csv(`url` or `file_path`)\n\n\nLoad Data\n\n#| label: project data\n#| code-summary: Read and format project data\n# Include and execute your code here\nurl = \"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\"\ndf = pd.read_csv(url)\n\nData Frames come with attributes and built-in functions that can help us get a feel for our df.\nRun the code below one at a time (or use other functions of your choice) to explore the names df. What do you learn?\n\n\n.columns\n\ndf.columns\n\n\n\n.shape\n\ndf.shape\n\n\n\n.size\n\ndf.size\n\n\n\n.head()\n\ndf.head()\n\n\n\n.describe()\n\ndf.describe()",
    "crumbs": [
      "Workbooks",
      "Project 1"
    ]
  },
  {
    "objectID": "Workbooks/wb1.html#pandas-data-transformation",
    "href": "Workbooks/wb1.html#pandas-data-transformation",
    "title": "Project 1 Workbook",
    "section": "Pandas Data Transformation",
    "text": "Pandas Data Transformation\n\n\n\n\n\n\nExpand To See Links to Chapter in the book Python for DataScience\n\n\n\n\n\n\nChapter on Pandas: Transformations",
    "crumbs": [
      "Workbooks",
      "Project 1"
    ]
  },
  {
    "objectID": "Syllabus/syllabus.html",
    "href": "Syllabus/syllabus.html",
    "title": "DS 250 Syllabus",
    "section": "",
    "text": "Most people would sooner die than think, and most of them do.\n-Bertrand Russell-",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "Syllabus/syllabus.html#footnotes",
    "href": "Syllabus/syllabus.html#footnotes",
    "title": "DS 250 Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://medium.com/@nikhilbd/what-makes-a-good-data-scientist-engineer-a8b4d7948a86#.jr80wl98y. I suppose some of you are just taking this class because your degree says you can, and it fits in your schedule. If so, we should chat to make sure this is the right class for you.↩︎\nhttps://arxiv.org/ftp/arxiv/papers/1612/1612.07140.pdf. You will see this pattern in DS 350, DS 460, and Math 488. It will progressively get more realistic.↩︎\nWe do expect that this is not your first experience with Python and VS Code. If you have done other programming courses, you should be able to succeed in this course. If you have any questions, please ask.↩︎\nMaking the right checklists can be difficult. Bad checklists could fall in the following categories – vague and imprecise; too long; hard to use; impractical; too pedantic. Useful checklists are precise, efficient, easy to use and understand. This is the first time this course has been offered, so we will have to work together to ensure the requirements are reasonable.↩︎",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "Skill Builders/relational_data.html",
    "href": "Skill Builders/relational_data.html",
    "title": "SQL & Databases",
    "section": "",
    "text": "For this skill builder, we are exploring some important topics in relational databases. This exercise will require you to create SQL queries through python. You may want to at least scan the readings before beginning this task since this serves as an assessment of your understanding of the assigned readings.\nThis should be able to be finished within 75 minutes. Work through it on your own or in a group based in your professors instruction.",
    "crumbs": [
      "Skill Builders",
      "Project 3: SQL & Databases"
    ]
  },
  {
    "objectID": "Skill Builders/relational_data.html#skill-builder",
    "href": "Skill Builders/relational_data.html#skill-builder",
    "title": "SQL & Databases",
    "section": "",
    "text": "For this skill builder, we are exploring some important topics in relational databases. This exercise will require you to create SQL queries through python. You may want to at least scan the readings before beginning this task since this serves as an assessment of your understanding of the assigned readings.\nThis should be able to be finished within 75 minutes. Work through it on your own or in a group based in your professors instruction.",
    "crumbs": [
      "Skill Builders",
      "Project 3: SQL & Databases"
    ]
  },
  {
    "objectID": "Skill Builders/relational_data.html#before-you-start",
    "href": "Skill Builders/relational_data.html#before-you-start",
    "title": "SQL & Databases",
    "section": "Before you start",
    "text": "Before you start\nMake sure you have installed VS-code, pandas, and Plotly Express on your computer.\nAlso make sure you have gone through the tutorial on under course materials called SQL for Data Science: we assume that you have a connection to your data.\n\n\nExercise 1\n\nReadme file\nA database can consist of more than one table/data set. A relational database consists of tables/data sets that share columns. These shared columns then establish the relationship between the tables, thus the name relational database. The relations are sometimes not easily found and they require careful investigations.\nTo understand what is in a relational database, we can start with understanding the tables and the columns within.\nHere is a link to the readme file of the baseball database.\n\nWhat is the name of the table that records data about pitchers in the regular seasons?\n\n\nWhat do the HR and HBP columns mean in that table respectively?\n\n\n\n\n\nExcercise 2\n\nSELECT and FROM\nThe simplest SQL query is a query with SELECT and FROM. These are the keywords you will see again and again in SQL. Usually, when constructing a more complex query, it is easier to identify what goes into these two clauses first.\n\nCreate a query that shows all columns from the table you found in Exercise 1, save the dataframe in a variable “pitch”\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nresult = pd.read_sql_query(\n    'SELECT _______ FROM _______',\n    con)\n\nresults\n\n\n\n\n\n\n\nExcercise 3\n\nWHERE\nThe WHERE keyword allows us to filter down the table horizontally (fewer rows).\nIt goes after SELECT and FROM.\n\nUsing a SQL query, select all rows in the same table where HR is lesser than 10 and gs is greater than 25.\n\n\nFind out what the columns mean and explain your query in words\n\n\n\n\n\nExcercise 4\n\nORDER BY\nORDER BY sort the table you select by one or more columns and goes after WHERE\n\nUsing the same query in exercise 2, edit it so that the table is ordered by the year of the season (nearest to furthermost) and the player ID (alphabetically).\n\n\n\n\n\nExcercise 5\n\nJoins\nJoins are used when you wish to create a new table through two different tables. Keep in mind that you have to identify the relationship between two tables before you can correctly join them.\nJOIN goes between FROM and WHERE.\n\nIdentify the shared columns (keys) and join the table in exercise 2 with the salaries table, then filter the data so that it shows only pitchers in the year 1986.\n\nYou should get a dataframe with 306 rows.\n\n\n\n\nExercise 6\n\nGroup by\nGroup by is a keyword we use to lower the level of granularity of a table. Meaning we are combining rows into one by the given column(s).\nCreate a query that captures the number of pitchers the Washington Nationals used in each year, then sort the table by year\nYou should get a dataframe with 23 rows.",
    "crumbs": [
      "Skill Builders",
      "Project 3: SQL & Databases"
    ]
  },
  {
    "objectID": "Skill Builders/relational_data.html#because-youre-extra",
    "href": "Skill Builders/relational_data.html#because-youre-extra",
    "title": "SQL & Databases",
    "section": "Because You’re Extra",
    "text": "Because You’re Extra\n\nExercise 7\nResearch the order of operations for SQL and put the following keywords in that order.\n\nSELECT\nFROM\nJOIN\nWHERE\nHAVING\nORDER BY\nGROUP BY\nLIMIT\n\n\n\n\n\n\n\n\nAfter you have completed this skill builder with your team (or on your own) then compare your work to our script\n\n\n\n\n\nSee the script.",
    "crumbs": [
      "Skill Builders",
      "Project 3: SQL & Databases"
    ]
  },
  {
    "objectID": "Skill Builders/pandas_altair.html",
    "href": "Skill Builders/pandas_altair.html",
    "title": "Pandas and Altair",
    "section": "",
    "text": "For this skill builder, we are exploring some important functions in the package of pandas and Altair. DS programming requires a lot of data wrangling. Using the proper functions, we can create concise and comprehensive codes. You should be exposed to a few functions through the readings this week.\nYou may want to at least scan the readings before beginning this task since this serves as an assessment of your understanding of the assigned readings. This should be able to be finished within 60 minutes. You should work through it on your own or in a group based in your professors instruction."
  },
  {
    "objectID": "Skill Builders/pandas_altair.html#skill-builder",
    "href": "Skill Builders/pandas_altair.html#skill-builder",
    "title": "Pandas and Altair",
    "section": "",
    "text": "For this skill builder, we are exploring some important functions in the package of pandas and Altair. DS programming requires a lot of data wrangling. Using the proper functions, we can create concise and comprehensive codes. You should be exposed to a few functions through the readings this week.\nYou may want to at least scan the readings before beginning this task since this serves as an assessment of your understanding of the assigned readings. This should be able to be finished within 60 minutes. You should work through it on your own or in a group based in your professors instruction."
  },
  {
    "objectID": "Skill Builders/pandas_altair.html#data-import",
    "href": "Skill Builders/pandas_altair.html#data-import",
    "title": "Pandas and Altair",
    "section": "Data Import",
    "text": "Data Import\nRun the following code to import the data we need for this skill builder:\n\n# package import\nimport numpy as np\nimport pandas as pd\nimport altair as alt\n\n# data import\nurl = 'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/AER/Guns.csv'\ndf = pd.read_csv(url)\nMake sure the variable df is correctly assigned in your environment and finish the following exercises. You can read the documentation of the data on this page - https://vincentarelbundock.github.io/Rdatasets/doc/AER/Guns.html\n\n\nExercise 1\nOne of the first things we can do to a freshly imported data is to check its columns. This will help us understand the basic structure of the dataframe(table).\n\nUsing one line of code, select all the columns in dat, assign it to a variable called col_list.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nEvery dataframe has an attribute “columns”.\nAccessing this attribute will give you a list of all column names\n\n\n\nWe often want to know the dimension of a dataframe. How many columns are in the dataset? How many rows are in the dataset?\n\nUsing one line of code, show the number of columns and rows in df.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nEvery dataframe has an attribute “shape”.\nAccessing this attribute will give you the dimension of a datafarme\n\n\n\nNow run df.head(). It will print out the first 5 rows of data in df.\n\nJust from looking at the output, what column(s) seems to be redundant with the row number?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThere is one column that serves as nothing but a row counter, that columns is redundant.\n\n\n\n\n\n\nExercise 2\nAfter a brief investigation of the data, we will clean up the data. By cleaning up, we are trying to filter down df so this only holds data we need. We will first get rid of the extra column we found in the previous excercise.\n\nUsing one line of code, drop the redundant column using the variable col_list (created in excercise 1)\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse drop().\nUnderstand what “axis” is as a parameter of drop().\nYour function should looks like this:\ndf.drop([col_list[_]], axis = _)\nfill the “_“’s with the correct values and assign the output to df.\n\n\n\nDon’t forget to save the changes in df. Run df.head() to make sure the column is dropped in df.\n\n\n\nExercise 3\nWe have filtered df vertically by dropping a column. Now we will try to filter df horizontally, meaning we will get rid of some the rows.\nWe can do that by applying a condition to df. A condition is an expression that can be evaluated as True/False. For example, 8 &gt; 5 is an expression that evaluates to be True. This is trivial because 8 will always be greater than 5.\nRun the code below:\n\nwhat is the difference between exp1 and exp2?\n\nexp1 = 8 &gt; 5\nexp2 = df.violent &lt; 300\n\n\n\n\n\n\nHint\n\n\n\n\n\nTry type() on else variable OR calling else variable.\n\n\n\nRun ths code below:\n\nBy putting df.violent &lt; 300, and the violent column from df into a dataframe, what is the relationship between the two columns?\n\nexp = pd.DataFrame({\"df.violent &lt; 300\" : exp2,\n                    \"violent value from dat\" : df.violent})\n\nexp\n\n\n\n\n\n\nHint\n\n\n\n\n\nTry computing df.violent[n] &lt; 300 and (df.violent &lt; 300)[n] where n is less than the number of row. The two expressions will always be the same as long as n is less than the number of rows.\n\n\n\n\nUsing query()to filter down the df so that it only contains the data for idaho\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nquery() takes in expressions and filters down data.\n\n\n\nDon’t forget to save the changes in df. Run df.shape() to make sure the there are 23 rows and 13 columns.\n\n\n\nExercise 4\nBesides filtering, we can manipulate the data by adding new data to it. By adding a new column to the data, we assign a new value to each row.\n\nUsing assign(), create a new column that show the ratio between murder rate and violent rate.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse assign()\nYou see get the ratio by computing this code:\ndf.murder/df.violent\n\n\n\n\n\n\nExercise 5\n\nCreate a scatter plot that shows the relationship between murder rate and violent rate for the state of Idaho. Your chart should show murder rate as the x-axis, violent as the y-axis.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nCan you mimic this plot?\nhttps://altair-viz.github.io/gallery/scatter_tooltips.html"
  },
  {
    "objectID": "Skill Builders/pandas_altair.html#because-youre-extra",
    "href": "Skill Builders/pandas_altair.html#because-youre-extra",
    "title": "Pandas and Altair",
    "section": "Because You’re Extra",
    "text": "Because You’re Extra\n\nExercise 6\n\nUsing a line of code, filter down the data set so that it only shows the data in years between 1993 and 1997.\n\n\n\n\nExercise 7\n\nCreate a line chart that show prisoners numbers for the state of Idaho, Utah, and Oregon.\n\nYour chart should show year as the x-axis, prisoner as the y-axis, states as different colours, along with an appropriate title.\n\n\n\nExercise 8\n\nWithout using query(), finshed the data wrangling in question 2,5 and 6.\n\n\n\n\n\n\n\n\nAfter you have completed this skill builder with your team (or on your own) then compare your work to our script\n\n\n\n\n\nSee the script."
  },
  {
    "objectID": "Skill Builders/ml_sklearn.html#data",
    "href": "Skill Builders/ml_sklearn.html#data",
    "title": "Machine Learning",
    "section": "Data",
    "text": "Data\nLink to data",
    "crumbs": [
      "Skill Builders",
      "Project 4: Machine Learning"
    ]
  },
  {
    "objectID": "Skill Builders/ml_sklearn.html#intro-to-titanic-machine-learning-skill-builder",
    "href": "Skill Builders/ml_sklearn.html#intro-to-titanic-machine-learning-skill-builder",
    "title": "Machine Learning",
    "section": "Intro to Titanic Machine Learning Skill Builder",
    "text": "Intro to Titanic Machine Learning Skill Builder\nFor this skill builder, we’ll be putting our machine learning hats on. We’ll be creating a model that predicts whether a passenger survived. With machine learning, there is a lot of jargon! It can be quite overwhelming at times. This skill builder attempts to keep things basic and simple. With that being said, there are some terms that are important to understand. Let’s look at the first few rows of our dataset before proceeding with the definitions.\nThe titanic dataset will be used for examples of each definition.\n\n\n\n\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsiblings_spouses_aboard\nparents_children_aboard\nfare\n\n\n\n\n0\n3\n1\n22\n1\n0\n7.25\n\n\n1\n1\n0\n38\n1\n0\n71.2833\n\n\n1\n3\n0\n26\n0\n0\n7.925\n\n\n1\n1\n0\n35\n1\n0\n53.1\n\n\n0\n3\n1\n35\n0\n0\n8.05\n\n\n\n\nImportant Terms:\n\nfeatures: measurable property of the object you’re trying to predict. We use this information to predict our target of interest.\n\nExample: pclass, sex, age, siblings_spouses_aboard , parents_children_aboard, fare columns are all examples of different features.\nSynonyms: attributes, explanatory variables, independent variables, variables, X’s, covariates\n\ntarget: the feature that you are wanting to gain more insight into. The thing you are trying to predict.\n\nExample: in the titanic dataset our target is survived\nSynonyms: label, dependent variable, y\n\ntrain set: Usually 70% of the rows from the original dataset are randomly sampled to create this training data. It’s used by the algorithm, to determine, or learn, the optimal combinations of variables that will generate a good predictive model\n\nExample: Random sample of 70% of the original titanic dataset rows\nSynonyms: training data, train data, X_train, y_train\n\ntest set: Usually the remaining 30% of the rows in the original dataset are used to create this dataset. The testing data is a set of rows used only to assess the performance (i.e. generalization) of a model. To do this, the final model is used to predict classifications of examples in the test set. Those predictions are compared to the examples’ true classifications to assess the model’s accuracy.\n\nExample: Random sample of 30% of the original titanic dataset rows\nSynonyms: testing data, test data, X_test, y_test\n\nevaluation metrics: A statistic that tells you how well your predictions align with the actual values. Other words, tells you how good your model is.\n\nExample: Accuracy, Precision, Recall, MSE, MAE, Rsquared\nSynonyms: performance metric\n\n\nAgain, this is a very light and oversimplified treatment of machine learning. The purpose of this project is to help you understand the main concepts of ml and walk you through the process of building a machine learning model. A simplified work flow of a machine learning project is shown below. Spend some time getting familiar with this flow &mdash as you are about to code it… Exciting!\nNote in order to do this skill builder you will need to have scikit-learn installed on your machine. Run the following command in your terminal if you haven’t already.\npip install scikit-learn\n\n\n\nExercise 1\n\nImports and Loading in Data\n# Loading in packages\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Loading in data\ndata = pd.read_csv(___)\n\n\n\n\nExercise 2\nCreate a chart exploring the relationship between age and survived in the titanic dataset. A strip plot, density plot, or boxplot might be useful here. Below is an example of a density plot. Feel free to replicate this chart or create your own.\nThe purpose of making this chart is to explore the relationships between a feature and the target. We want to see if the feature contains predictive information about the target. This is a large part of machine learning called Exploratory Data Analysis that should never be skipped! Spend time getting to know your features and how they interact with other features and the target.\n\n\n\n\nExercise 3\nBuild a random forest model that is able to predict whether a passenger survived. This exercise is the bulk of the skill builder and contains several steps.\n\nStep 0: Split the data into X and y variables\nThe X variable will contain all your features\n# Removes the target and keeps all features\nX = data.drop(___, axis=1)  \nThe y variable will hold the target\n# Selects the target column\ny = data['___']  \n\n\nStep 1: Split data into train and test sets\nThe train_test_split function is useful for this task. Review the train_test_split function documentation\n# Splitting X and y variables into train and test sets using stratified sampling\nX_train, X_test, y_train, y_test = train_test_split(___, ___, test_size=0.3,\n                                                    random_state=24, stratify=y)\n\n\nStep 2: Train the model\nExplore the RandomForestClassifier documentation for the RandomForestClassifier. It’s not necessary to understand the inner workings of the Random Forest algorithm for this class - just learn the syntax of fitting the model.\n# Creating random forest object\nrf = RandomForestClassifier(random_state=24)  \n\n# Fit with the training data\nrf.fit(___, ___)  \n\n\nStep 3: Use test set to make predictions\n# Using the features in the test set to make predictions\ny_pred = rf.predict(___)  \n\n\nStep 4: Compare test set predictions to actual values. Calculate the accuracy.\n# Comparing predictions to actual values\naccuracy_score(___, ___)  \n\n\n\n\nExercise 4\nWhat is the most important feature in making predictions? Why do you think this is?\nCreate a table that shows the feature importances in descending order. The random forest classifier has a feature importances attribute. It can be accessed by rf.feature_importances_. The table should look something like this.\n\n\n\nfeature names\nimportances\n\n\n\n\nfare\n0.288051\n\n\nsex\n0.281853\n\n\nage\n0.266491\n\n\npclass\n0.0814224\n\n\nsiblings_spouses_aboard\n0.0475633\n\n\nparents_children_aboard\n0.034619\n\n\n\n\n\n\n\n\n\n\nAfter you have completed this skill builder with your team (or on your own) then compare your work to our script\n\n\n\n\n\nSee the script.",
    "crumbs": [
      "Skill Builders",
      "Project 4: Machine Learning"
    ]
  },
  {
    "objectID": "Skill Builders/introduction.html",
    "href": "Skill Builders/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "This should be able to be finished within 60 minutes. You should work through it on your own or in a group based in your professors instruction. This serves as an assessment of your understanding of the assigned readings.",
    "crumbs": [
      "Skill Builders",
      "Project 0: Introduction"
    ]
  },
  {
    "objectID": "Skill Builders/introduction.html#skill-builder",
    "href": "Skill Builders/introduction.html#skill-builder",
    "title": "Introduction",
    "section": "",
    "text": "This should be able to be finished within 60 minutes. You should work through it on your own or in a group based in your professors instruction. This serves as an assessment of your understanding of the assigned readings.",
    "crumbs": [
      "Skill Builders",
      "Project 0: Introduction"
    ]
  },
  {
    "objectID": "Skill Builders/introduction.html#before-you-start",
    "href": "Skill Builders/introduction.html#before-you-start",
    "title": "Introduction",
    "section": "Before You Start",
    "text": "Before You Start\nMake sure you have installed VS-code, pandas, and plotly express on your computer. You can install these package by typing this line in the terminal.\npip install pandas plotly.express\nOR if you have more than one version of python\npip3.10 install pandas plotly.express\npip3.10 indicates the version of python you are installing the packages to.\n\n\nGet Familiar With Your Tools\nProgramming involves a lot of research. Unlike subjects like Mathematics or History, we are not required to remember every single function and its usage. It is natural for experienced programmers to look for answers on the internet, books, even from other people’s code. Programming will be extremely frustrating if we are not allowed to do web searches, so please get familiar with the tools you have and use them often.\n\n\n\nOffical Documentation\nThis should be your first resort for understanding any code/function. Scanning the documentation of a function will allow you to get an overview of its usage.\nHere is a link to the documentation of the assign() function:\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html\nExample of assign() (as shown in the documentation)\n    import pandas as pd\n\n    df1 = pd.DataFrame({'temp_c': [17.0, 25.0]},\n                  index=['Portland', 'Berkeley'])\n    \n    df2 = df1.assign(temp_f=df1.temp_c * 9 / 5 + 32)\n\n\n\nExercise 1\nAfter reading the documentation for assign(), write a short paragraph to explain assign() as if you were talking to someone with zero programming experience (use the example above to help you explain assign()).\n\nWhat is the difference between df1 and df2?\nHow was df2 derived from df1?\n\n\nOnline Textbook\nIt pains us to see students would rather be stuck at problems for hours yet they refuse to use the textbook. This is another very useful resource since this is designed for this class. link to the textbook: https://byuidatascience.github.io/python4ds/\n\n\n\n\nExercise 2\nLocate the section where the textbook talks about query() and answer these questions.\n\nWhat function in R’s dplyr is equivalent or comparable to query() in pandas (You should include the section number in your answer)?\nWhat is the easiest mistake for python beginner to make that was shown in the text about query() (You should include the section number in your answer)?\n\n\nThe Internet\nGoogle is a programmer’s friend. Get used to googling thing, in fact, you want to be an expert in googling\n\nQuestion that cannot be answered by the textbook and documentation? Google it.\n\nA function you have never seen before? Google it.\nAn error in your code? Google it.\n\n\n\n\n\nExercise 3\nProvide at least 2 extra resources you could find about the pandas function drop() on the internet.\n\nTutor, TA (Through Slack, Zoom, or In-Person)\nWe want to help you with your work; we want to answer your questions; but most importantly, we want to help you succeed in this class. That will require you to put in the necessary time in understanding the readings, coding and debugging. When you ask us a question, we expect that you have read the documentation, searched the textbook, and done your own research. Then we can be most helpful and can provide insights on top of your understanding.\n\n\nExamples of Bad Questions\n\nHow does drop() work? We will ask you to read the documentation for drop().\nHow do you make a table in a markdown file? We will refer you to the textbook.\nI don’t want these columns in my data, how can I drop them? We will ask you if you have found any things on the internet.\n\n\n\nExamples of good questions\n\nI am still confused about the syntax of drop(). After reading the documentation, this is my understanding of the function… . What am I missing?\nI tried making a table in markdown (show code), it is still not giving me what I want, how can I fix this?\nI am trying to drop these columns in my dataframe, I think drop() is what I am looking for. Am I in the right direction? If not, what keywords should I be googling?\n\n\n\n\n\nExercise 4\nUsing the code and tools mentioned above, finish question 4 and 5 under 3.2.4 in the textbook.(use the data in mpg for your plot):\n# library import\nimport pandas as pd \nimport plotly.express as px\n\n# data import\nurl = \"https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/mpg/mpg.csv\"\n\nmpg = pd.read_csv(url)\n\nQuestion 4: Make a scatterplot of hwy vs cyl.\nQuestion 5: What happens if you make a scatterplot of class vs drv? Why is the plot not useful?\n\n\n\n\n\n\n\n\nAfter you have completed this skill builder with your team (or on your own) then compare your work to our script\n\n\n\n\n\nSee the script.",
    "crumbs": [
      "Skill Builders",
      "Project 0: Introduction"
    ]
  },
  {
    "objectID": "Setup/vs_code_setup.html",
    "href": "Setup/vs_code_setup.html",
    "title": "VS Code for Data Science",
    "section": "",
    "text": "Download and Install Visual Studio Code\n\nUse the link below to download and install the latest version of VS Code:\n\nVS Code\n\n\n\nInstall VS Code Extensions\nUse these links to install the VS Code Extensions. Note: DO NOT install python when you install the python extension. You will install python from a different source in the next step.\n\nJupyter\nLive Share\nPython\nPylance\nQuarto\n\n\n\nLearn More About VS Code\nVS Code\n\n\nContinue to Install Quarto\nQuarto\n\n\n\n\n Back to top",
    "crumbs": [
      "Setup",
      "VS Code"
    ]
  },
  {
    "objectID": "Setup/slack_setup.html",
    "href": "Setup/slack_setup.html",
    "title": "VS Code for Data Science",
    "section": "",
    "text": "Intro to Slack\n\n\n\nDownload and Install Slack\nUse the link below to download and install the latest version of Slack (both desktop and mobile):\n\nSlack for Windows\nSlack for Mac\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Setup",
      "Slack"
    ]
  },
  {
    "objectID": "Setup/python_setup.html",
    "href": "Setup/python_setup.html",
    "title": "Python for Data Science",
    "section": "",
    "text": "Download and Install Python\n\nWe need to download and install the latest version of python approved by your professor. (Python 3.11.7)\nNote: [do not] install python from VS Code or the Microsoft Store or Anaconda Python.org is the only place you should install VS Code from (link below)\n\nPython\n\n\nInstall Python on Windows\nMake sure to check the box that says Add Python to PATH before clicking Install Now. Sometimes it is phrased as Add Python to environment variables.\n\n\n\nContinue to Install Python Libraries\nInstall Python Libraries\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Setup",
      "Python"
    ]
  },
  {
    "objectID": "Setup/plotly_setup.html",
    "href": "Setup/plotly_setup.html",
    "title": "Altair for Charts",
    "section": "",
    "text": "Plotly Express Visualization\nTo get started with plotly.express (typically imported as px), we must first install the library in the usual using pip in the VSCode terminal.\n\npip install plotly.express\n\nRequirement already satisfied: plotly.express in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.4.1)\nRequirement already satisfied: pandas&gt;=0.20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from plotly.express) (1.5.3)\nRequirement already satisfied: plotly&gt;=4.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from plotly.express) (5.18.0)\nRequirement already satisfied: statsmodels&gt;=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from plotly.express) (0.14.1)\nRequirement already satisfied: scipy&gt;=0.18 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from plotly.express) (1.12.0)\nRequirement already satisfied: patsy&gt;=0.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from plotly.express) (0.5.6)\nRequirement already satisfied: numpy&gt;=1.11 in /Users/chazclar/Library/Python/3.11/lib/python/site-packages (from plotly.express) (1.26.4)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /Users/chazclar/Library/Python/3.11/lib/python/site-packages (from pandas&gt;=0.20.0-&gt;plotly.express) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas&gt;=0.20.0-&gt;plotly.express) (2024.1)\nRequirement already satisfied: six in /Users/chazclar/Library/Python/3.11/lib/python/site-packages (from patsy&gt;=0.5-&gt;plotly.express) (1.16.0)\nRequirement already satisfied: tenacity&gt;=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from plotly&gt;=4.1.0-&gt;plotly.express) (8.2.3)\nRequirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from plotly&gt;=4.1.0-&gt;plotly.express) (23.2)\n\n[notice] A new release of pip is available: 24.0 -&gt; 24.1\n[notice] To update, run: pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n\n\nNext, we load the library into a Python script or notebook.\n\nimport plotly.express as px\n\nThis gives us access to all functions and built-in datasets to begin using plotly.express to build visualizations in Python.\n\n\nPlotly.Express Basics\nThe generic syntax of a px graph is px.graphic_type(dataset, x, y, color, size, title...)\nFor an incredibly boring first graph, let’s try:\n\npx.scatter(x = [7,8,9,10,11,12], y = [6,5,4,3,2,1], title = \"We did it!\")\n\n                                                \n\n\n\nPlotly Express Styling\nPlotly Express in Python\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Setup/copilot_setup.html",
    "href": "Setup/copilot_setup.html",
    "title": "Getting started with GitHub Copilot",
    "section": "",
    "text": "GitHub Copilot is free to use for verified students, teachers, and maintainers of popular open source projects.",
    "crumbs": [
      "Setup",
      "Copilot"
    ]
  },
  {
    "objectID": "Setup/copilot_setup.html#footnotes",
    "href": "Setup/copilot_setup.html#footnotes",
    "title": "Getting started with GitHub Copilot",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n1↩︎\n2↩︎\nhttps://docs.github.com/en/copilot/using-github-copilot/getting-started-with-github-copilot#next-steps↩︎",
    "crumbs": [
      "Setup",
      "Copilot"
    ]
  },
  {
    "objectID": "Setup/altair_setup.html",
    "href": "Setup/altair_setup.html",
    "title": "Altair for Charts",
    "section": "",
    "text": "Altair Visualization\nWe will be using Altair in our course. It is a declarative visualization package in Python that is based on Vega-Lite which leverages the grammar of graphics.\n\nUser Guide\nData Visualization Curriculum or the Quarto version\nP4DS Data Visualization Chapter\n\n\n\nRendering Altair Charts in Quarto\nWe use Quarto to render Altair images automagically into our HTML reports. The process should simply work. If not, read in the following section if you need to export one of your images as a .png or another image format.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Setup/Archive/altair_setup_v0.html",
    "href": "Setup/Archive/altair_setup_v0.html",
    "title": "Altair for Charts",
    "section": "",
    "text": "Altair Visualization\nWe will be using Altair in our course. It is a declarative visualization package in Python that is based on Vega-Lite which leverages the grammar of graphics.\n\nUser Guide\nData Visualization Curriculum or the Quarto version\nP4DS Data Visualization Chapter\n\n\n\nRendering Altair Charts in Quarto\nWe use Quarto to render Altair images automagically into our HTML reports. The process should simply work. If not, read in the following section if you need to export one of your images as a .png or another image format.\n\n\nBelow are backup methods if Quarto is not working to render Altair Charts\n\n\nRendering Altair Charts for Win 10/11 and Intel-Mac (NodeJS Installation)\n\nInstall the NodeJS for your platform\nRun the following in your Terminal (Intel-Mac) or PowerShell (Windows) to install all the packages we need from NodeJS. \n\nnpm install -g vega-lite vega-cli canvas\n\n\n\nRendering Altair Charts for M1/M2 Mac\n\nInstall selenium using the chromedriver package form this link: https://chromedriver.chromium.org.\nUnzip the file and move the file to your chrome path /usr/local/bin/chromedriver\n\nSee the selenium_fix.py script for an example.\nNote: This process will run a local server on your computer that opens the chart as an PNG file in chrome and downloads the file to the folder in which that VSCode file is located on your computer.\n\n\nSaving Altair Charts (not needed with Quarto - reference only)\nJust installing altair and altair_saver will not allow you to leverage the .save() method to save your chart. The javascript visualization you see in your interactive python window needs additional external applications to allow .save('chart.png') to work.\nWe will go through a few ways for us to save our Altair plots.\n\n\n\n1. Saving altair plots programmatically\n\n\n\nabove plot to be saved\n\n\nLet’s say we want to save the above plot as a PNG file. Assuming we have already installed the altair library, we need to install the altair_saver.\n\n1.1 Installing the altair_saver\nWithin your interactive python window execute the following command.\nimport sys\n!{sys.executable} -m pip install altair_saver\n\n\n1.2 Additional tool for saving plots\nWe suggest NodeJS path. However, you are more than welcome to study Selenium for further understanding. The Github repository for altair_saver, the developers exclusively told us to install additional tools.\n\n\n1.3 Saving a plot using altair_saver\nIt might require you to restart VScode and import everything again for this to work. Please note that the plot will be saved in the same folder of the script.\nchart = alt.chart(&lt;data&gt;).&lt;chart_methods&gt;\nchart.save('name_of_chart.png')\n\n\n\n2. Save as PNG method\nThe method only requires us to have Altair library. Whenever we output a plot, we will see a button with three dots at the top right corner of the plot.\n\n\n\nClicking Save as PNG will bring us to a window to save our plot.\n\n\nClicking Save as PNG will bring us to a window to save our plot.\n\n\n3. Screenshot method\nIf all thing fails and we need to save a plot, the snip & sketch (Windows) or taking a screenshot (MacOS) will be our last resort.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Projects/project_5.html",
    "href": "Projects/project_5.html",
    "title": "Project 5: The War with Star Wars",
    "section": "",
    "text": "Walkthrough\n\n\n\nBackground\nSurvey data is notoriously difficult to munge. Even when the data is recorded cleanly the options for ‘write in questions’, ‘choose from multiple answers’, ‘pick all that are right’, and ‘multiple choice questions’ makes storing the data in a tidy format difficult.\nIn 2014, FiveThirtyEight surveyed over 1000 people to write the article titled, America’s Favorite ‘Star Wars’ Movies (And Least Favorite Characters). They have provided the data on GitHub.\nFor this project, your client would like to use the Star Wars survey data to figure out if they can predict an interviewing job candidate’s current income based on a few responses about Star Wars movies.\n\n\nClient Request\nThe Client is who performed the survey but outsourced the analitics to a 3rd party. They want you to clean up the data so you can: a. Validate the data provided on GitHub lines up with the article by recreating 2 of the visuals from the article a. Determine if you predict if a person from the survey makes more than $50k\n\n\nData\nDownload: StarWars.csv\nInformation: Article\n\n\nReadings\n\nP4DS: CH6 Tidy Data\nP4DS: CH14 Graphics for Communication\nP4DS: CH16 Numbers\nP4DS: CH17 Strings and Text\nP4DS: Ch18 Regular Expressions\nP4DS: CH19 Categorical Data\n\n\nOptional References\n\nWhy to not use get_dummies\n\n\n\n\nQuestions and Tasks (Core)\n\nShorten the column names and clean them up for easier use with pandas. Provide a table or list that exemplifies how you fixed the names.\nClean and format the data so that it can be used in a machine learning model. As you format the data, you should complete each item listed below. In your final report provide example(s) of the reformatted data with a short description of the changes made.\n\nFilter the dataset to respondents that have seen at least one film\n\nCreate a new column that converts the age ranges to a single number. Drop the age range categorical column\n\nCreate a new column that converts the education groupings to a single number. Drop the school categorical column\n\nCreate a new column that converts the income ranges to a single number. Drop the income range categorical column\n\nCreate your target (also known as “y” or “label”) column based on the new income range column\n\nOne-hot encode all remaining categorical columns\n\nValidate that the data provided on GitHub lines up with the article by recreating 2 of the visuals from the article.\nBuild a machine learning model that predicts whether a person makes more than $50k. Describe your model and report the accuracy.\n\n\n\nQuestions and Tasks (Stretch)\nHere is an example Stretch question(s) for this project. Your instructor may assign different Stretch question(s). You must comment in Canvas when submitting your project if you completed any of the Stretch questions.\n\nBuild a machine learning model that predicts whether a person makes more than $50k. With accuracy of at least 65%. Describe your model and report the accuracy.\nValidate the data provided on GitHub lines up with the article by recreating a 3rd visual from the article.\nCreate a new colum that converts the location groupings to a single number. Drop the location categorical column.\n\n\n\nDeliverables\nUse this template to submit your Client Report. The template has two sections:\n\nA short elevator pitch that highlights key values or metrics from the results. Describing these key insights to interest or hook the reader to want to read more about your work. The writing style should be more technical with some creative elements. Do not summarize what you did.\n\nAnswers to the questions | tasks. Each answer should include a written description of your results, code cells with comments, charts, and/or tables.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nYour report should be written in quarto markdown files and pushed to GitHub which will render it to HTML. Submit a URL of the rendered project in Canvas. (Do not submit the URL to the GitHub .qmd file)\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Projects",
      "Project 5: The War with Star Wars"
    ]
  },
  {
    "objectID": "Projects/project_3.html",
    "href": "Projects/project_3.html",
    "title": "Project 3: Finding Relationships in Baseball",
    "section": "",
    "text": "Walkthrough\n\n\n\nSQL Refresher\n\n\n\nBackground\nWhen you hear the word “relationship” what is the first thing that comes to mind? Probably not baseball. But a relationship is simply a way to describe how two or more objects are connected. There are many relationships in baseball such as those between teams and managers, players and salaries, even stadiums and concession prices.\nThe graphs on Data Visualizations from Best Tickets show many other relationships that exist in baseball.\n\n\nClient Request\nFor this project, the Client wants SQL queries that they can use to retrieve data for use on their website without needing Python. They would also like to see the results in Plotly Express charts.\n\n\nData\nDownload: lahmansbaseballdb\nInformation: Lahman Data Dictionary\nSetup Instructions: See SQL Setup\n\n\nReadings\n\nSQL Setup and References\nSQL for Data Science\n\n\nOptional References\n\nWhy SQL is beating NoSQL, and what this means for the future of data\n\n\n\n\nQuestions and Tasks (Core)\n\nWrite an SQL query to create a new dataframe about baseball players who attended BYU-Idaho. The new table should contain five columns: playerID, schoolID, salary, and the yearID/teamID associated with each salary. Order the table by salary (highest to lowest) and print out the table in your report.\nThis three-part question requires you to calculate batting average (number of hits divided by the number of at-bats)\n\nWrite an SQL query that provides playerID, yearID, and batting average for players with at least 1 at bat that year. Sort the table from highest batting average to lowest, and then by playerid alphabetically. Show the top 5 results in your report.\n\nUse the same query as above, but only include players with at least 10 at bats that year. Print the top 5 results.\n\nNow calculate the batting average for players over their entire careers (all years combined). Only include players with at least 100 at bats, and print the top 5 results.\n\nPick any two baseball teams and compare them using a metric of your choice (average salary, home runs, number of wins, etc). Write an SQL query to get the data you need, then make a graph using Plotly Express to visualize the comparison. What do you learn?\n\n\n\nQuestions and Tasks (Stretch)\nHere is an example Stretch question(s) for this project. Your instructor may assign different Stretch question(s). You must comment in Canvas when submitting your project if you completed any of the Stretch questions.\n\nAdvanced Salary Distribution by Position (with Case Statement):\n\nWrite an SQL query that provides a summary table showing the average salary for players in each position (e.g., pitcher, catcher, outfielder) across all years. Include the following columns:\n\nposition\naverage_salary\ntotal_players\nhighest_salary\n\nThe highest_salary column should display the highest salary ever earned by a player in that position. If no player in that position has a recorded salary, display “N/A” for the highest salary.\nAdditionally, create a new column called salary_category using a case statement:\n\nIf the average salary is above $1 million, categorize it as “High Salary.”\n\nIf the average salary is between $500,000 and $1 million, categorize it as “Medium Salary.”\n\nOtherwise, categorize it as “Low Salary.”\n\nOrder the table by average salary in descending order.\nPrint the top 10 rows of this summary table.\n\nAdvanced Career Longevity and Performance (with Subqueries):\n\nCalculate the average career length (in years) for players who have played at least one game. Then, identify the top 10 players with the longest careers (based on the number of years they played). Include their:\n\nplayerID\nfirst_name\nlast_name\ncareer_length\n\nThe career_length should be calculated as the difference between the maximum and minimum yearID for each player.\n\n\n\n\nDeliverables\nUse this template to submit your Client Report. The template has two sections:\n\nA short elevator pitch that highlights key values or metrics from the results. Describing these key insights to interest or hook the reader to want to read more about your work. The writing style should be more technical with some creative elements. Do not summarize what you did.\nAnswers to the questions | tasks. Each answer should include a written description of your results, code cells with comments, charts, and/or tables.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nYour report should be written in quarto markdown files and pushed to GitHub which will render it to HTML. Submit a URL of the rendered project in Canvas. (Do not submit the URL to the GitHub .qmd file)\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Projects",
      "Project 3: Finding Relationships in Baseball"
    ]
  },
  {
    "objectID": "Projects/project_1.html",
    "href": "Projects/project_1.html",
    "title": "Project 1: What’s in a Name?",
    "section": "",
    "text": "Walkthrough\n\n\n\nBackground\n\n\n\n\n\n\nNote\n\n\n\n\n\nWe will complete six projects during the semester that each take about two weeks (four days of class). On average, a student will spend 2 hours outside of class per hour in class to complete the assigned readings, submit any Canvas items, and complete the project (for a total of 8 hours per project). The instruction for each project will be structured into sections as written on this page.\nThis first Background section provides context for the project. Make sure you read the background carefully to see the big picture needs and purpose of the project.\n\n\n\nEarly in prehistory, some descriptive names began to be used again and again until they formed a name pool for a particular culture. Parents would choose names from the pool of existing names rather than invent new ones for their children.\nWith the rise of Christianity, certain trends in naming practices manifested. Christians were encouraged to name their children after saints and martyrs of the church. These early Christian names can be found in many cultures today, in various forms. These were spread by early missionaries throughout the Mediterranean basin and Europe.\nBy the Middle Ages, the Christian influence on naming practices was pervasive. Each culture had its pool of names, which were a combination of native names and early Christian names that had been in the language long enough to be considered native. [ref]\n\n\nClient Request\nThis csv file contains the number of times a name was given to a child in a specific year. The Client has a passion for names throughout history. They would like to know how the usage of names has changed over time. They are particularly interested in the names Mary, Martha, Peter, and Paul. They would also like to know how the usage of a name from a famous movie has changed over time.\n\n\nData\n\n\n\n\n\n\nNote\n\n\n\n\n\nEvery data science project should start with data, and our class projects are no different. Each project will have ‘Download’ and ‘Information’ links like the ones below.\n\n\n\nDownload: names_year.csv\nInformation: data.md\n\n\nReadings\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe Readings section will contain links to reading assignments that are required for each project, as well as optional references. Remember that you are reading this material to build skills. Take the time to comprehend the readings and the skills contained within.\nWe recommend reading through the assigned material once for a general understanding before the first day of each project. You will reread and reference the material multiple times as you complete the project.\n\n\n\n\nP4DS: CH7 Workflow: Writing Code\nP4DS: CH8 Data Import\nP4DS: CH14 Graphics for Communication\nP4DS: CH30 Markdown\nP4DS: CH31 Quarto\n\n\nOptional References\n\nThe query method\nEasy Data Visualiztation for Tidy Data with Lets-Plot\nExplore Your Data with Lets-Plot\n\n\n\n\nQuestions and Tasks (Core)\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis section lists the questions and tasks that need to be completed for the project. Your work on the project must be compiled into a report, pushed to GitHub and a URL submitted in Canvas by the weekend following the last day of material for the project.\nThere are two types of questions: Core and Stretch. Core questions are required for each project. The course syllabus competencies requires specic a number of projects having all the Stretch questions achived based on your goals for the grade level you are seeking.\n\n\n\nFor Project 1 the answer to each question should include a chart and a written response. The years labels on your charts should not include a comma. At least two of your charts must include reference marks.\n\nHow does your name at your birth year compare to its use historically?\n\nIf you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess?\n\nMary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names. What trends do you notice?\n\nThink of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?\n\n\n\nQuestions and Tasks (Stretch)\nHere is an example Stretch question(s) for this project. Your instructor may assign different Stretch question(s). You must comment in Canvas when submitting your project if you completed any of the Stretch questions.\n\nReproduce the chart Elliot using the data from the names_year.csv file.\n\n\n\n\nDeliverables\n\n\n\n\n\n\nNote\n\n\n\n\n\nDeliverables are “the quantifiable goods or services that must be provided upon the completion of a project”. In this class the deliverable for each project is a GitHub published report created using Quarto files. This final section will be the same for each project.\n\n\n\nUse this template to submit your Client Report. The template has two sections:\n\nA short elevator pitch that highlights key values or metrics from the results. Describing these key insights to interest or hook the reader to want to read more about your work. The writing style should be more technical with some creative elements. Do not summarize what you did.\n\nAnswers to the questions | tasks. Each answer should include a written description of your results, code cells with comments, charts, and/or tables.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nYour report should be written in quarto markdown files and pushed to GitHub which will render it to HTML. Submit a URL of the rendered project in Canvas. (Do not submit the URL to the GitHub .qmd file)\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Projects",
      "Project 1: What's in a Name?"
    ]
  },
  {
    "objectID": "Projects/Archive/project_6_v0.html",
    "href": "Projects/Archive/project_6_v0.html",
    "title": "Project 6: Git your resume online",
    "section": "",
    "text": "Background\nGitHub is an online platform where data scientists and developers can communicate and share work. As students, you will want to curate your creative work on GitHub using a program called Git. GitHub is the place to share your original work, not your homework assignments.\nMany people store their personal websites, blogs, and project websites on GitHub. Our textbook and course are hosted on GitHub, and you can see J. Hathaway’s or Ryan Hafen’s personal Data Science websites that are hosted on GitHub as well. For this project, you will be making a public resume that will be hosted on GitHub.\nDuring this project you will learn the process of Git and the tools of GitHub. We will use Git to have others in our class to edit your resume. Take the process seriously (pick a suitable username and write a good resume), and you will have the beginning of your social presence in the DS/CS space.\n\n\nData\nRepository: Markdown Resume (mdresume) Repository\nInformation: BYUI Data Science Resumes\n\n\nReadings\n\nNew to Git and GitHub? This Essential Beginners Guide is for you\nGit vs. GitHub: What is the difference between them?\nUsing Version Control in VS Code\nGit in Visual Studio Code video\n\n\n\n\nQuestions and Tasks\n\nJoin GitHub. Pick a username you would be ok sharing with a potential employer.\nJoin the BYUI Data Science Resumes GitHub organization and use the template repository to make a resume repository under your own GitHub account. A good name might be “Lastname-Resume”\nClone your repository to your computer and build a first draft of your resume.\nPush your results to GitHub and have another student fork your repository to make edits.\nAccept the proposed changes from the student review and finish your final version.\nAfter your resume is complete, make sure your it is forked into BYU-I Data Science Resumes\n\n\n\nDeliverables:\n\nComplete the questions\nSubmit a URL link to your repo within the byuids-resumes group.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Course Materials/vs_code.html",
    "href": "Course Materials/vs_code.html",
    "title": "VS Code for Data Science",
    "section": "",
    "text": "Learn How to Install VS Code\nVS Code\n\n\n\nWhat if my interactive Python window in VS Code is not using the same version of Python as my terminal?\nYou can set your Python version in VS Code by opening a .py script and then clicking on the Python text in the bottom left corner as shown below.\n\nOnce you click, VS Code will open the command pallete where you can select your installation of Python that you would like to use with this workspace.\n\nThis setting will not fix what version your interactive Python window is using. You can get there by opening settings by using the ⌘, shortcut.\nYou can then search your settings for jupyter and you should see a section that has Jupyter Command Line Arguments. Click on the Edit in settings.json.\n\nHere you can set the jupyter path to Python to match the one you picked for your Terminal. An example for a Mac computer is shown below.\n\"python.pythonPath\": \"/usr/local/opt/python/bin/python3\",\n\n\n\nWhat if I am not able to read in files from the GitHub links using read_csv()?\nMost likely your Python SSl certificates are not installed. Follow the answer in this post.\n\n\n\nHow do I use VS Code to collaborate?”\nMicrosft’s Live Share extension documentation says, ‘Live Share enables you to quickly collaborate with a friend, classmate, or professor on the same code without the need to sync code or to configure the same development tools, settings, or environment.’ You can follow their guide or use our course created video.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Materials",
      "VS Code"
    ]
  },
  {
    "objectID": "Course Materials/sql_page.html",
    "href": "Course Materials/sql_page.html",
    "title": "Introduction",
    "section": "",
    "text": "Structured Query Language (SQL) is a powerful tool for managing and manipulating relational databases. It is essential for data scientists, analysts, and anyone working with large datasets. This chapter will explore the importance of SQL, its applications, and provide example code format to illustrate its utility."
  },
  {
    "objectID": "Course Materials/sql_page.html#why-sql-is-important",
    "href": "Course Materials/sql_page.html#why-sql-is-important",
    "title": "Introduction",
    "section": "Why SQL is Important",
    "text": "Why SQL is Important\n\nData Management: SQL allows for efficient management of large volumes of data. It provides the means to create, read, update, and delete data in a relational database.\nData Retrieval: With SQL, you can perform complex queries to retrieve specific data from one or more tables, making it easier to analyze and draw insights.\nData Manipulation: SQL enables the manipulation of data through operations such as sorting, filtering, and aggregating. This is crucial for data cleaning and preprocessing.\nData Integration: SQL supports the integration of data from different sources, allowing for comprehensive data analysis.\nStandardization: SQL is a standardized language used by most relational database management systems (RDBMS), making it a versatile and essential skill for professionals in the field."
  },
  {
    "objectID": "Course Materials/sql_page.html#basic-sql-concepts",
    "href": "Course Materials/sql_page.html#basic-sql-concepts",
    "title": "Introduction",
    "section": "Basic SQL Concepts",
    "text": "Basic SQL Concepts\n\nimport sqlite3\nimport pandas as pd"
  },
  {
    "objectID": "Course Materials/sql_page.html#basic-sql-concepts-1",
    "href": "Course Materials/sql_page.html#basic-sql-concepts-1",
    "title": "Introduction",
    "section": "Basic SQL Concepts",
    "text": "Basic SQL Concepts\nTo illustrate the following SQL concepts, we will use the employees table with the following data:\nTable: employees\n\n\n\nid\nfirst_name\nlast_name\ndepartment\nsalary\n\n\n\n\n1\nAlice\nSmith\nHR\n60000\n\n\n2\nBob\nJohnson\nIT\n80000\n\n\n3\nCharlie\nLee\nSales\n55000\n\n\n4\nDavid\nKim\nHR\n75000\n\n\n5\nEva\nBrown\nIT\n65000\n\n\n6\nFrank\nWilson\nSales\n70000\n\n\n7\nGrace\nTaylor\nHR\n62000\n\n\n8\nHenry\nAnderson\nIT\n77000\n\n\n9\nIrene\nThomas\nSales\n53000\n\n\n10\nJack\nWhite\nHR\n58000\n\n\n11\nKaren\nHarris\nIT\n69000\n\n\n12\nLeo\nMartin\nSales\n50000\n\n\n13\nMia\nJackson\nHR\n64000\n\n\n14\nNoah\nLee\nIT\n72000\n\n\n15\nOlivia\nPerez\nSales\n68000\n\n\n16\nPaul\nYoung\nHR\n61000\n\n\n17\nQuinn\nKing\nIT\n76000\n\n\n18\nRachel\nScott\nSales\n57000\n\n\n19\nSam\nGreen\nHR\n63000\n\n\n20\nTina\nAdams\nIT\n81000\n\n\n\n\nSELECT and FROM\nThe SELECT statement is used to fetch data from a database, and the FROM clause specifies the table.\n-- Selecting all columns from a table\np = \"\"\"\n\nSELECT * \nFROM \n  employees;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\ndepartment\nsalary\n\n\n\n\n1\nAlice\nSmith\nHR\n60000\n\n\n2\nBob\nJohnson\nIT\n80000\n\n\n3\nCharlie\nLee\nSales\n55000\n\n\n4\nDavid\nKim\nHR\n75000\n\n\n5\nEva\nBrown\nIT\n65000\n\n\n6\nFrank\nWilson\nSales\n70000\n\n\n7\nGrace\nTaylor\nHR\n62000\n\n\n8\nHenry\nAnderson\nIT\n77000\n\n\n9\nIrene\nThomas\nSales\n53000\n\n\n10\nJack\nWhite\nHR\n58000\n\n\n11\nKaren\nHarris\nIT\n69000\n\n\n12\nLeo\nMartin\nSales\n50000\n\n\n13\nMia\nJackson\nHR\n64000\n\n\n14\nNoah\nLee\nIT\n72000\n\n\n15\nOlivia\nPerez\nSales\n68000\n\n\n16\nPaul\nYoung\nHR\n61000\n\n\n17\nQuinn\nKing\nIT\n76000\n\n\n18\nRachel\nScott\nSales\n57000\n\n\n19\nSam\nGreen\nHR\n63000\n\n\n20\nTina\nAdams\nIT\n81000\n\n\n\n-- Selecting specific columns\np = \"\"\"\n\nSELECT \n  first_name, \n  last_name, \n  salary \nFROM \n  employees;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\nsalary\n\n\n\n\n1\nAlice\nSmith\n60000\n\n\n2\nBob\nJohnson\n80000\n\n\n3\nCharlie\nLee\n55000\n\n\n4\nDavid\nKim\n75000\n\n\n5\nEva\nBrown\n65000\n\n\n6\nFrank\nWilson\n70000\n\n\n7\nGrace\nTaylor\n62000\n\n\n8\nHenry\nAnderson\n77000\n\n\n9\nIrene\nThomas\n53000\n\n\n10\nJack\nWhite\n58000\n\n\n11\nKaren\nHarris\n69000\n\n\n12\nLeo\nMartin\n50000\n\n\n13\nMia\nJackson\n64000\n\n\n14\nNoah\nLee\n72000\n\n\n15\nOlivia\nPerez\n68000\n\n\n16\nPaul\nYoung\n61000\n\n\n17\nQuinn\nKing\n76000\n\n\n18\nRachel\nScott\n57000\n\n\n19\nSam\nGreen\n63000\n\n\n20\nTina\nAdams\n81000\n\n\n\n\n\nSELECT EXCLUDE and RENAME\nYou can exclude columns using SELECT and rename them for clarity.\n-- Selecting all but one column\np = \"\"\"\n\nSELECT * \nEXCLUDE \n  salary \nFROM \n  employees;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\ndepartment\n\n\n\n\n1\nAlice\nSmith\nHR\n\n\n2\nBob\nJohnson\nIT\n\n\n3\nCharlie\nLee\nSales\n\n\n4\nDavid\nKim\nHR\n\n\n5\nEva\nBrown\nIT\n\n\n6\nFrank\nWilson\nSales\n\n\n7\nGrace\nTaylor\nHR\n\n\n8\nHenry\nAnderson\nIT\n\n\n9\nIrene\nThomas\nSales\n\n\n10\nJack\nWhite\nHR\n\n\n11\nKaren\nHarris\nIT\n\n\n12\nLeo\nMartin\nSales\n\n\n13\nMia\nJackson\nHR\n\n\n14\nNoah\nLee\nIT\n\n\n15\nOlivia\nPerez\nSales\n\n\n16\nPaul\nYoung\nHR\n\n\n17\nQuinn\nKing\nIT\n\n\n18\nRachel\nScott\nSales\n\n\n19\nSam\nGreen\nHR\n\n\n20\nTina\nAdams\nIT\n\n\n\n-- Renaming columns\np = \"\"\"\n\nSELECT \n  first_name AS fname, \n  last_name AS lname \nFROM \n  employees;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nfname\nlname\n\n\n\n\nAlice\nSmith\n\n\nBob\nJohnson\n\n\nCharlie\nLee\n\n\nDavid\nKim\n\n\nEva\nBrown\n\n\nFrank\nWilson\n\n\nGrace\nTaylor\n\n\nHenry\nAnderson\n\n\nIrene\nThomas\n\n\nJack\nWhite\n\n\nKaren\nHarris\n\n\nLeo\nMartin\n\n\nMia\nJackson\n\n\nNoah\nLee\n\n\nOlivia\nPerez\n\n\nPaul\nYoung\n\n\nQuinn\nKing\n\n\nRachel\nScott\n\n\nSam\nGreen\n\n\nTina\nAdams\n\n\n\n\n\nLIMIT and OFFSET\nThe LIMIT clause restricts the number of rows returned, and OFFSET skips rows before beginning to return rows.\n-- Limiting the number of rows returned\n\np = \"\"\"\n\nSELECT * \nFROM \n  employees \nLIMIT \n  10;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\ndepartment\nsalary\n\n\n\n\n1\nAlice\nSmith\nHR\n60000\n\n\n2\nBob\nJohnson\nIT\n80000\n\n\n3\nCharlie\nLee\nSales\n55000\n\n\n4\nDavid\nKim\nHR\n75000\n\n\n5\nEva\nBrown\nIT\n65000\n\n\n6\nFrank\nWilson\nSales\n70000\n\n\n7\nGrace\nTaylor\nHR\n62000\n\n\n8\nHenry\nAnderson\nIT\n77000\n\n\n9\nIrene\nThomas\nSales\n53000\n\n\n10\nJack\nWhite\nHR\n58000\n\n\n\n-- Skipping rows\np = \"\"\"\n\nSELECT * \nFROM \n  employees \nLIMIT 10 \nOFFSET 5;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\ndepartment\nsalary\n\n\n\n\n6\nFrank\nWilson\nSales\n70000\n\n\n7\nGrace\nTaylor\nHR\n62000\n\n\n8\nHenry\nAnderson\nIT\n77000\n\n\n9\nIrene\nThomas\nSales\n53000\n\n\n10\nJack\nWhite\nHR\n58000\n\n\n11\nKaren\nHarris\nIT\n69000\n\n\n12\nLeo\nMartin\nSales\n50000\n\n\n13\nMia\nJackson\nHR\n64000\n\n\n14\nNoah\nLee\nIT\n72000\n\n\n15\nOlivia\nPerez\nSales\n68000\n\n\n\n\n\nORDER BY\nThe ORDER BY clause sorts the result set.\n-- Sorting the result set by salary in ascending order\np = \"\"\"\n\nSELECT * \nFROM  \n  employees \nORDER BY \n  salary;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\ndepartment\nsalary\n\n\n\n\n12\nLeo\nMartin\nSales\n50000\n\n\n9\nIrene\nThomas\nSales\n53000\n\n\n3\nCharlie\nLee\nSales\n55000\n\n\n10\nJack\nWhite\nHR\n58000\n\n\n1\nAlice\nSmith\nHR\n60000\n\n\n19\nSam\nGreen\nHR\n63000\n\n\n7\nGrace\nTaylor\nHR\n62000\n\n\n13\nMia\nJackson\nHR\n64000\n\n\n5\nEva\nBrown\nIT\n65000\n\n\n11\nKaren\nHarris\nIT\n69000\n\n\n18\nRachel\nScott\nSales\n57000\n\n\n15\nOlivia\nPerez\nSales\n68000\n\n\n6\nFrank\nWilson\nSales\n70000\n\n\n14\nNoah\nLee\nIT\n72000\n\n\n17\nQuinn\nKing\nIT\n76000\n\n\n8\nHenry\nAnderson\nIT\n77000\n\n\n4\nDavid\nKim\nHR\n75000\n\n\n2\nBob\nJohnson\nIT\n80000\n\n\n20\nTina\nAdams\nIT\n81000\n\n\n16\nPaul\nYoung\nHR\n61000\n\n\n\n-- Sorting in descending order\np = \"\"\"\n\nSELECT * \nFROM \n  employees \nORDER BY \n  salary DESC;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\ndepartment\nsalary\n\n\n\n\n20\nTina\nAdams\nIT\n81000\n\n\n2\nBob\nJohnson\nIT\n80000\n\n\n8\nHenry\nAnderson\nIT\n77000\n\n\n17\nQuinn\nKing\nIT\n76000\n\n\n4\nDavid\nKim\nHR\n75000\n\n\n14\nNoah\nLee\nIT\n72000\n\n\n6\nFrank\nWilson\nSales\n70000\n\n\n15\nOlivia\nPerez\nSales\n68000\n\n\n11\nKaren\nHarris\nIT\n69000\n\n\n5\nEva\nBrown\nIT\n65000\n\n\n13\nMia\nJackson\nHR\n64000\n\n\n19\nSam\nGreen\nHR\n63000\n\n\n7\nGrace\nTaylor\nHR\n62000\n\n\n16\nPaul\nYoung\nHR\n61000\n\n\n1\nAlice\nSmith\nHR\n60000\n\n\n10\nJack\nWhite\nHR\n58000\n\n\n3\nCharlie\nLee\nSales\n55000\n\n\n9\nIrene\nThomas\nSales\n53000\n\n\n18\nRachel\nScott\nSales\n57000\n\n\n12\nLeo\nMartin\nSales\n50000\n\n\n\n\n\nAND, OR, NOT\nLogical operators filter records based on multiple conditions.\n-- Using AND, OR, NOT operators\np = \"\"\"\n\nSELECT * \nFROM \n  employees \nWHERE \n  department = 'Sales' AND salary &gt; 50000;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\ndepartment\nsalary\n\n\n\n\n6\nFrank\nWilson\nSales\n70000\n\n\n15\nOlivia\nPerez\nSales\n68000\n\n\n18\nRachel\nScott\nSales\n57000\n\n\n\n\n\nNumeric Operations\nPerform arithmetic operations in SQL.\n-- Calculating a new column\np = \"\"\"\n\nSELECT \n  first_name, \n  last_name, \n  salary, \n  salary * 1.1 AS new_salary \nFROM \n  employees;\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nfirst_name\nlast_name\nsalary\nnew_salary\n\n\n\n\nAlice\nSmith\n60000\n66000.0\n\n\nBob\nJohnson\n80000\n88000.0\n\n\nCharlie\nLee\n55000\n60500.0\n\n\nDavid\nKim\n75000\n82500.0\n\n\nEva\nBrown\n65000\n71500.0\n\n\nFrank\nWilson\n70000\n77000.0\n\n\nGrace\nTaylor\n62000\n68200.0\n\n\nHenry\nAnderson\n77000\n84700.0\n\n\nIrene\nThomas\n53000\n58300.0\n\n\nJack\nWhite\n58000\n63800.0\n\n\nKaren\nHarris\n69000\n75900.0\n\n\nLeo\nMartin\n50000\n55000.0\n\n\nMia\nJackson\n64000\n70400.0\n\n\nNoah\nLee\n72000\n79200.0\n\n\nOlivia\nPerez\n68000\n74800.0\n\n\nPaul\nYoung\n61000\n67100.0\n\n\nQuinn\nKing\n76000\n83600.0\n\n\nRachel\nScott\n57000\n62700.0\n\n\nSam\nGreen\n63000\n69300.0\n\n\nTina\nAdams\n81000\n89100.0\n\n\n\n\n\nLIKE and NOT LIKE\nPattern matching using LIKE.\n-- Pattern matching\np = \"\"\"\n\nSELECT * \nFROM \n  employees \nWHERE \n  last_name \nLIKE 'S%';\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\ndepartment\nsalary\n\n\n\n\n1\nAlice\nSmith\nHR\n60000\n\n\n18\nRachel\nScott\nSales\n57000\n\n\n\n\n\nBETWEEN\nRange filtering using BETWEEN.\n-- Filtering within a range\np = \"\"\"\n\nSELECT * \nFROM \n  employees \nWHERE \n  salary BETWEEN 40000 AND 60000;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\ndepartment\nsalary\n\n\n\n\n12\nLeo\nMartin\nSales\n50000\n\n\n3\nCharlie\nLee\nSales\n55000\n\n\n9\nIrene\nThomas\nSales\n53000\n\n\n10\nJack\nWhite\nHR\n58000\n\n\n1\nAlice\nSmith\nHR\n60000\n\n\n\n\n\nOFFSET\nSkip a specific number of rows before starting to return rows.\n-- Skipping the first 5 rows\np = \"\"\"\n\nSELECT * \nFROM \n  employees \nOFFSET 5;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nid\nfirst_name\nlast_name\ndepartment\nsalary\n\n\n\n\n6\nFrank\nWilson\nSales\n70000\n\n\n7\nGrace\nTaylor\nHR\n62000\n\n\n8\nHenry\nAnderson\nIT\n77000\n\n\n9\nIrene\nThomas\nSales\n53000\n\n\n10\nJack\nWhite\nHR\n58000\n\n\n11\nKaren\nHarris\nIT\n69000\n\n\n12\nLeo\nMartin\nSales\n50000\n\n\n13\nMia\nJackson\nHR\n64000\n\n\n14\nNoah\nLee\nIT\n72000\n\n\n15\nOlivia\nPerez\nSales\n68000\n\n\n16\nPaul\nYoung\nHR\n61000\n\n\n17\nQuinn\nKing\nIT\n76000\n\n\n18\nRachel\nScott\nSales\n57000\n\n\n19\nSam\nGreen\nHR\n63000\n\n\n20\nTina\nAdams\nIT\n81000"
  },
  {
    "objectID": "Course Materials/sql_page.html#intermediate-sql-concepts",
    "href": "Course Materials/sql_page.html#intermediate-sql-concepts",
    "title": "Introduction",
    "section": "Intermediate SQL Concepts",
    "text": "Intermediate SQL Concepts\n\nJoins\nTable: employees\n\n\n\nid\nfirst_name\nlast_name\ndepartment_id\nsalary\n\n\n\n\n1\nAlice\nSmith\n1\n60000\n\n\n2\nBob\nJohnson\n2\n80000\n\n\n3\nCharlie\nLee\n3\n55000\n\n\n4\nDavid\nKim\n1\n75000\n\n\n5\nEva\nBrown\n2\n65000\n\n\n6\nFrank\nWilson\n3\n70000\n\n\n7\nGrace\nTaylor\n1\n62000\n\n\n8\nHenry\nAnderson\n2\n77000\n\n\n9\nIrene\nThomas\n3\n53000\n\n\n10\nJack\nWhite\n1\n58000\n\n\n11\nKaren\nHarris\n2\n69000\n\n\n12\nLeo\nMartin\n3\n50000\n\n\n13\nMia\nJackson\n1\n64000\n\n\n14\nNoah\nLee\n2\n72000\n\n\n15\nOlivia\nPerez\n3\n68000\n\n\n16\nPaul\nYoung\n1\n61000\n\n\n17\nQuinn\nKing\n2\n76000\n\n\n18\nRachel\nScott\n3\n57000\n\n\n19\nSam\nGreen\n1\n63000\n\n\n20\nTina\nAdams\n2\n81000\n\n\n\nTable: departments\n\n\n\ndepartment_id\ndepartment_name\n\n\n\n\n1\nHR\n\n\n2\nIT\n\n\n3\nSales\n\n\n\nCombine rows from two or more tables based on a related column.\n-- Inner join example\np = \"\"\"\n\nSELECT \n  employees.first_name, \n    employees.last_name, \n    departments.department_name\nFROM \n  employees\nINNER JOIN \n  departments ON employees.department_id = departments.department_id;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nfirst_name\nlast_name\ndepartment_name\n\n\n\n\nAlice\nSmith\nHR\n\n\nDavid\nKim\nHR\n\n\nGrace\nTaylor\nHR\n\n\nJack\nWhite\nHR\n\n\nMia\nJackson\nHR\n\n\nPaul\nYoung\nHR\n\n\nSam\nGreen\nHR\n\n\nBob\nJohnson\nIT\n\n\nEva\nBrown\nIT\n\n\nHenry\nAnderson\nIT\n\n\nKaren\nHarris\nIT\n\n\nNoah\nLee\nIT\n\n\nQuinn\nKing\nIT\n\n\nTina\nAdams\nIT\n\n\nCharlie\nLee\nSales\n\n\nFrank\nWilson\nSales\n\n\nIrene\nThomas\nSales\n\n\nLeo\nMartin\nSales\n\n\nOlivia\nPerez\nSales\n\n\nRachel\nScott\nSales\n\n\n\n\n\nCAST\nConvert data from one type to another.\n-- Casting a column\np = \"\"\"\n\nSELECT \n  CAST(salary AS DECIMAL(10, 2)) \nFROM \n  employees;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nsalary\n\n\n\n\n60000.00\n\n\n80000.00\n\n\n55000.00\n\n\n75000.00\n\n\n65000.00\n\n\n70000.00\n\n\n62000.00\n\n\n77000.00\n\n\n53000.00\n\n\n58000.00\n\n\n69000.00\n\n\n50000.00\n\n\n64000.00\n\n\n72000.00\n\n\n68000.00\n\n\n61000.00\n\n\n76000.00\n\n\n57000.00\n\n\n63000.00\n\n\n81000.00\n\n\n\n\n\nAggregations\nPerform calculations on a set of values.\n-- Using aggregation functions\np = \"\"\"\n\nSELECT \n  department_id, \n  COUNT(employee_id) AS num_employees\nFROM \n  employees\nGROUP BY \n  department_id;\n\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\ndepartment_id\nnum_employees\n\n\n\n\n1\n7\n\n\n2\n7\n\n\n3\n6\n\n\n\n\n\nGROUP BY and HAVING\nGroup rows that have the same values and filter groups.\n-- Grouping rows and filtering groups\np = \"\"\"\nSELECT \n  department_id,\n  COUNT(employee_id) AS num_employees\nFROM \n  employees\nGROUP BY \n  department_id\nHAVING COUNT(employee_id) &gt; 5;\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\ndepartment_id\nnum_employees\n\n\n\n\n1\n7\n\n\n2\n7\n\n\n3\n6\n\n\n\n\n\nUNION, INTERSECT, MINUS\nCombine result sets\nTable: managers\n\n\n\nmanager_id\nfirst_name\nlast_name\n\n\n\n\n1\nMichael\nBrown\n\n\n2\nSarah\nJohnson\n\n\n3\nJohn\nLee\n\n\n\n-- Union example\np = \"\"\"\nSELECT \n  first_name \nFROM \n  employees\nUNION\n\nSELECT \n  first_name\nFROM\n   managers;\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nfirst_name\n\n\n\n\nAlice\n\n\nBob\n\n\nCharlie\n\n\nDavid\n\n\nEva\n\n\nFrank\n\n\nGrace\n\n\nHenry\n\n\nIrene\n\n\nJack\n\n\nKaren\n\n\nLeo\n\n\nMia\n\n\nNoah\n\n\nOlivia\n\n\nPaul\n\n\nQuinn\n\n\nRachel\n\n\nSam\n\n\nTina\n\n\nMichael\n\n\nSarah\n\n\nJohn\n\n\n\n\n\nPOSITION\nFind the position of a substring.\n-- Finding substring position\np = \"\"\"\nSELECT \n  POSITION('e' IN first_name) \nFROM \n  employees;\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nposition\n\n\n\n\n0\n\n\n0\n\n\n4\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n3\n\n\n2\n\n\n0\n\n\n0\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n\n\n\nCASE\nConditional logic in `SQL.\n-- Using CASE statements\np = \"\"\"\nSELECT \n  first_name, \n  last_name,\n       CASE \n         WHEN salary &gt; 60000 THEN 'High'\n         WHEN salary BETWEEN 40000 AND 60000 THEN 'Medium'\n         ELSE 'Low'\n       END AS salary_category\nFROM \n  employees;\n\"\"\"\n\npd.read_sql_query(p, con)\n\n\n\nfirst_name\nlast_name\nsalary_category\n\n\n\n\nAlice\nSmith\nMedium\n\n\nBob\nJohnson\nHigh\n\n\nCharlie\nLee\nMedium\n\n\nDavid\nKim\nHigh\n\n\nEva\nBrown\nMedium\n\n\nFrank\nWilson\nHigh\n\n\nGrace\nTaylor\nMedium\n\n\nHenry\nAnderson\nHigh\n\n\nIrene\nThomas\nMedium\n\n\nJack\nWhite\nMedium\n\n\nKaren\nHarris\nHigh\n\n\nLeo\nMartin\nMedium\n\n\nMia\nJackson\nMedium\n\n\nNoah\nLee\nHigh\n\n\nOlivia\nPerez\nHigh\n\n\nPaul\nYoung\nMedium\n\n\nQuinn\nKing\nHigh\n\n\nRachel\nScott\nMedium\n\n\nSam\nGreen\nMedium\n\n\nTina\nAdams\nHigh"
  },
  {
    "objectID": "Course Materials/sql_page.html#introduction",
    "href": "Course Materials/sql_page.html#introduction",
    "title": "Introduction",
    "section": "Introduction",
    "text": "Introduction\nIt’s rare that a data analysis involves only a single data frame. Typically you have many data frames, and you must join them together to answer the questions that you’re interested in.\npandas has a really rich set of options for combining one or more data frames, with the two most important being concatenate and merge. Some of the examples in this chapter show you how to join a pair of data frames. Fortunately this is enough, since you can combine three data frames by combining two pairs.\n\n# remove cell\nimport matplotlib_inline.backend_inline\nimport matplotlib.pyplot as plt\n\n# Plot settings\nplt.style.use(\"https://github.com/aeturrell/python4DS/raw/main/plot_style.txt\")\nmatplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\")\n\n\nPrerequisites\nThis chapter will use the pandas data analysis package."
  },
  {
    "objectID": "Course Materials/sql_page.html#concatenate",
    "href": "Course Materials/sql_page.html#concatenate",
    "title": "Introduction",
    "section": "Concatenate",
    "text": "Concatenate\nIf you have two or more data frames with the same index or the same columns, you can glue them together into a single data frame using pd.concat().\n\nFor the same columns, pass axis=0 to glue the index together; for the same index, pass axis=1 to glue the columns together. The concatenate function will typically be used on a list of data frames.\nIf you want to track where the original data came from in the final data frame, use the keys keyword.\nHere’s an example using data on two different states’ populations that also makes uses of the keys option:\n\nimport pandas as pd\nimport urllib.request\n\nbase_url = \"http://www.stata-press.com/data/r14/\"\nstate_codes = [\"ca\", \"il\"]\nend_url = \"pop.dta\"\nheaders = {'User-Agent': 'Mozilla/5.0'}\n\ndef fetch_data(url):\n    req = urllib.request.Request(url, headers=headers)\n    with urllib.request.urlopen(req) as response:\n        return pd.read_stata(response)\n\n# This grabs the two data frames, one for each state\nlist_of_state_dfs = [fetch_data(base_url + state + end_url) for state in state_codes]\n\n# Show example of first entry in list of data frames\nprint(list_of_state_dfs[0])\n\n        county      pop\n0  Los Angeles  9878554\n1       Orange  2997033\n2      Ventura   798364\n\n\n\n# Concatenate the list of data frames\ndf = pd.concat(list_of_state_dfs, keys=state_codes, axis=0)\n\nNote that the keys argument is optional, but is useful for keeping track of origin data frames within the merged data frame.\n\nExercise\nConcatenate the follow two data frames:\ndf1 = pd.DataFrame([['a', 1], ['b', 2]],\n                   columns=['letter', 'number'])\n\ndf2 = pd.DataFrame([['c', 3], ['d', 4]],\n                   columns=['letter', 'number'])\n\n\nMerge\nThere are so many options for merging data frames using pd.merge(left, right, on=..., how=... that we won’t be able to cover them all here. The most important features are: the two data frames to be merged, what variables (aka keys) to merge on (and these can be indexes) via on=, and how to do the merge (eg left, right, outer, inner) via how=. This diagram shows an example of a merge using keys from the left-hand data frame:\n\nThe how= keyword works in the following ways: - how='left' uses keys from the left data frame only to merge. - how='right' uses keys from the right data frame only to merge. - how='inner' uses keys that appear in both data frames to merge. - how='outer' uses the cartesian product of keys in both data frames to merge on.\nLet’s see examples of some of these:\n\nleft = pd.DataFrame(\n    {\n        \"key1\": [\"K0\", \"K0\", \"K1\", \"K2\"],\n        \"key2\": [\"K0\", \"K1\", \"K0\", \"K1\"],\n        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n    }\n)\nright = pd.DataFrame(\n    {\n        \"key1\": [\"K0\", \"K1\", \"K1\", \"K2\"],\n        \"key2\": [\"K0\", \"K0\", \"K0\", \"K0\"],\n        \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n        \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n    }\n)\n# Right merge\npd.merge(left, right, on=[\"key1\", \"key2\"], how=\"right\")\n\n\n\n\n\n\n\n\n\nkey1\nkey2\nA\nB\nC\nD\n\n\n\n\n0\nK0\nK0\nA0\nB0\nC0\nD0\n\n\n1\nK1\nK0\nA2\nB2\nC1\nD1\n\n\n2\nK1\nK0\nA2\nB2\nC2\nD2\n\n\n3\nK2\nK0\nNaN\nNaN\nC3\nD3\n\n\n\n\n\n\n\n\nNote that the key combination of K2 and K0 did not exist in the left-hand data frame, and so its entries in the final data frame are NaNs. But it does have entries because we chose the keys from the right-hand data frame.\nWhat about an inner merge?\n\npd.merge(left, right, on=[\"key1\", \"key2\"], how=\"inner\")\n\n\n\n\n\n\n\n\n\nkey1\nkey2\nA\nB\nC\nD\n\n\n\n\n0\nK0\nK0\nA0\nB0\nC0\nD0\n\n\n1\nK1\nK0\nA2\nB2\nC1\nD1\n\n\n2\nK1\nK0\nA2\nB2\nC2\nD2\n\n\n\n\n\n\n\n\nNow we see that the combination K2 and K0 are excluded because they didn’t exist in the overlap of keys in both data frames.\nFinally, let’s take a look at an outer merge that comes with some extra info via the indicator keyword:\n\npd.merge(left, right, on=[\"key1\", \"key2\"], how=\"outer\", indicator=True)\n\n\n\n\n\n\n\n\n\nkey1\nkey2\nA\nB\nC\nD\n_merge\n\n\n\n\n0\nK0\nK0\nA0\nB0\nC0\nD0\nboth\n\n\n1\nK0\nK1\nA1\nB1\nNaN\nNaN\nleft_only\n\n\n2\nK1\nK0\nA2\nB2\nC1\nD1\nboth\n\n\n3\nK1\nK0\nA2\nB2\nC2\nD2\nboth\n\n\n4\nK2\nK1\nA3\nB3\nNaN\nNaN\nleft_only\n\n\n5\nK2\nK0\nNaN\nNaN\nC3\nD3\nright_only\n\n\n\n\n\n\n\n\nNow we can see that the products of all key combinations are here. The indicator=True option has caused an extra column to be added, called ’_merge’, that tells us which data frame the keys on that row came from.\n\n\nExercise\nMerge the following two data frames using the left_on and right_on keyword arguments to specify a join on lkey and rkey respectively:\ndf1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n                    'value': [1, 2, 3, 5]})\ndf2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n                    'value': [5, 6, 7, 8]})\n\n\nExercise\nMerge the following two data frames on \"a\" using how=\"left\" as a keyword argument:\ndf1 = pd.DataFrame({'a': ['foo', 'bar'], 'b': [1, 2]})\ndf2 = pd.DataFrame({'a': ['foo', 'baz'], 'c': [3, 4]})\nWhat do you notice about the position .loc[1, \"c\"] in the merged data frame?\nFor more on the options for merging, see pandas’ comprehensive merging documentation."
  },
  {
    "objectID": "Course Materials/sql_page.html#introduction-to-sql-joins",
    "href": "Course Materials/sql_page.html#introduction-to-sql-joins",
    "title": "Introduction",
    "section": "Introduction to SQL Joins",
    "text": "Introduction to SQL Joins\nSQL joins are used to combine rows from two or more tables based on a related column between them. Understanding joins is crucial for effective data retrieval and manipulation. This section will cover the four main types of joins: INNER JOIN, LEFT JOIN, RIGHT JOIN, and FULL JOIN, using visual aids and code snippets with a fake data frame."
  },
  {
    "objectID": "Course Materials/sql_page.html#types-of-joins",
    "href": "Course Materials/sql_page.html#types-of-joins",
    "title": "Introduction",
    "section": "Types of Joins",
    "text": "Types of Joins\n\nINNER JOIN\nAn INNER JOIN returns only the rows that have matching values in both tables.\n\n\n\nINNER JOIN\n\n\n-- Inner join example\np = \"\"\"\nSELECT \n  A.id, \n  A.name, \n  B.order_id\nFROM \n  Customers A\nINNER JOIN \n  Orders B ON A.id = B.customer_id;\n\"\"\"\npd.read_sql_query(p, con)\n\n\nLEFT JOIN\nA LEFT JOIN returns all the rows from the left table and the matched rows from the right table. If no match is found, NULL values are returned for columns from the right table.\n\n\n\nLEFT JOIN\n\n\n-- Left join example\np = \"\"\"\nSELECT \n  A.id, \n  A.name, \n  B.order_id\nFROM \n  Customers A\nLEFT JOIN \n  Orders B ON A.id = B.customer_id;\n\"\"\"\npd.read_sql_query(p, con)\n\n\nRIGHT JOIN\nA RIGHT JOIN returns all the rows from the right table and the matched rows from the left table. If no match is found, NULL values are returned for columns from the left table.\n\n\n\nRIGHT JOIN\n\n\np = \"\"\"\nSELECT \n  A.id, \n  A.name, \n  B.order_id\nFROM \n  Customers A\nRIGHT JOIN \n  Orders B ON A.id = B.customer_id;\n\"\"\"\npd.read_sql_query(p, con)\n\n\nFULL JOIN\nA FULL JOIN returns all the rows when there is a match in either left or right table. Rows without a match in one of the tables will contain NULL values for columns from that table.\n\n\n\nFULL JOIN\n\n\n-- Full join example\np = \"\"\"\nSELECT \n  A.id, \n  A.name, \n  B.order_id\nFROM \n  Customers A\nFULL JOIN \n  Orders B ON A.id = B.customer_id;\n\"\"\"\npd.read_sql_query(p, con)"
  },
  {
    "objectID": "Course Materials/sql_page.html#example-data-frames",
    "href": "Course Materials/sql_page.html#example-data-frames",
    "title": "Introduction",
    "section": "Example Data Frames",
    "text": "Example Data Frames\nTo illustrate these joins, let’s consider two fake data frames: Customers and Orders.\n\nimport pandas as pd\n\n# Creating a fake data frame for Customers\ncustomers = pd.DataFrame({\n    'id': [1, 2, 3, 4],\n    'name': ['Alice', 'Bob', 'Charlie', 'David']\n})\n\n# Creating a fake data frame for Orders\norders = pd.DataFrame({\n    'order_id': [101, 102, 103, 104],\n    'customer_id': [1, 2, 2, 4]\n})\n\n# Display the data frames\nprint(\"Customers Data Frame\")\nprint(customers)\n\nCustomers Data Frame\n   id     name\n0   1    Alice\n1   2      Bob\n2   3  Charlie\n3   4    David\n\n\n\nprint(\"Orders Data Frame\")\nprint(orders)\n\nOrders Data Frame\n   order_id  customer_id\n0       101            1\n1       102            2\n2       103            2\n3       104            4"
  },
  {
    "objectID": "Course Materials/sql_page.html#conclusion",
    "href": "Course Materials/sql_page.html#conclusion",
    "title": "Introduction",
    "section": "Conclusion",
    "text": "Conclusion\nSQL is an indispensable tool for anyone working with data. Its ability to manage, manipulate, and integrate data makes it essential for data analysis and decision-making. The examples provided in this chapter illustrate some of the key operations and techniques used in SQL, highlighting its importance in the field of data science."
  },
  {
    "objectID": "Course Materials/python.html",
    "href": "Course Materials/python.html",
    "title": "Python for Data Science",
    "section": "",
    "text": "Python for Data Science python4DS is a port of R for Data Science (2e) into Python. The authors keep Garrett Grolemund and Hadley Wickham’s writing and examples as much as possible while demonstrating Python instead of R. The book focuses on pandas and LetsPlot in the Python code snippets.\nThis book will teach you how to do data science with Python: You’ll learn how to get your data into Python, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you’ll learn how to clean data and draw plots—and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with Python. You’ll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You’ll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.\n\nLearn How to Setup Python\nPython\n\n\n\n\n Back to top",
    "crumbs": [
      "Materials",
      "Python for Data Science"
    ]
  },
  {
    "objectID": "Course Materials/ml.html",
    "href": "Course Materials/ml.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Everyone seems to have a slightly different take on the differences between Artificial Intelligence, Machine Learning, and Data Science. The following four articles cover some of the most common definitions.\nAs you read them, think about the differences and similarities of the definitions. Given the backgrounds of the various authors, whose opinions might you give more weight to?\n\nMichael Copeland writing for NVidia\nBernard Marr writing for Forbes\nVincent Granville writing for Data Science Central\nSimply Statistics Blog - The key word in “Data Science” is not Data, it is Science\n\nOf particular note is this quote from the Granville article:\n\nEarlier in my career (circa 1990) I worked on image remote sensing technology, among other things to identify patterns (or shapes or features, for instance lakes) in satellite images and to perform image segmentation: at that time my research was labeled as computational statistics, but the people doing the exact same thing in the computer science department next door in my home university, called their research artificial intelligence. Today, it would be called data science or artificial intelligence, the sub-domains being signal processing, computer vision or IoT.\n\nAs with most things in the realm of science, there tends to be a wide gap between how the media, government, and business sectors view a particular technology compared to how it’s viewed by the engineers and scientists using that technology.\nFor our purposes in this course, we’ll define these terms as follows:\n\nArtificial Intelligence: The study of man-made “agents” that perceive their environment and take actions that maximize their chances of success at some goal.1\nMachine Learning: A subfield within Artificial Intelligence that gives “computers the ability to learn without being explicitly programmed.”2\nData Science: The study and use of the techniques, statistics, algorithms, and tools needed to extract knowledge and insights from data.3",
    "crumbs": [
      "Materials",
      "Machine Learning (ML)"
    ]
  },
  {
    "objectID": "Course Materials/ml.html#footnotes",
    "href": "Course Materials/ml.html#footnotes",
    "title": "Machine Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nArtificial Intelligence: A Modern Approach by Russell and Norvig (Prentice Hall, 2009).↩︎↩︎\nSome Studies in Machine Learning Using the Game of Checkers, by Arthur L. Samuel (IBM Journal, Vol 3, No 3, 1959).↩︎↩︎\nWikipedia article on Data Science.↩︎↩︎\nMind Children, by Hans Moravec (Harvard University Press, 1988).↩︎↩︎\nXKCD 1425: Tasks.↩︎↩︎\nCambridge Alumni Magazine, Issue 79, pg 19.↩︎↩︎\nCambridge Alumni Magazine, Issue 79, pg 19.↩︎↩︎\nCross Validated: Prediction vs Inference.↩︎↩︎\nCross Validated: Prediction vs Inference.↩︎↩︎",
    "crumbs": [
      "Materials",
      "Machine Learning (ML)"
    ]
  },
  {
    "objectID": "Course Materials/data_visualise_plotly.html",
    "href": "Course Materials/data_visualise_plotly.html",
    "title": "Data Visualization",
    "section": "",
    "text": "“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\nThis chapter will teach you how to visualize your data using Plotly.\nThere are broadly two categories of approach to using code to create data visualizations: imperative, where you build what you want, and declarative, where you say what you want. Choosing which to use involves a trade-off: imperative libraries offer you flexibility but at the cost of some verbosity; declarative libraries offer you a quick way to plot your data, but only if it’s in the right format to begin with, and customization to special chart types is more difficult. Python has many excellent plotting packages, including perhaps the most powerful imperative plotting package around, matplotlib.\nHowever, we’ll get further faster by learning one system and applying it in many places—and the beauty of declarative plotting is that it covers lots of standard charts simply and well. Plotly implements an imperative system with interactive features, making it suitable for a wide range of visualization needs.\nWe will start by creating a simple scatterplot and use that to introduce aesthetic mappings and geometric objects—the fundamental building blocks of Plotly. We will then walk you through visualizing distributions of single variables as well as visualizing relationships between two or more variables. We’ll finish off with saving your plots and troubleshooting tips.\n\n\nYou will need to install the plotly package for this chapter. To do this, open up the command line of your computer, type in pip install plotly, and hit enter.\nThe command line can be opened within Visual Studio Code and Codespaces by going to View -&gt; Terminal.\nNote that you only need to install a package once in each Python environment.\nWe’ll also need to have the pandas package installed—this package, which we’ll be seeing a lot of, is for data. You can similarly install it by running pip install pandas on the command line.\nFinally, we’ll also need some data (you can’t science without data). We’ll be using the Palmer penguins dataset. Unusually, this can also be installed as a package—normally you would load data from a file, but these data are so popular for tutorials they’ve found their way into an installable package. Run pip install palmerpenguins to get these data.\nOur next task is to load these into our Python session, either in a Python notebook cell within a Jupyter Notebook, by writing it in a script that we then send to the interactive window, or by typing it directly into the interactive window and hitting shift and enter. Here’s the code:\n\nimport pandas as pd\nimport plotly.express as px\nfrom palmerpenguins import load_penguins\nfrom numpy import polyfit\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport pandas as pd\nfrom numpy import polyfit\n\nimport numpy as np\n\nThese lines import parts of the pandas and palmerpenguins packages, then import the plotly.express module as px for creating plots."
  },
  {
    "objectID": "Course Materials/data_visualise_plotly.html#introduction",
    "href": "Course Materials/data_visualise_plotly.html#introduction",
    "title": "Data Visualization",
    "section": "",
    "text": "“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\nThis chapter will teach you how to visualize your data using Plotly.\nThere are broadly two categories of approach to using code to create data visualizations: imperative, where you build what you want, and declarative, where you say what you want. Choosing which to use involves a trade-off: imperative libraries offer you flexibility but at the cost of some verbosity; declarative libraries offer you a quick way to plot your data, but only if it’s in the right format to begin with, and customization to special chart types is more difficult. Python has many excellent plotting packages, including perhaps the most powerful imperative plotting package around, matplotlib.\nHowever, we’ll get further faster by learning one system and applying it in many places—and the beauty of declarative plotting is that it covers lots of standard charts simply and well. Plotly implements an imperative system with interactive features, making it suitable for a wide range of visualization needs.\nWe will start by creating a simple scatterplot and use that to introduce aesthetic mappings and geometric objects—the fundamental building blocks of Plotly. We will then walk you through visualizing distributions of single variables as well as visualizing relationships between two or more variables. We’ll finish off with saving your plots and troubleshooting tips.\n\n\nYou will need to install the plotly package for this chapter. To do this, open up the command line of your computer, type in pip install plotly, and hit enter.\nThe command line can be opened within Visual Studio Code and Codespaces by going to View -&gt; Terminal.\nNote that you only need to install a package once in each Python environment.\nWe’ll also need to have the pandas package installed—this package, which we’ll be seeing a lot of, is for data. You can similarly install it by running pip install pandas on the command line.\nFinally, we’ll also need some data (you can’t science without data). We’ll be using the Palmer penguins dataset. Unusually, this can also be installed as a package—normally you would load data from a file, but these data are so popular for tutorials they’ve found their way into an installable package. Run pip install palmerpenguins to get these data.\nOur next task is to load these into our Python session, either in a Python notebook cell within a Jupyter Notebook, by writing it in a script that we then send to the interactive window, or by typing it directly into the interactive window and hitting shift and enter. Here’s the code:\n\nimport pandas as pd\nimport plotly.express as px\nfrom palmerpenguins import load_penguins\nfrom numpy import polyfit\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport pandas as pd\nfrom numpy import polyfit\n\nimport numpy as np\n\nThese lines import parts of the pandas and palmerpenguins packages, then import the plotly.express module as px for creating plots."
  },
  {
    "objectID": "Course Materials/data_visualise_plotly.html#first-steps",
    "href": "Course Materials/data_visualise_plotly.html#first-steps",
    "title": "Data Visualization",
    "section": "First Steps",
    "text": "First Steps\nDo penguins with longer flippers weigh more or less than penguins with shorter flippers? You probably already have an answer, but try to make your answer precise. What does the relationship between flipper length and body mass look like? Is it positive? Negative? Linear? Nonlinear? Does the relationship vary by the species of the penguin? How about by the island where the penguin lives? Let’s create visualisations that we can use to answer these questions.\n\nThe penguins data frame\nYou can test your answers to those questions with the penguins data frame found in palmerpenguins (a.k.a. from palmerpenguins import load_penguins). A data frame is a rectangular collection of variables (in the columns) and observations (in the rows). penguins contains 344 observations collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER.{cite:p}horst2020palmerpenguins.\nTo make the discussion easier, let’s define some terms:\n\nA variable is a quantity, quality, or property that you can measure.\nA value is the state of a variable when you measure it. The value of a variable may change from measurement to measurement.\nAn observation is a set of measurements made under similar conditions (you usually make all of the measurements in an observation at the same time and on the same object). An observation will contain several values, each associated with a different variable. We’ll sometimes refer to an observation as a data point.\nTabular data is a set of values, each associated with a variable and an observation. Tabular data is tidy if each value is placed in its own “cell”, each variable in its own column, and each observation in its own row.\n\nIn this context, a variable refers to an attribute of all the penguins, and an observation refers to all the attributes of a single penguin.\nType the name of the data frame in the interactive window and Python will print a preview of its contents.\n\npenguins = load_penguins()\npenguins\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n339\nChinstrap\nDream\n55.8\n19.8\n207.0\n4000.0\nmale\n2009\n\n\n340\nChinstrap\nDream\n43.5\n18.1\n202.0\n3400.0\nfemale\n2009\n\n\n341\nChinstrap\nDream\n49.6\n18.2\n193.0\n3775.0\nmale\n2009\n\n\n342\nChinstrap\nDream\n50.8\n19.0\n210.0\n4100.0\nmale\n2009\n\n\n343\nChinstrap\nDream\n50.2\n18.7\n198.0\n3775.0\nfemale\n2009\n\n\n\n\n344 rows × 8 columns\n\n\n\n\nFor an alternative view, where you can see the first few observations of each variable, use penguins.head().\n\npenguins.head()\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\nAmong the variables in penguins are:\n\nspecies: a penguin’s species (Adelie, Chinstrap, or Gentoo).\nflipper_length_mm: length of a penguin’s flipper, in millimeters.\nbody_mass_g: body mass of a penguin, in grams.\n\nTo learn more about penguins, open the help page of its data-loading function by running help(load_penguins).\n\n\nUltimate Goal\nOur ultimate goal in this chapter is to recreate the following visualisation displaying the relationship between flipper lengths and body masses of these penguins, taking into consideration the species of the penguin.\n\nscatter_fig = px.scatter(\n    penguins,\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n    color=\"species\",\n)\n\n\nfit = polyfit(penguins[\"flipper_length_mm\"].dropna(), penguins[\"body_mass_g\"].dropna(), 1)\ntrendline = fit[0] * penguins[\"flipper_length_mm\"] + fit[1]\n\n\ntrendline_fig = go.Scatter(\n    x=penguins[\"flipper_length_mm\"],\n    y=trendline,\n    mode='lines',\n    name='Trendline'\n)\n\n\nscatter_fig.add_trace(trendline_fig)\n\n\nscatter_fig.show()\n\n                                                \n\n\n\n\nCreating a Plot\nLet’s recreate this plot step-by-step.\nWith Plotly, you begin a plot by defining the data and specifying how you want it to be visualized.\nFirst, you need to import Plotly and your dataset. In this case, we’ll use the penguins dataset.\nimport plotly.express as px\nimport pandas as pd\n\n\npenguins = pd.read_csv(\"path_to_penguins_dataset.csv\")\nNext, we’ll create a basic scatter plot by mapping the flipper_length_mm to the x-axis and body_mass_g to the y-axis.\nfig = px.scatter(penguins, x='flipper_length_mm', y='body_mass_g', title='Scatterplot of Flipper Length vs. Body Mass')\n\n\nfig.show()\nIn this example, the px.scatter() function is used to create a scatter plot. The data_frame argument specifies the dataset to use, and the x and y arguments define the variables to be plotted on the x and y axes, respectively. The title argument adds a title to the plot.\nWe can add more details and customize the plot further by adding layers such as color, size, and other aesthetics.\nTo map the species variable to the color aesthetic:\nfig = px.scatter(penguins, x='flipper_length_mm', y='body_mass_g', color='species',\n                 title='Scatterplot of Flipper Length vs. Body Mass Colored by Species')\n\nfig.show()\n\nfig = px.scatter(\n    penguins,\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n)\n\nfig.show()\n\n                                                \n\n\nIn Plotly, the px.scatter() function handles both the data mapping and the creation of the plot, making it straightforward to create complex visualizations with minimal code.\nNow we have something that looks like what we might think of as a “scatterplot”. It doesn’t yet match our “ultimate goal” plot, but using this plot we can start answering the question that motivated our exploration: “What does the relationship between flipper length and body mass look like?” The relationship appears to be positive (as flipper length increases, so does body mass), fairly linear (the points are clustered around a line instead of a curve), and moderately strong (there isn’t too much scatter around such a line). Penguins with longer flippers are generally larger in terms of their body mass.\nIt’s a good point to flag that although we have plotted everything in the penguins data frame, there were a couple of rows with undefined values—and of course these cannot be plotted.\n\n\nAdding aesthetics and layers\nScatterplots are useful for displaying the relationship between two numerical variables, but it’s always a good idea to be skeptical of any apparent relationship and ask if other variables might explain or change the nature of this relationship. For example, does the relationship between flipper length and body mass differ by species?\nLet’s incorporate species into our plot to see if this reveals any additional insights into the apparent relationship between these variables. We will do this by representing species with different colored points.\nTo achieve this, we need to modify the aesthetic mapping in the Plotly plot:\n\nimport plotly.express as px\n\nfig = px.scatter(\n    penguins,\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n    color=\"species\",\n    title=\"Scatterplot of Flipper Length vs. Body Mass Colored by Species\"\n)\n\nfig.show()\n\n                                                \n\n\nWhen a categorical variable is mapped to an aesthetic, Plotly automatically assigns a unique value of the aesthetic (here a unique color) to each unique level of the variable (each of the three species), a process known as scaling. Plotly also adds a legend that explains which values correspond to which levels.\nNow let’s add one more layer: a smooth curve displaying the relationship between body mass and flipper length. To do this, we’ll use the trendline argument in Plotly to add a line of best fit based on a linear model.\n\nfig = px.scatter(\n    penguins,\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n    color=\"species\",\n    trendline=\"ols\",\n    title=\"Scatterplot with Trendline of Flipper Length vs. Body Mass\"\n)\n\nfig.show()\n\n                                                \n\n\nWe have successfully added lines, but this plot shows separate lines for each of the penguin species. If we want a single line for the entire dataset instead, we can remove the color argument from the plot.\n\nscatter_fig = px.scatter(\n    penguins,\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n    color=\"species\",\n)\n\n\nfit = polyfit(penguins[\"flipper_length_mm\"].dropna(), penguins[\"body_mass_g\"].dropna(), 1)\ntrendline = fit[0] * penguins[\"flipper_length_mm\"] + fit[1]\n\n\ntrendline_fig = go.Scatter(\n    x=penguins[\"flipper_length_mm\"],\n    y=trendline,\n    mode='lines',\n    name='Trendline'\n)\n\n\nscatter_fig.add_trace(trendline_fig)\n\n\nscatter_fig.show()\n\n                                                \n\n\nVoila! We have something that looks very much like our ultimate goal, though it’s not yet perfect.\nWe still need to use different shapes for each species of penguins and improve labels.\nIt’s generally not a good idea to represent information using only colors on a plot, as people perceive colors differently due to color blindness or other color vision differences. Therefore, in addition to color, we can also map species to the shape aesthetic.\n\nscatter_fig = px.scatter(\n    penguins,\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n    color=\"species\",\n    symbol=\"species\",\n)\n\n\nfit = polyfit(penguins[\"flipper_length_mm\"].dropna(), penguins[\"body_mass_g\"].dropna(), 1)\ntrendline = fit[0] * penguins[\"flipper_length_mm\"] + fit[1]\n\n\ntrendline_fig = go.Scatter(\n    x=penguins[\"flipper_length_mm\"],\n    y=trendline,\n    mode='lines',\n    name='Trendline'\n)\n\n\nscatter_fig.add_trace(trendline_fig)\n\n\nscatter_fig.show()\n\n                                                \n\n\nNote that the legend is automatically updated to reflect the different shapes of the points as well.\nAnd finally, we can improve the labels of our plot using the labs() function in a new layer. Some of the arguments to labs() might be self explanatory: title adds a title and subtitle adds a subtitle to the plot. Other arguments match the aesthetic mappings, x is the x-axis label, y is the y-axis label, and color and shape define the label for the legend.\n\nscatter_fig = px.scatter(\n    penguins,\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n    color=\"species\",\n    symbol=\"species\",\n    title=\"Body mass and flipper length\",\n    labels={\n        \"flipper_length_mm\": \"Flipper length (mm)\",\n        \"body_mass_g\": \"Body mass (g)\",\n        \"species\": \"Species\",\n    }\n)\n\nscatter_fig.update_layout(\n    title={\n        'text': \"Body mass and flipper length&lt;br&gt;&lt;sup&gt;Dimensions for Adelie, Chinstrap, and Gentoo Penguins&lt;/sup&gt;\",\n        'y':0.9,\n        'x':0.5,\n        'yanchor': 'top'\n    }\n)\n\n\nfit = polyfit(penguins[\"flipper_length_mm\"].dropna(), penguins[\"body_mass_g\"].dropna(), 1)\ntrendline = fit[0] * penguins[\"flipper_length_mm\"] + fit[1]\n\n\ntrendline_fig = go.Scatter(\n    x=penguins[\"flipper_length_mm\"],\n    y=trendline,\n    mode='lines',\n    name='Trendline'\n)\n\n\nscatter_fig.add_trace(trendline_fig)\n\n\nscatter_fig.show()\n\n                                                \n\n\nWe finally have a plot that perfectly matches our “ultimate goal”!\n\n\nExercises\n\nHow many rows are in penguins? How many columns?\nWhat does the bill_depth_mm variable in the penguins data frame describe? Read the help for ?penguins to find out.\nMake a scatterplot of bill_depth_mm vs. bill_length_mm. That is, make a scatterplot with bill_depth_mm on the y-axis and bill_length_mm on the x-axis. Describe the relationship between these two variables.\nWhat happens if you make a scatterplot of species vs. bill_depth_mm? What might be a better choice of geom?\nWhy does the following give an error and how would you fix it?\nfig = px.scatter(penguins x='bill_length_mm' y='bill_depth_mm')\nfig.show()\nAdd the following caption to the plot you made in the previous exercise: “Data come from the palmerpenguins package.” Hint: Take a look at the documentation for labs().\nRecreate the following visualization. What aesthetic should bill_depth_mm be mapped to? And should it be mapped at the global level or at the geom level?\n\n\nfrom scipy.stats import linregress\n\n\nscatter_fig = px.scatter(\n    penguins,\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n    color=\"bill_depth_mm\",\n    color_continuous_scale=px.colors.sequential.Viridis\n)\n\n\nslope, intercept, r_value, p_value, std_err = linregress(penguins[\"flipper_length_mm\"].dropna(), penguins[\"body_mass_g\"].dropna())\ntrendline = slope * penguins[\"flipper_length_mm\"] + intercept\n\ntrendline_fig = go.Scatter(\n    x=penguins[\"flipper_length_mm\"],\n    y=trendline,\n    mode='lines',\n    name='Trendline',\n    line=dict(color='red')\n)\n\n\nscatter_fig.add_trace(trendline_fig)\n\nscatter_fig.show()"
  },
  {
    "objectID": "Course Materials/data_visualise_plotly.html#a-categorical-variable",
    "href": "Course Materials/data_visualise_plotly.html#a-categorical-variable",
    "title": "Data Visualization",
    "section": "A categorical variable",
    "text": "A categorical variable\nA variable is categorical if it can only take one of a small set of values. To examine the distribution of a categorical variable, you can use a bar chart. The height of the bars displays how many observations occurred with each x value.\n\nbar_fig = px.bar(\n    penguins,\n    x=\"species\",\n)\n\nbar_fig.update_layout(\n    plot_bgcolor='white',\n    paper_bgcolor='white',\n    xaxis=dict(showgrid=True, gridcolor='white'),\n    yaxis=dict(showgrid=True, gridcolor='white')\n)\n\nbar_fig.show()\n\n                                                \n\n\nYou may have seen earlier that the data type of the \"species\" column is string. Ideally, we want it to be categorical, so that there’s no confusion about the fact that we’re dealing with a finite number of mutually exclusive groups here. Another advantage is that it allows plotting tools to realise what kind of data it is working with.\nWe can transform the variable to a categorical variable using pandas like so:\n\npenguins[\"species\"] = penguins[\"species\"].astype(\"category\")\npenguins.head()\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\nYou will learn more about categorical variables later in the book.\n\nA numerical variable\nA variable is numerical (or quantitative) if it can take on a wide range of numerical values, and it is sensible to add, subtract, or take averages with those values. Numerical variables can be continuous or discrete.\nOne commonly used visualisation for distributions of continuous variables is a histogram.\n\nhist_fig = px.histogram(\n    penguins,\n    x=\"body_mass_g\"\n)\n\n\n\n\nhist_fig.show()\n\n                                                \n\n\nA histogram divides the x-axis into equally spaced bins and then uses the height of a bar to display the number of observations that fall in each bin. In the graph above, the tallest bar shows that 39 observations have a body_mass_g value between 3,500 and 3,700 grams, which are the left and right edges of the bar.\nYou can set the width of the intervals in a histogram with the binwidth argument, which is measured in the units of the x variable. You should always explore a variety of binwidths when working with histograms, as different binwidths can reveal different patterns. In the plots below a binwidth of 20 is too narrow, resulting in too many bars, making it difficult to determine the shape of the distribution. Similarly, a binwidth of 2,000 is too high, resulting in all data being binned into only three bars, and also making it difficult to determine the shape of the distribution. A binwidth of 200 provides a sensible balance, but you should always look at your data a few different ways, especially with histograms as they can be misleading.\n\nfrom scipy.stats import gaussian_kde\n\nbody_mass = penguins['body_mass_g'].dropna()\n\ndensity = gaussian_kde(body_mass)\nx = np.linspace(body_mass.min(), body_mass.max(), 1000)\ny = density(x)\n\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=x,\n    y=y,\n    mode='lines',\n    fill='tozeroy',\n    name='Density'\n))\n\nfig.show()\n\n                                                \n\n\n\n\nExercises\n\nMake a bar plot of \"species\" of penguins, where you assign \"species\" to the y aesthetic. How is this plot different?\nimport plotly.express as px\nfig = px.bar(penguins, x='species', y='count')\nfig.show()\nHow are the following two plots different? Which aesthetic, color or fill, is more useful for changing the color of bars?\nfig1 = px.bar(penguins, x='species', color_discrete_sequence=['red'])\nfig1.show()\n\nfig2 = px.bar(penguins, x='species', color='species', color_discrete_map={'Adelie':'red', 'Chinstrap':'red', 'Gentoo':'red'})\nfig2.show()\nWhat does the nbins argument in px.histogram() do?\nfig = px.histogram(penguins, x='body_mass_g', nbins=30)\nfig.show()"
  },
  {
    "objectID": "Course Materials/data_visualise_plotly.html#visualising-relationships",
    "href": "Course Materials/data_visualise_plotly.html#visualising-relationships",
    "title": "Data Visualization",
    "section": "Visualising Relationships",
    "text": "Visualising Relationships\nTo visualise a relationship we need to have at least two variables mapped to aesthetics of a plot—though you should remember that correlation is not causation, and causation is not correlation!\nIn the following sections you will learn about commonly used plots for visualising relationships between two or more variables and the geoms used for creating them.\nA Numerical and a Categorical Variable To visualize the relationship between a numerical and a categorical variable, we can use side-by-side box plots.\nA boxplot is a type of visual shorthand for measures of position within a distribution (percentiles).\nIt is also useful for identifying potential outliers. Each boxplot consists of:\nA box that indicates the range of the middle half of the data, a distance known as the interquartile range (IQR), stretching from the 25th percentile of the distribution to the 75th percentile. In the middle of the box is a line that displays the median, i.e. the 50th percentile, of the distribution. These three lines give you a sense of the spread of the distribution and whether or not the distribution is symmetric about the median or skewed to one side.\nVisual points that display observations that fall more than 1.5 times the IQR from either edge of the box. These outlying points are unusual, so they are plotted individually.\nA line (or whisker) that extends from each end of the box and goes to the farthest non-outlier point in the distribution.\nLet’s take a look at the distribution of body mass by species using px.box():\n\nfig = px.box(\n    penguins,\n    x=\"species\",\n    y=\"body_mass_g\",\n)\n\n\nfig.show()\n\n                                                \n\n\nAlternatively, we can make probability density plots with px.density_contour()\n\nfig = go.Figure()\n\ncolors = {\n    \"Adelie\": \"red\",\n    \"Chinstrap\": \"green\",\n    \"Gentoo\": \"blue\"\n}\n\nfor species in penguins['species'].unique():\n    species_data = penguins[penguins['species'] == species]['body_mass_g'].dropna()\n    density = gaussian_kde(species_data)\n    x = np.linspace(species_data.min(), species_data.max(), 1000)\n    y = density(x)\n    \n    fig.add_trace(go.Scatter(\n        x=x,\n        y=y,\n        mode='lines',\n        fill='tozeroy',\n        name= species,\n        line=dict(color=colors[species], width=2)\n    ))\n\n\nfig.show()\n\n                                                \n\n\nNote the terminology we have used here:\n\nWe map variables to aesthetics if we want the visual attribute represented by that aesthetic to vary based on the values of that variable.\nOtherwise, we set the value of an aesthetic.\n\n\nTwo categorical variables\nWe can use stacked bar plots to visualise the relationship between two categorical variables.\nFor example, the following two stacked bar plots both display the relationship between island and species, or specifically, visualising the distribution of species within each island.\nThe first plot shows the frequencies of each species of penguins on each island. The plot of frequencies show that there are equal numbers of Adelies on each island.\nBut we don’t have a good sense of the percentage balance within each island.\n\nbar_fig = px.histogram(\n    penguins,\n    x=\"island\",\n    color=\"species\"\n)\n\nbar_fig.update_layout(\n    plot_bgcolor='white',\n    paper_bgcolor='white',\n    xaxis=dict(showgrid=True, gridcolor='lightgray'),\n    yaxis=dict(showgrid=True, gridcolor='lightgray')\n)\n\nbar_fig.show()\n\n                                                \n\n\nThe second plot is a relative frequency plot, created by setting position = \"fill\" in the geom is more useful for comparing species distributions across islands since it’s not affected by the unequal numbers of penguins across the islands.\nUsing this plot we can see that Gentoo penguins all live on Biscoe island and make up roughly 75% of the penguins on that island, Chinstrap all live on Dream island and make up roughly 50% of the penguins on that island, and Adelie live on all three islands and make up all of the penguins on Torgersen.\n\nbar_fig = px.histogram(\n    penguins,\n    x=\"island\",\n    color=\"species\",\n    barnorm='percent'\n)\n\nbar_fig.update_layout(\n    plot_bgcolor='white',\n    paper_bgcolor='white',\n    xaxis=dict(showgrid=True, gridcolor='lightgray'),\n    yaxis=dict(showgrid=True, gridcolor='lightgray')\n)\n\nbar_fig.show()\n\n                                                \n\n\n\n\nTwo Numerical Variables\nSo far you’ve learned about scatterplots (created with px.scatter()) and smooth curves (created with px.line()) for visualizing the relationship between two numerical variables. A scatterplot is probably the most commonly used plot for visualizing the relationship between two numerical variables.\n\nscatter_fig = px.scatter(\n    penguins,\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n)\n\nscatter_fig.show()\n\n                                                \n\n\n\n\nThree or more variables\nAs we saw already, we can incorporate more variables into a plot by mapping them to additional aesthetics.\nFor example, in the following scatterplot the colors of points represent species and the shapes of points represent islands.\n\nscatter_fig = px.scatter(\n    penguins,\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n    color=\"species\",\n    symbol=\"island\"\n)\n\nscatter_fig.update_layout(\n    legend=dict(\n        tracegroupgap=10,\n        itemsizing='constant',\n        bgcolor='rgba(255, 255, 255, 0.5)',\n        bordercolor='black',\n        borderwidth=1,\n    )\n)\n\nscatter_fig.show()\n\n                                                \n\n\nAdding too many aesthetic mappings to a plot can make it cluttered and difficult to understand.\nAnother way to handle this, especially for categorical variables, is to split your plot into facets (also known as small multiples), which are subplots that each display one subset of the data.\nTo facet your plot by a single variable in Plotly, you can use the facet_col or facet_row arguments in px.scatter(), px.bar(), or other Plotly Express functions.\nThe variable you pass to facet_col or facet_row should be categorical.\n\nscatter_fig = px.scatter(\n    penguins,\n    x=\"flipper_length_mm\",\n    y=\"body_mass_g\",\n    color=\"species\",\n    symbol=\"species\",\n    facet_col=\"island\"\n)\n\nscatter_fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n\n\nscatter_fig.show()\n\n                                                \n\n\n\n\nExercises\n\nMake a scatterplot of bill_depth_mm vs. bill_length_mm and color the points by species. What does adding coloring by species reveal about the relationship between these two variables? What about faceting by species?\n\nimport plotly.express as px\n\n# Scatterplot colored by species\nfig = px.scatter(penguins, x='bill_length_mm', y='bill_depth_mm', color='species',\n                 title='Scatterplot of Bill Depth vs. Bill Length Colored by Species')\nfig.show()\n\n# Scatterplot faceted by species\nfig_facet = px.scatter(penguins, x='bill_length_mm', y='bill_depth_mm', color='species',\n                       facet_col='species',\n                       title='Scatterplot of Bill Depth vs. Bill Length Faceted by Species')\nfig_facet.show()\n\nWhy does the following yield two separate legends? How would you fix it to combine the two legends?\n\n# This code will yield two separate legends in ggplot:\n# ggplot(penguins, aes(x=\"bill_length_mm\", y=\"bill_depth_mm\", color=\"species\", shape=\"species\")) +\n# geom_point() + labs(color=\"Species\")\n\n# In Plotly, we can achieve a combined legend by ensuring consistent mapping:\nfig = px.scatter(penguins, x='bill_length_mm', y='bill_depth_mm', color='species', symbol='species',\n                 title='Scatterplot with Combined Legend')\nfig.update_layout(legend_title_text='Species')\nfig.show()\n\nCreate the two following stacked bar plots. Which question can you answer with the first one? Which question can you answer with the second one?\n\n# Stacked bar plot 1: island vs species\nfig1 = px.bar(penguins, x='island', color='species', barmode='stack',\n              title='Stacked Bar Plot: Island vs. Species')\nfig1.show()\n\n# Stacked bar plot 2: species vs island\nfig2 = px.bar(penguins, x='species', color='island', barmode='stack',\n              title='Stacked Bar Plot: Species vs. Island')\nfig2.show()\nIn these exercises, you’ll create scatterplots and stacked bar plots to explore the relationships between variables and visualize the data in different ways.\n\n\nSaving Plots\nOnce you’ve made a plot, you might want to save it as an image that you can use elsewhere. In Plotly, you can use the write_image() function to save the plot to disk. This function is part of the plotly.io module:\nimport plotly.express as px\nimport plotly.io as pio\n\n# Example plot\nfig = px.scatter(penguins, x='bill_length_mm', y='bill_depth_mm', color='species',\n                 title='Scatterplot of Bill Depth vs. Bill Length Colored by Species')\n\n# Show the plot\nfig.show()\n\n# Save the plot to a file\npio.write_image(fig, 'scatterplot.png')\nMake sure you have the kaleido package installed to save Plotly figures as static images:\npip install -U kaleido\n\n\nExercises\n\nSave the figure above as a PNG."
  },
  {
    "objectID": "Course Materials/data_visualise_plotly.html#common-problems",
    "href": "Course Materials/data_visualise_plotly.html#common-problems",
    "title": "Data Visualization",
    "section": "Common Problems",
    "text": "Common Problems\nAs you start to run code, you’re likely to run into problems. Don’t worry—it happens to everyone. We have all been writing Python code for years, but every day we still write code that doesn’t work on the first try!\nStart by carefully comparing the code that you’re running to the code in the book: A misplaced character can make all the difference! Make sure that every ( is matched with a ) and every \" is paired with another \". In Visual Studio Code, you can get extensions that color match brackets so you can easily see if you closed them or not.\nSometimes you’ll run the code and nothing happens.\nFor those coming from the R statistical programming language, you may be concerned about getting your + in the wrong place. Have no fear, however, as in the syntax for Plotly, the + is not required to add layers or elements to your plot.\nIf you’re still stuck, try the help. You can get help about any Python function by running help(function_name) in the interactive window. Don’t worry if the help doesn’t seem that helpful - instead, skip down to the examples and look for code that matches what you’re trying to do.\nIf you’re still stuck, check out the Plotly documentation or do a Google search (especially helpful for error messages)."
  },
  {
    "objectID": "Course Materials/data_visualise_plotly.html#summary",
    "href": "Course Materials/data_visualise_plotly.html#summary",
    "title": "Data Visualization",
    "section": "Summary",
    "text": "Summary\nIn this chapter, you’ve learned the basics of data visualization with Plotly. We started with the basic idea that underpins Plotly: a visualization is a mapping from variables in your data to aesthetic properties like position, color, size, and shape. You then learned about increasing the complexity and improving the presentation of your plots layer-by-layer. You also learned about commonly used plots for visualizing the distribution of a single variable as well as for visualizing relationships between two or more variables, by leveraging additional aesthetic mappings and/or splitting your plot into small multiples using faceting.\nWe’ll use visualizations again and again throughout this book, introducing new techniques as we need them as well as doing a deeper dive into creating visualizations with Plotly in subsequent chapters.\nWith the basics of visualization under your belt, in the next chapter we’re going to switch gears a little and give you some practical workflow advice. We intersperse workflow advice with data science tools throughout this part of the book because it’ll help you stay organized as you write more Python code."
  },
  {
    "objectID": "Course Materials/altair.html",
    "href": "Course Materials/altair.html",
    "title": "Data Visualization with Plotly Express",
    "section": "",
    "text": "Getting started with Altair\n\n\n\n\n\n\n\nExpand To See Links to Chapter in the book Python for DataScience\n\n\n\n\n\n\nChapter on Altair: Setup and Basics\nChapter on Altair: Labels,Annotation, Scales\n\n\n\n\nWhat makes a chart look good?\n\n\nAltair Chart Structure\n\nalt.Chart()\n.mark_*()\n.encode()\n\n\n\nSize of chart\n\n\nWidth and Height\n\n.properties(\n    width=600,\n    height=150,\n)\n\n\n\nTitle and subtitle\n\n\nTitle\n\n.properties(\n    title=\"Bump Chart for Stock Prices\",,\n)\n\n\n\nTitle w/ Subtitle 1\n\n.properties(\n    title={\n    \"text\": [\"First line of title\", \"Second line of title\"], \n    \"subtitle\": [\"Cool first line of subtitle\", \"Even cooler second line wow dang\"],\n    \"color\": \"red\",\n    \"subtitleColor\": \"green\"\n    }\n)\n\n\n\nTitle w/ Subtitle 2\n\ntitle=alt.Title(\n    \"Iowa's green energy boom\",\n    subtitle=\"A growing share of the state's energy has come from renewable sources\"\n)\n\n\n\n\n\n\n\nExpand For Links to Additional Examples\n\n\n\n\n\n\nExample 1\nExample 2\n\n\n\n\n\n\nSize and color of bars\n\n\nBar Chart Size\n\nalt.Chart(data).mark_bar(size=30)\n\n\n\nBar Chart Size and Width\n\nalt.Chart(data).mark_bar(size=30).encode(\n    x='name:O',\n    y='value:Q'\n).properties(width=200)\n\n\n\nBar Chart Size and Step\n\nalt.Chart(data).mark_bar(size=30).encode(\n    x='name:N',\n    y='value:Q'\n).properties(width=alt.Step(100))\n\n\n\nAxis formatting\n\n\nAxis Titles\n\naxis = alt.Axis(format = 'd', title = \"Year\")\naxis = alt.Axis(title = \"Children with Name\")\n\n\n\nAxis Scale removing Zero\n\nalt.X('Acceleration:Q').scale(zero=False)\n\n\n\nAxis Domain Sizing\n\n.scale(domain=(5, 20))\n\n\n\nReference marks\n\n\nVerticle Reference Line with Color\n\nline = alt.Chart(line_df).mark_rule(color=\"red\").encode(x = \"year\")\nchart + line\n\n\n\n\n\n\n\nExpand For Links to Additional Examples\n\n\n\n\n\n\nExample 1\n\n\n\n\n\n\nReference text\n\n\nVerticle Reference Line with Text\n\ndat_label = pd.DataFrame({'x': [1976], 'y': [50000], 'label': ['October 6, 1976']})\nchart + line + text\n\n\n\n\n\n\n\nExpand For Links to Additional Examples\n\n\n\n\n\n\nExample 1\nExample 2\nExample 3\n\n\n\n\n\n\nData Labels\n\n\nData Labels\n\nsource = data.wheat()\n\nbase = alt.Chart(source).encode(\n    x='wheat',\n    y=\"year:O\",\n    text='wheat'\n)\nbase.mark_bar() + base.mark_text(align='left', dx=2)\n\n\n\nHorizontal Datum Line\n\n\nHorizontal Reference Line\n\nalt.Chart().mark_rule().encode(y=alt.datum(1))\n\n\n\nAdditional Resources\n\nLearn more about Customizing Altair Visualizations\nAltair Example Gallery\n\n\n\nAltair videos\n\nWhat is Altair?\nAltair’s Visualization Grammar\nAltair’s Data Types\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Course Materials/git_pull_merge.html",
    "href": "Course Materials/git_pull_merge.html",
    "title": "Pull and Merge Forks on GitHub",
    "section": "",
    "text": "Create Pull Request\n\n\nGo the the forked repository in byuids-resumes and click Pull request.\n\n\n\n\n\nThis will bring you to the the following page where you need to click switching the base.\n\n\n\n\n\nNow you can Create pull request.\n\n\n\n\n\nHere you can type a note and then actually Create pull request.\n\n\n\n\n\nNow you need to View pull request.\n\n\n\n\n\nMerge Request\nIf you have admin access of the forked repository where you are doing the pull request, you can finish the next two steps.\n\n\nClick the Merge pull request button.\n\n\n\n\n\nNow confirm the merge.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Materials",
      "GitHub - Pull and Merge Forks"
    ]
  },
  {
    "objectID": "Course Materials/plotly_express.html",
    "href": "Course Materials/plotly_express.html",
    "title": "Data Visualization with Plotly Express",
    "section": "",
    "text": "Introduction\n\n“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\nThis chapter introudces data visualization with the Plotly Express library. We briefly discus the grammar of graphics which is a useful paradigm for understanding the fundamentals of building graphs. Then we introudce the basics of Plotly Express and provide resources for further development.\n\n\nPrerequisites\nPlotly Express Install\nFor a slightly more interesting introduction, let’s look at the classic iris dataset built into the plotly.express data library. This dataset contains measurements on 3 species of irises.\nFirst, load the data by calling .data from the px library as follows:\n\nimport plotly.express as px\ndata = px.data.iris()\ndata.head()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\nspecies_id\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n1\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n1\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n1\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n1\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n1\n\n\n\n\n\n\n\n\nTo explore the relationship between sepal width and sepal length, we can start with a basic scatterplot.\n\npx.scatter(data,  x=\"sepal_width\", y=\"sepal_length\", title = \"The relationship between sepal width and sepal length\")\n\n                                                \n\n\nWhen all species are lumped together in this scatterplot it doesn’t look like there is much of a relationship between the sepal width and sepal length. We can improve our visualiztion by coloring the points based on species.\n\npx.scatter(data,  x=\"sepal_width\", y=\"sepal_length\", color = \"species\", title = \"The relationship between sepal width and sepal length\")\n\n                                                \n\n\nNotice that px has default hover labels related to x, y, and color. If we would like to modify what information is displayed when we hover over a point, we can add a “hover_data” option.\n\npx.scatter(data,  x=\"sepal_width\", y=\"sepal_length\", color = \"species\", hover_data=[\"species\", \"petal_width\"], title = \"The relationship between sepal width and sepal length\")\n\n                                                \n\n\nAs we increase the complexity of a visualization, our code starts to become less readable. Readable code is a magnificent skill to master. So a better way to express the same code as above is:\n\npx.scatter(data,\n    x=\"sepal_width\", \n    y=\"sepal_length\", \n    color = \"species\", \n    hover_data=[\"species\", \"petal_width\"], \n    title = \"The relationship between sepal width and sepal length\"\n)\n\n                                                \n\n\nWe can also easily change the shape and size of the points in a scatterplot.\n\npx.scatter(data,\n    x=\"sepal_width\", \n    y=\"sepal_length\", \n    color = \"species\", \n    size = \"petal_length\",\n    symbol = \"species\",\n    hover_data=[\"species\", \"petal_width\"], \n    title = \"The relationship between sepal width and sepal length\"\n)\n\n                                                \n\n\nNow that’s just silly! But hopefully the basic syntax for using px makes sense.\n\n\nMaking Plotly.Express More Presentable\nIn this section, we look at how to customize charts to be more informative and presentable. For example, column names in a dataset are rarely a good idea to present to someone not as intimately familiar with the data as you. We may also wish to highlight certain points, or draw attention to areas on a graph.\nTo begin, let’s return to a reasonable visualization of the iris data. We will start by naming our px chart object “fig” and changing the X and Y axis labels.\nThere are 2 ways to change labels. The more direct way is to include a “labels” dictionary in the figure inputs. The key-value pairs in this list are first, the variable name, and second, the desired label.\n\nfig = px.scatter(data,\n    x=\"sepal_width\", \n    y=\"sepal_length\", \n    color = \"species\", \n    symbol = \"species\",\n    labels={\"sepal_width\": \"Sepal Width\", \"sepal_length\": \"Sepal Length\", \"species\": \"Species\"},\n    hover_data=[\"species\", \"petal_width\"], \n    title = \"The relationship between sepal width and sepal length\"\n)\n\nfig.show()\n\n                                                \n\n\nThe next method makes the same adjustments but modifies the chart “post hoc”. Start with a very basic chart, “fig” and use the .update.layout() method to modify titles.\n\nfig = px.scatter(data,\n    x=\"sepal_width\", \n    y=\"sepal_length\", \n    color = \"species\", \n    symbol = \"species\",\n    hover_data=[\"species\", \"petal_width\"], \n    title = \"The relationship between sepal width and sepal length\"\n)\n\nfig.update_layout(\n    xaxis_title = \"Sepal Width\",\n    yaxis_title = \"Sepal Length\",\n    legend_title = \"Species\"\n)\n\n                                                \n\n\n The 2 approaches above have the same outcome, but the latter example introduces a flexible px paradigm that can be extended to other chart additions and modifications. For example, if we want to add a reference line to “fig”, we can use the .add_hline() method. \n\nfig.add_hline(\n    y = 7,\n    line_dash = \"dot\",\n    line_color = \"black\"\n)\n\n                                                \n\n\nWe can add several different shapes, including circles, lines or rectanges using the .add_shape() method. This method specifies what type of shape to add to the graph given a set of coordinates (x0, x1, y0, y1). “.add_shape()” can be used to draw reference lines as well, but still requires a all 4 coordinates.\n\n# Adds a horizontal line\nfig.add_shape(\n    type = \"circle\",\n    line_color = \"green\",\n    line_width = 3,\n    line_dash = \"dot\",\n    opacity = 1,\n    x0 = 3.7, x1=3.9,\n    y0=7.6,y1=8\n)\n\n                                                \n\n\nThe code above introduces many of the .add_shape() features all at once, not all of which are necessary for every situation. But hopefully this gives a flavor of what can be done.\nNotice also, that the .add_ methods actually update fig. No need to overwrite the original object or create a new chart object for each modification.\n\nfig.show()\n\n                                                \n\n\n\n\nOther .add_itions\nThere are several other useful “post hoc” graph modifications that we only mention in this chapter. For further exploration, see Plotly Express Styling.\nUse fig.add_annotation() to add text with or without arrows pointing to specific locations. Update axes by using fig.update_yaxes() or fig.update_xaxes which allows you to modify gridlines and add units of measure like “%” or “$”. .add_vrect() and add_hrect() allow you to highlight regions vertically or horizontally. The possibilities are almost endless!\n\n\nResources\nThis chapter has introudced the basics of plotly.express, but we have only looked at a scatterplot. For links to further documentation and a whole gallery of px possibilities, see Plotly Express in Python.\n\n\n\n\n Back to top",
    "crumbs": [
      "Materials",
      "Plotly Express"
    ]
  },
  {
    "objectID": "Course Materials/sql.html",
    "href": "Course Materials/sql.html",
    "title": "SQL for Data Science",
    "section": "",
    "text": "There are many flavors of SQL but most flavors have the same base commands. SQL queries are typed in the following pattern;\nSELECT -- &lt;columns&gt; and &lt;column calculations&gt;\nFROM -- &lt;table name&gt;\n  JOIN -- &lt;table name&gt;\n  ON -- &lt;columns to join&gt;\nWHERE -- &lt;filter condition on rows&gt;\nGROUP BY -- &lt;subsets for column calculations&gt;\nHAVING -- &lt;filter conditions on groups&gt;\nORDER BY -- &lt;how the output is returned in sequence&gt;\nLIMIT -- &lt;number of rows to return&gt;\n\nIntroductory SQL links\n\nSQL Guide\nSELECT and FROM clauses\nWHERE and comparison operators\nORDER BY\nJoins\nAggregations\nGROUP BY\n\n\n\nLearn How to Setup SQLite\nSQLite\n\n\n\n\n Back to top",
    "crumbs": [
      "Materials",
      "SQLite"
    ]
  },
  {
    "objectID": "Course Materials/visualize.html",
    "href": "Course Materials/visualize.html",
    "title": "Data visualisation",
    "section": "",
    "text": "“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\nThis chapter will teach you how to visualise your data using Altair. Python has several systems for making graphs, but altiar is one of the most elegant and versatile. Altair implements the declarative visualization much like the grammar of graphics, a coherent system for describing and building graphs. With altair, you can do more faster by learning one system and applying it in many places.\nIf you’d like to learn more about Altair before you start, I’d recommend reading “Altair: Interactive Statistical Visualizations for Python”, https://joss.theoj.org/papers/10.21105/joss.01057.pdf.\nWe should note that we are building this book using R with the package bookdown. Rendering Altair graphics using a python chunk is not straight forward but is not important for our use in VS Code. In VS Code the example chunks will render in the interactive Python viewer automatically. The following R code chunks show how we are rendering the Altair graphics in this book. Thanks to ijlyttle for his GitHub Gist."
  },
  {
    "objectID": "Course Materials/visualize.html#introduction",
    "href": "Course Materials/visualize.html#introduction",
    "title": "Data visualisation",
    "section": "",
    "text": "“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\nThis chapter will teach you how to visualise your data using Altair. Python has several systems for making graphs, but altiar is one of the most elegant and versatile. Altair implements the declarative visualization much like the grammar of graphics, a coherent system for describing and building graphs. With altair, you can do more faster by learning one system and applying it in many places.\nIf you’d like to learn more about Altair before you start, I’d recommend reading “Altair: Interactive Statistical Visualizations for Python”, https://joss.theoj.org/papers/10.21105/joss.01057.pdf.\nWe should note that we are building this book using R with the package bookdown. Rendering Altair graphics using a python chunk is not straight forward but is not important for our use in VS Code. In VS Code the example chunks will render in the interactive Python viewer automatically. The following R code chunks show how we are rendering the Altair graphics in this book. Thanks to ijlyttle for his GitHub Gist."
  },
  {
    "objectID": "Course Materials/visualize.html#first-steps",
    "href": "Course Materials/visualize.html#first-steps",
    "title": "Data visualisation",
    "section": "First steps",
    "text": "First steps\nLet’s use our first graph to answer a question: Do cars with big engines use more fuel than cars with small engines? You probably already have an answer, but try to make your answer precise. What does the relationship between engine size and fuel efficiency look like? Is it positive? Negative? Linear? Nonlinear?\n\nThe mpg data frame\nYou can test your answer with the mpg data frame found in ggplot2 (aka ggplot2::mpg). A data frame is a rectangular collection of variables (in the columns) and observations (in the rows). The ‘mpg’ data contains observations collected by the US Environmental Protection Agency on 38 models of car. We will identify the ‘mpg’ data using mpg for the remainder of this introduction.\n\nurl = \"https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/mpg/mpg.csv\"\nmpg = pd.read_csv(url)\n\nAmong the variables in mpg are:\n\ndispl, a car’s engine size, in litres.\nhwy, a car’s fuel efficiency on the highway, in miles per gallon (mpg). A car with a low fuel efficiency consumes more fuel than a car with a high fuel efficiency when they travel the same distance.\n\nTo learn more about mpg, read its format at data4python4ds.\n\n\nCreating an Altair plot\nTo plot mpg, run this code to put displ on the x-axis and hwy on the y-axis:\n\n\n\nAltair\n\nchart = (alt.Chart(mpg)\n  .encode(\n    x='displ', \n    y='hwy')\n  .mark_circle()\n)\n\n\n\n\n\n\n\nPlotly Express\n\nchart = px.scatter(mpg,\n    x='displ', \n\n    y='hwy'\n\n)\n\n\n\n\n\nThe plot shows a negative relationship between engine size (displ) and fuel efficiency (hwy). In other words, cars with big engines use more fuel. Does this confirm or refute your hypothesis about fuel efficiency and engine size?\nWith Altair, you begin a plot with the function Chart(). Chart() creates a Chart object that you can add layers to. The only argument of Chart() is the dataset to use in the graph. So Chart(mpg) creates an Chart object upon which we can marks.\nYou complete your graph by adding one or more marks to Chart(). The attribute .mark_point() adds a layer of points to your plot, which creates a scatterplot. Altair comes with many mark methods that each add a different type of layer to a plot. You’ll learn a whole bunch of them throughout this chapter.\nEach mark method in Altair has an .encode() attribute. This defines how variables in your dataset are encoded to visual properties. The .encode() method is always paired with x and y arguments to specify which variables to map to the x and y axes. Altair looks for the encoded variables in the data argument, in this case, mpg. For pandas dataframes, Altair automatically determines the appropriate data type for the mapped column.\n\n\nA graphing template\nLet’s turn this code into a reusable template for making graphs with ggplot2. To make a graph, replace the bracketed sections in the code below with a dataset, a geom function, or a collection of mappings.\n(alt.Chart(&lt;DATA&gt;)  \n   .encode(&lt;ENCODINGS&gt;)\n    &lt;.mark_*()&gt;)\nThe rest of this chapter will show you how to complete and extend this template to make different types of graphs. We will begin with the &lt;ENCODINGS&gt; component.\n\n\nExercises\n\nRun Chart(mpg).mark_point(). What do you see?\nHow many rows are in mpg? How many columns?\nWhat does the drv variable describe?\nMake a scatterplot of hwy vs cyl.\nWhat happens if you make a scatterplot of class vs drv? Why is the plot not useful?"
  },
  {
    "objectID": "Course Materials/visualize.html#aesthetic-mappings",
    "href": "Course Materials/visualize.html#aesthetic-mappings",
    "title": "Data visualisation",
    "section": "Aesthetic mappings",
    "text": "Aesthetic mappings\n\n“The greatest value of a picture is when it forces us to notice what we never expected to see.” — John Tukey\n\nIn the plot below, one group of points (highlighted in red) seems to fall outside of the linear trend. These cars have a higher mileage than you might expect. How can you explain these cars?\n\n\n\nAltair\n\nchartC = (alt.Chart(mpg)\n  .encode(\n    x = 'displ',\n\n    y = 'hwy',\n\n    color = alt.condition(\n\n      (alt.datum.displ &gt; 5) & (alt.datum.hwy &gt; 20) , \n\n      alt.ColorValue('red'),\n\n      alt.ColorValue('black')),\n\n    size = alt.condition(\n      (alt.datum.displ &gt; 5) & (alt.datum.hwy &gt; 20), \n      alt.SizeValue(50),\n\n\n      alt.SizeValue(25))\n\n      )\n  .mark_circle())\n\n\n\n\n\n\n\nPlotly Express\n\nmpg = mpg.assign(\n  color = np.where(\n    (mpg.displ &gt; 5) & (mpg.hwy &gt;20), \n      1,\n      0\n  ).astype(str),\n  size = np.where(\n    (mpg.displ &gt; 5) & (mpg.hwy &gt;20), \n      5,\n      2.5\n  )\n)\n\nchartC = (px.scatter(mpg,\n  x = 'displ',\n  y = 'hwy',\n  color = 'color',\n  size = 'size',\n  size_max = 5,\n  color_discrete_map = {\"1\": 'red',\"0\":'black'}\n    )\n)\n\n\n\n\n\nLet’s hypothesize that the cars are hybrids. One way to test this hypothesis is to look at the class value for each car. The class variable of the mpg dataset classifies cars into groups such as compact, midsize, and SUV. If the outlying points are hybrids, they should be classified as compact cars or, perhaps, subcompact cars (keep in mind that this data was collected before hybrid trucks and SUVs became popular).\nYou can add a third variable, like class, to a two dimensional scatterplot by mapping it to an encoding. An encoding is a visual property of the objects in your plot. Encodings include things like the size, the shape, or the color of your points. You can display a point (like the one below) in different ways by changing the values of its encoded properties. Since we already use the word “value” to describe data, let’s use the word “level” to describe encoded properties. Here we change the levels of a point’s size, shape, and color to make the point small, triangular, or blue:\nYou can convey information about your data by mapping the encodings in your plot to the variables in your dataset. For example, you can map the colors of your points to the class variable to reveal the class of each car.\n\n\n\nAltair\n\nchart = (alt.Chart(mpg)\n  .encode(\n    x = \"displ\",\n    y = \"hwy\",\n    color = \"class\"\n    )\n  .mark_circle())\n\n\n\n\n\n\n\nPlotly Express\n\nchart = (px.scatter(mpg,\n  x = \"displ\",\n  y = \"hwy\",\n  color = \"class\"\n  )\n\n)\n\n\n\n\n\n(We don’t prefer British English, like Hadley, so don’t use colour instead of color.)\nTo map an encoding to a variable, associate the name of the encoding to the name of the variable inside .encode(). Altair will automatically assign a unique level of the encoding (here a unique color) to each unique value of the variable, a process known as scaling. Altair will also add a legend that explains which levels correspond to which values.\nThe colors reveal that many of the unusual points are two-seater cars. These cars don’t seem like hybrids, and are, in fact, sports cars! Sports cars have large engines like SUVs and pickup trucks, but small bodies like midsize and compact cars, which improves their gas mileage. In hindsight, these cars were unlikely to be hybrids since they have large engines.\nIn the above example, we mapped class to the color encoding, but we could have mapped class to the size encoding in the same way. In this case, the exact size of each point would reveal its class affiliation. Mapping an unordered variable (class) to an ordered aesthetic (size) is not a good idea.\n\n\n\nAltair\n\nchart = (alt.Chart(mpg)\n  .encode(\n    x = \"displ\",\n    y = \"hwy\",\n    size = \"class\"\n    )\n  .mark_circle())\n\n\n\n\n\n\n\nPlotly Express\n\nchart = (px.scatter(mpg,\n  x = \"displ\",\n  y = \"hwy\",\n  size = \"size\",\n  size_max = 5,\n  )\n)\n\n\n\n\n\nOr we could have mapped class to the opacity encoding, which controls the transparency of the points, or to the shape encoding, which controls the shape of the points.\n\n\n\nAltair\n\nchart1 = (alt.Chart(mpg)\n  .encode(\n    x = \"displ\",\n    y = \"hwy\",\n    opacity = \"class\"\n    )\n  .mark_circle())\n\n\n\n\n\n\n\nchart2 = (alt.Chart(mpg)\n  .encode(\n    x = \"displ\",\n    y = \"hwy\",\n    shape = \"class\"\n    )\n  .mark_point())\n\n\n\n\n\n\nPlotly Express\n\nchart1 = (px.scatter(mpg,\n    x = \"displ\",\n    y = \"hwy\",\n    opacity=0.5\n    )\n)\n\n\n\n\n\n\n\nchart2 = (px.scatter(mpg,\n    x = \"displ\",\n    y = \"hwy\",\n    symbol = \"class\"\n    )\n)\n\n\n\n\nAltair will only use 8 shapes for one chart. Charting more than 8 shapes is not recommended as the shapes simply recycle.\nFor each encoding, you use .encode() to associate the name of the encoding with a variable to display. The .encode() function gathers together each of the encoded mappings used by a layer and passes them to the layer’s mapping argument. The syntax highlights a useful insight about x and y: the x and y locations of a point are themselves encodings, visual properties that you can map to variables to display information about the data.\nOnce you map an encoding, Altair takes care of the rest. It selects a reasonable scale to use with the encoding, and it constructs a legend that explains the mapping between levels and values. For x and y aesthetics, Altair does not create a legend, but it creates an axis line with tick marks and a label. The axis line acts as a legend; it explains the mapping between locations and values.\nYou can also configure the encoding properties of your mark manually. For example, we can make all of the points in our plot blue:\n\n\n\nAltair\n\nchart = (alt.Chart(mpg)\n  .encode(\n    x='displ',\n    y='hwy',\n    color=alt.value('blue')\n  )\n  .mark_circle())\n\n\n\n\n\n\n\nPlotly Express\n\nchart = px.scatter(mpg,\n  x='displ',\n  y='hwy'\n).update_traces(marker=dict(color='blue'))\n\n\n\n\n\nHere, the color doesn’t convey information about a variable, but only changes the appearance of the plot. To set an encoding manually, use alt.value() by name as an argument of your .encode() function; i.e. the value goes inside of alt.value(). You’ll need to pick a level that makes sense for that encoding:\n\nThe name of a color as a character string.\nThe size of a point in pixels.\nThe shape of a point as a character string.\n\nNote that only a limited set of mark properties can be bound to encodings, so for some (e.g. fillOpacity, strokeOpacity, etc.) the encoding approach using alt.value() is not available. Encoding settings will always override local or global configuration settings. There are other methods for manually encoding properties as explained in the Altair documentation\n\nExercises\n\nWhich variables in mpg are categorical? Which variables are continuous? How can you see this information when you run mpg? (Hint mpg.dtypes)\nMap a continuous variable to color, size, and shape. How do these aesthetics behave differently for categorical vs. continuous variables?\nWhat happens if you map the same variable to multiple encodings?\nWhat does the stroke encoding do? What shapes does it work with? (Hint: use mark_point())"
  },
  {
    "objectID": "Course Materials/visualize.html#common-problems",
    "href": "Course Materials/visualize.html#common-problems",
    "title": "Data visualisation",
    "section": "Common problems",
    "text": "Common problems\nAs you start to run Python code, you’re likely to run into problems. Don’t worry — it happens to everyone. I have been writing Python code for months, and every day I still write code that doesn’t work!\nStart by carefully comparing the code that you’re running to the code in the book. Python is extremely picky, and a misplaced character can make all the difference. Make sure that every ( is matched with a ) and every \" is paired with another \".\nOne common problem when creating Altair graphics as shown in this book, is to put the () in the wrong place: the ( comes before the alt.chart() command and the ) has to come at the end of the command.\nFor example the code below works in Python.\nalt.Chart(mpg).encode(x = \"displ\", y = \"hwy\").mark_circle()\nHowever, the complexity of the more details graphics necessitates placing the code on multiple lines. When using multiple lines we need the enclosing (). Make sure you haven’t accidentally excluded a ( or ) like this\n(alt.Chart(mpg)\n  .encode(\n    x = \"displ\",\n    y = \"hwy\")\n  .mark_circle()\nor placed the () incorrectly like this\n(chart = alt.Chart(mpg)\n  .encode(\n    x = \"displ\",\n    y = \"hwy\")\n  .mark_circle(\n)    \nIf you’re still stuck, try the help. You can get help about any Altair function from their website - https://altair-viz.github.io/, or hovering over the function name in VS Code. If that doesn’t help, carefully read the error message. Sometimes the answer will be buried there! But when you’re new to Python, the answer might be in the error message but you don’t yet know how to understand it. Another great tool is Google: try googling the error message, as it’s likely someone else has had the same problem, and has gotten help online."
  },
  {
    "objectID": "Course Materials/visualize.html#facets",
    "href": "Course Materials/visualize.html#facets",
    "title": "Data visualisation",
    "section": "Facets",
    "text": "Facets\nOne way to add additional variables is with encodings. Another way, particularly useful for categorical variables, is to split your plot into facets, subplots that each display one subset of the data.\nTo facet your plot by a single variable, use .facet(). Assign a discrete variable to the .facet argument and adjust the size of your facet grid using the columns argument.\n\n\n\nAltair\n\nchart_f = (alt.Chart(mpg)\n  .encode(\n    x = \"displ\",\n    y = \"hwy\",\n   )\n  .mark_circle()\n  .facet(\n    facet = \"class\",\n    columns = 4\n  ))\n\n\n\n\n\n\n\nPlotly Express\n\nchart_f = (px.scatter(mpg,\n  x = 'displ',\n\n  y = 'hwy',\n\n  facet_col= \"class\", \n\n  facet_col_wrap=4\n    )\n)\n\n\n\n\n\nTo facet your plot on the combination of two variables, the first argument of .facet() is column and the second is row. This time the formula should contain two discrete variable names.\n\n\n\nAltair\n\nchart_f2 = (alt.Chart(mpg)\n  .encode(\n    x = \"displ\",\n    y = \"hwy\",\n    )\n  .mark_circle()\n  .facet(\n    column = \"drv\",\n    row = \"cyl\"\n    ))\n\n\n\n\n\n\n\nPlotly Express\n\nchart_f2 = (px.scatter(mpg,\n  x = 'displ',\n\n  y = 'hwy',\n\n  facet_col= \"drv\", \n\n  facet_row= \"cyl\"\n    )\n)\n\n\n\n\n\nIf you prefer to not facet in the rows or columns dimension, simply remove that facet argument. You can read more about compound charts in the Altair documentation.\n\nExercises\n\nWhat happens if you facet on a continuous variable?\nWhat do the empty cells in plot with .facet(column = \"drv\", row = \"cyl\") mean? How do they relate to this plot?\n\n\n\n\nAltair\n\nchart = (alt.Chart(mpg)\n  .encode(\n    x = \"drv\",\n    y = \"cyl\"\n    )\n    .mark_circle())\n\n\n\n\n\n\n\nPlotly Express\n\nchart = (px.scatter(\n      mpg,\n      x = 'drv',\n      y = 'cyl'\n        )\n)\n\n\n\n\n\n\nWhat plots does the following code make? What does . do?\n\n\n\n\nAltair\n\nchart = (alt.Chart(mpg)\n  .encode(\n    x = \"displ\",\n    y = \"hwy\"\n    )\n    .mark_circle()\n    .facet(column = \"drv\"))\n\n\n\n\n\n\n\nchart = (alt.Chart(mpg)\n  .encode(\n    x = \"displ\",\n    y = \"hwy\"\n    )\n    .mark_circle()\n    .facet(row = \"cyl\"))\n\n\n\n\n\n\nPlotly Express\n\nchart = (px.scatter(mpg,\n      x = 'displ',\n      y = 'hwy',\n      facet_col= \"drv\"\n        ))\n\n\n\n\n\n\n\nchart = (px.scatter(mpg,\n      x = 'displ',\n      y = 'hwy',\n      facet_row= \"cyl\"\n        ))  \n\n\n\n\n\nTake the first faceted plot in this section:\nWhat are the advantages to using faceting instead of the colour aesthetic? What are the disadvantages? How might the balance change if you had a larger dataset?\nWhen using facet() you should usually put the variable with more unique levels in the columns. Why?"
  },
  {
    "objectID": "Course Materials/visualize.html#geometric-objects",
    "href": "Course Materials/visualize.html#geometric-objects",
    "title": "Data visualisation",
    "section": "Geometric objects",
    "text": "Geometric objects\nHow are these two plots similar?\n\n\n\nAltair\n\nchartp = (alt.Chart(mpg)\n  .encode(\n    x = \"displ\",\n    y = \"hwy\"\n    )\n\n  .mark_circle()) \n\n\n\n\n\n\n\nchartf = (alt.Chart(mpg)\n  .encode(\n    x = \"displ\",\n    y = \"hwy\"\n    )\n  .transform_loess(\"displ\", \"hwy\")\n  .mark_line())\n\n\n\n\nPlotly Express\n\nchart = (px.scatter(mpg,\n  x = 'displ',\n  y = 'hwy',\n    )\n\n)\n\n\n\n\n\n\n\nchart = (px.scatter(mpg,\n  x = 'displ',\n  y = 'hwy',\n  trendline = \"lowess\"\n    )\n).update_traces(visible=False, selector=dict(mode=\"markers\"))\n\n\n\n\nBoth plots contain the same x variable, the same y variable, and both describe the same data. But the plots are not identical. Each plot uses a different visual object to represent the data. In Altair syntax, we say that they use different marks.\nA mark is the geometrical object that a plot uses to represent data. People often describe plots by the type of mark that the plot uses. For example, bar charts use bar marks, line charts use line marks, boxplots use boxplot marks, and so on. Scatterplots break the trend; they use the point mark. As we see above, you can use different marks to plot the same data. The first plot uses the point mark, and the second plot uses the line mark, a smooth line fitted to the data is calculated using a transformation. To change the mark in your plot, change the mark function that you add to Chart().\nEvery mark function in Altair has encode arguments. However, not every encoding works with every mark. You could set the shape of a point, but you couldn’t set the “shape” of a line. On the other hand, you could set the type of line. .mark_line() will draw a different line, with a different strokeDash, for each unique value of the variable that you map to strokeDash.\n\n\n\nAltair\n\nchart = (alt.Chart(mpg)\n  .transform_loess(\"displ\", \"hwy\", groupby = [\"drv\"])\n  .encode(\n    x = \"displ\",\n    y = \"hwy\",\n    strokeDash = \"drv\"\n    )\n    .mark_line())\n\n\n\n\n\n\n\nPlotly Express\n\nchart = px.scatter(mpg, \n\n                  x='displ',\n\n                  y='hwy', \n\n                  color='drv'\n                  )\n\n\n\n\n\nHere .mark_line() separates the cars into three lines based on their drv value, which describes a car’s drivetrain. One line describes all of the points with a 4 value, one line describes all of the points with an f value, and one line describes all of the points with an r value. Here, 4 stands for four-wheel drive, f for front-wheel drive, and r for rear-wheel drive.\nIf this sounds strange, we can make it more clear by overlaying the lines on top of the raw data and then coloring everything according to drv.\n\n\n\nAltair\n\nchartp = (alt.Chart(mpg).\n  mark_circle().\n  encode(\n    x = \"displ\",\n    y = \"hwy\",\n    strokeDash = \"drv\",\n    color = \"drv\"\n  ))\nchart = chartp + chartp.transform_loess(\"displ\", \"hwy\", groupby = [\"drv\"]).mark_line()\n\n\n\n\n\n\n\nPlotly Express\n\nfig = px.scatter(mpg, \n  x='displ',\n\n  y='hwy', \n\n  color='drv', \n\n  symbol='drv', \n  trendline='lowess')\n\n\n\n\n\nNotice that this plot contains two marks in the same graph! If this makes you excited, buckle up. We will learn how to place multiple marks on the same chart very soon.\nAltair provides about 15 marks. The best way to get a comprehensive overview is the Altair marks page, which you can find at https://altair-viz.github.io/user_guide/marks.html.\nMany marks, like .mark_line(), use a single mark object to display multiple rows of data. For these marks, you can set the detail encoding to a categorical variable to draw multiple objects. Altair will draw a separate object for each unique value of the detail variable. In practice, Altair will automatically group the data for these marks whenever you map an encoding to a discrete variable (as in the strokeDash example). It is convenient to rely on this feature because the detail encoding by itself does not add a legend or distinguishing features to the marks.\n\n\nAltair\n\nchartleft = (alt.Chart(mpg)\n  .encode(\n    x = \"displ\",\n    y = \"hwy\",\n    )\n\n  .transform_loess(\"displ\", \"hwy\")\n  .mark_line())\n\n\n\n\n\n\n\n\n\nchartmiddle = (alt.Chart(mpg)\n  .encode(\n    x = \"displ\",\n    y = \"hwy\",\n    detail = \"drv\"\n    )\n  .transform_loess(\"displ\", \"hwy\", groupby = [\"drv\"])\n  .mark_line())\n\n\n\n\n\n\n\n\nchartright = (alt.Chart(mpg)\n  .encode(\n    x = \"displ\",\n    y = \"hwy\",\n    color=alt.Color(\"drv\", legend=None)\n    )\n  .transform_loess(\"displ\", \"hwy\", groupby = [\"drv\"])\n  .mark_line())\n\n\n\n::::\n\n\nPlotly Express\n\nchartleftpx = px.scatter(mpg, \n\n  x='displ', \n\n  y='hwy', \n\n  trendline='lowess')\n\n\n\n\n\n\n\n\n\nchartmiddlepx = px.scatter(mpg, \n\n  x='displ',\n\n  y='hwy',\n\n\n  trendline='lowess')\n\n\n\n\n\n\n\n\nchartrightpx = px.scatter(mpg, \n  x='displ',\n\n  y='hwy', \n\n  color='drv', \n  trendline='lowess', \n  color_discrete_sequence=px.colors.qualitative.Plotly)\n\n\n\n::::\nTo display multiple marks in the same plot, you can used layered charts as shown in the example below that uses the chartleft object from the above code chunk:\n\n\n\nAltair\n\nchartp = (alt.Chart(mpg)\n  .encode(\n    x = \"displ\",\n    y = \"hwy\"\n  )\n  .mark_circle()\n)\nchart = chartp + chartleft \n\n\n\n\n\n\n\nPlotly Express\n\nfig = px.scatter(\n  mpg, \n\n  x='displ', \n\n  y='hwy',  \n\n  trendline='lowess')\n\n\n\n\n\nThis, however, introduces some duplication in our code. Imagine if you wanted to change the y-axis to display cty instead of hwy. You’d need to change the variable in two places, and you might forget to update one. You can avoid this type of repetition by passing a set of encodings to a base alt.Chart(). Altair will treat these encodings as global encodings that apply to each mark layer in the layered chart. In other words, this code will produce the same plot as the previous code:\n\n\nAltair\n\nbase = (alt.Chart(mpg)\n  .encode(\n    x = \"displ\",\n    y = \"hwy\"\n    ))\nchart = base.mark_circle() + base.transform_loess(\"displ\", \"hwy\").mark_line()\n\n\n\n\n\n\n\n\n\nPlotly Express\n\nchart = px.scatter(\n  mpg, \n  x='displ',\n  y='hwy',\n\n  trendline='lowess')\n\n\n\n\nIf you place encodings in an encode function, Altair will treat them as local mappings for the layer. It will use these mappings to extend or overwrite the base encodings for that layer only. This makes it possible to display different aesthetics in different layers. Alatair automatically chooses useful plot settings and chart configurations to allow you to think about data instead of the programming mechanics of the chart. You can review their guidance on customizing visualizations to see the varied ways to change the look of your graphic.\n\n\n\nAltair\n\nbase =(alt.Chart(mpg)\n  .encode(\n    x = \"displ\",\n    y = \"hwy\"\n  ))\n\nchart = base.encode(color = \"drv\").mark_circle() + base.transform_loess(\"displ\", \"hwy\").mark_line()\n\n\n\n\n\n\n\nPlotly Express\n\nchart = px.scatter(\n  mpg, \n  x='displ',\n  y='hwy',\n\n  trendline='lowess',\n\n  color = \"drv\")\n\n\n\n\n\nYou can use the same idea to specify different data for each layer. Here, our smooth line displays just a subset of the mpg dataset, the subcompact cars. The .transform_filter method overrides the global data from the base chart for that layer only.\n\n\n\nAltair\n\nbase = (alt.Chart(mpg.rename(columns = {\"class\": \"class1\"}))\n  .encode(\n    x = \"displ\",\n    y = \"hwy\"\n    ))\n\nchart_smooth_sub = (base\n  .transform_filter(alt.datum.class1 == \"subcompact\")\n  .transform_loess(\"displ\", \"hwy\")\n  .mark_line()\n)  \n\nchart = base.encode(color = \"class1\").mark_circle() + chart_smooth_sub\n\n\n\n\n\n\n\nPlotly Express\n\nmpg.rename(columns={\"class\": \"class1\"}, inplace=True)\n\nbase = px.scatter(mpg, x='displ', y='hwy', color='class1').data[0]\n\nsubcompact_mpg = mpg[mpg['class1'] == 'subcompact']\n\nsmoothed_subcompact = lowess(subcompact_mpg['hwy'], subcompact_mpg['displ'], return_sorted=False)\nsmoothed_subcompact_df = pd.DataFrame({'displ': subcompact_mpg['displ'], 'smoothed_hwy': smoothed_subcompact})\n\nline = px.line(smoothed_subcompact_df, x='displ', y='smoothed_hwy').data[0]\n\nfig = px.scatter(mpg, x='displ', y='hwy', color='class1')\n\n\n\n\n\n(You’ll learn how pandas filter works in the chapter on data transformations. To keep the same base chart, filtering is done with Altair in this example: for now, just know that this command selects only the subcompact cars.)\n\nExercises\n\nWhat geom would you use to draw a line chart? A boxplot? A histogram? An area chart?\nWhat does legend=None in alt.Color() do? What happens if you remove it? Why do you think I used it earlier in the chapter?\nRecreate the Python code necessary to generate the following graphs.\n\n\n\n\nAltair\n\nbase = (alt.Chart(mpg)\n  .encode(\n    x = \"displ\",\n    y = \"hwy\"\n    ))\n    \nsmooth1 = base.transform_loess(\"displ\", \"hwy\").mark_line()\nsmooth3 = base.transform_loess(\"displ\", \"hwy\", groupby = [\"drv\"]).mark_line()\n\nchart1 = base.mark_circle() + smooth1\nchart2 = base.mark_circle() + smooth3.encode(detail = \"drv\")\nchart3 = base.mark_circle().encode(color = \"drv\") + smooth3.encode(color = \"drv\")\nchart4 = base.mark_circle().encode(color = \"drv\") + smooth1\nchart5 = base.mark_circle().encode(color = \"drv\") + smooth3.encode(strokeDash = \"drv\")\nchart6 = (base\n  .encode(\n    color = alt.ColorValue(\"darkgrey\"),\n    size = alt.SizeValue(125)\n    )\n  .mark_point(filled = True)) + base.mark_circle().encode(color = \"drv\")\n\n\n\n\n\n\n\nPlotly Express\n\nsmoothed_all = lowess(mpg['hwy'], mpg['displ'], return_sorted=False)\nsmoothed_subcompact = lowess(mpg[mpg['class1'] == 'subcompact']['hwy'], mpg[mpg['class1'] == 'subcompact']['displ'], return_sorted=False)\n\n# Create figure\nfig1 = px.scatter(mpg, x='displ', y='hwy')\nfig1.add_traces(go.Scatter(x=mpg['displ'], y=smoothed_all, mode='lines', name='Smoothed'))\n\nfig2 = px.scatter(mpg, x='displ', y='hwy', color='drv')\nfig2.add_traces(go.Scatter(x=mpg['displ'], y=smoothed_subcompact, mode='lines', name='Smoothed', line=dict(dash='dash')))\n\nfig3 = px.scatter(mpg, x='displ', y='hwy', color='drv')\nfig3.add_traces(go.Scatter(x=mpg['displ'], y=smoothed_subcompact, mode='lines', name='Smoothed', line=dict(dash='dash')))\nfig3.add_traces(go.Scatter(x=mpg['displ'], y=smoothed_all, mode='lines', name='All Smoothed'))\n\nfig4 = px.scatter(mpg, x='displ', y='hwy', color='drv')\nfig4.add_traces(go.Scatter(x=mpg['displ'], y=smoothed_all, mode='lines', name='All Smoothed'))\n\nfig5 = px.scatter(mpg, x='displ', y='hwy', color='drv')\nfig5.add_traces(go.Scatter(x=mpg['displ'], y=smoothed_subcompact, mode='lines', name='Smoothed', line=dict(dash='dash')))\nfig5.add_traces(go.Scatter(x=mpg['displ'], y=smoothed_all, mode='lines', name='All Smoothed', line=dict(dash='dash')))\n\nfig6 = px.scatter(mpg, x='displ', y='hwy', color='drv')\nfig6.add_traces(go.Scatter(x=mpg['displ'], y=smoothed_all, mode='lines', name='All Smoothed'))\nfig6.add_traces(go.Scatter(x=mpg['displ'], y=smoothed_subcompact, mode='lines', name='Subcompact Smoothed', line=dict(dash='dash')))"
  },
  {
    "objectID": "Course Materials/visualize.html#statistical-transformations",
    "href": "Course Materials/visualize.html#statistical-transformations",
    "title": "Data visualisation",
    "section": "Statistical transformations",
    "text": "Statistical transformations\nNext, let’s take a look at a bar chart. Bar charts seem simple, but they are interesting because they reveal something subtle about plots. Consider a basic bar chart, as drawn with .mark_bar(). The following chart displays the total number of diamonds in the diamonds dataset, grouped by cut. The diamonds dataset comes in ggplot2 R package and can be used in Python using the following Python command. Note that we also need to use pandas to format a few of the columns as ordered categorical to have the diamonds DataFrame act like it does in R.\n\nurl = \"https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/diamonds/diamonds.csv\"\ndiamonds = pd.read_csv(url)\n\ndiamonds['cut'] = pd.Categorical(diamonds.cut, \n  ordered = True, \n  categories =  [\"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\" ])\n\ndiamonds['color'] = pd.Categorical(diamonds.color, \n  ordered = True, \n  categories =  [\"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"])\n\n\ndiamonds['clarity'] = pd.Categorical(diamonds.clarity, \n  ordered = True, \n  categories =  [\"I1\", \"SI2\", \"SI1\", \"VS2\", \"VS1\", \"VVS2\", \"VVS1\", \"IF\"])\n\nIt contains information about ~54,000 diamonds, including the price, carat, color, clarity, and cut of each diamond. The chart shows that more diamonds are available with high quality cuts than with low quality cuts.\n\n\nAltair\n\nchart = (alt.Chart(diamonds)\n  .encode(\n    x = \"cut\",\n    y = alt.Y(\"count():Q\")\n    )\n  .mark_bar()\n  .properties(width = 400))\n\n\n\n\n\n\n\n\n\nPlotly Express\n\nchart = px.bar(diamonds, \n  x='cut', \n  y='cut', \n  title='Number of Diamonds by Cut', \n  labels={'cut': 'Count'})\n\nchart.update_layout(width=400)\n\nchart.write_image(\"PlotlyCharts/chart25.png\")\n\n\n\n\nOn the x-axis, the chart displays cut, a variable from diamonds. On the y-axis, it displays count, but count is not a variable in diamonds! Many graphs, like scatterplots, plot the raw values of your dataset. Other graphs, like bar charts, calculate new values to plot:\n\nbar charts, histograms, and frequency polygons bin your data and then plot bin counts, the number of points that fall in each bin.\nsmoothers fit a model to your data and then plot predictions from the model.\nboxplots compute a robust summary of the distribution and then display a specially formatted box.\n\nThe algorithm used to calculate new values for a graph is called a transform, short for transformation. The figure below describes how this process works with .mark_bar().\nYou must explicitely define the transformation a mark uses through transformations using alt.Y() or alt.X() function. For example, mark_bar() requires the y encoding alt.Y(\"count():Q\"). A histogram is created using .mark_bar() with transformations on both the x and y axes. The bin argument accepts a boolean or an alt.Bin() function where the argument maxbins can be used - bin=alt.Bin(maxbins=100).\n\n\nAltair\n\nchart = (alt.Chart(diamonds)\n  .encode(\n    x =alt.X(\"price\", bin=True),\n    y =alt.Y(\"count()\")\n    )\n  .mark_bar())\n\n\n\n\n\n\n\n\n\nPlotly Express\n\nchart = px.histogram(diamonds, \n  x='price', \n  nbins=10, \n  title='Distribution of Diamond Prices')\n\n\n\n\nFor more complicated transformations Altair provides transform functions. We saw one of these transforms previously when we used .mark_line() to describe each drive type. If you are working with pandas DataFrames then you may want to do these transformations using pandas. Altair’s transformations can be used with DataFrames as well as JSON files or URL pointers to CSV files.\n\n\nAltair\n\nchartright = (alt.Chart(mpg)\n  .encode(\n    x = \"displ\",\n    y = \"hwy\",\n    color=alt.Color(\"drv\", legend=None)\n    )\n  .transform_loess(\"displ\", \"hwy\", groupby = [\"drv\"])\n  .mark_line())\n\n\n\n\n\n\n\n\n\nPlotly Express\n\nsmoothed = lowess(mpg['hwy'], mpg['displ'], return_sorted=False)\nmpg['smoothed_hwy'] = smoothed\n\nfig = px.line(mpg, \n  x='displ',\n  y='smoothed_hwy', \n  color='drv')\n\n\n\n\nFinally, .mark_boxplot() is available which does the statistical transformations for you after you specify the encodings for the x and y axes.\n\n\nAltair\n\nchart = (alt.Chart(diamonds)\n  .encode(\n    y =\"price\",\n    x =\"cut\"\n    )\n  .mark_boxplot(size = 25)\n  .properties(width = 300))\n\n\n\n\n\n\n\n\n\nPlotly Express\n\nfig = px.box(diamonds, y='price', x='cut', title='Price Distribution by Cut',\n             labels={'price': 'Price', 'cut': 'Cut'})"
  },
  {
    "objectID": "Course Materials/visualize.html#position-adjustments",
    "href": "Course Materials/visualize.html#position-adjustments",
    "title": "Data visualisation",
    "section": "Position adjustments",
    "text": "Position adjustments\nThere’s one more piece of magic associated with bar charts. You can colour a bar chart using either the stroke aesthetic, or, more usefully, color:\n\n\nAltair\n\nchart_left = (alt.Chart(diamonds)\n  .encode(\n    x = \"cut\",\n    y = alt.Y(\"count()\"),\n    stroke = \"cut\"\n    )\n  .mark_bar()\n  .properties(width = 200))\n\n\n\n\n\n\n\n\n\nchart_right = (alt.Chart(diamonds)\n  .encode(\n    x = \"cut\",\n    y = alt.Y(\"count()\"),\n    color = \"cut\"\n    )\n  .mark_bar()\n  .properties(width = 200))\n\n\n\n\n\n\n\n\nPlotly Express\n\nchart_left_px = px.bar(diamonds, \n  x='cut', \n  y='cut',\n  labels={'cut': 'Count'}, color='cut')\n\n\n\n\n\n\n\n\n\nchart_right_px = px.bar(diamonds, x='cut', y='cut', title='Number of Diamonds by Cut',\n             labels={'cut': 'Count'}, color='cut')\n\n\n\nNote what happens if you map the color encoding to another variable, like clarity: the bars are automatically stacked. Each colored rectangle represents a combination of cut and clarity.\n\n\nAltair\n\nchart = (alt.Chart(diamonds)\n  .encode(\n    x = \"cut\",\n    y = alt.Y(\"count()\"),\n    color = \"clarity\"\n    )\n  .mark_bar()\n  .properties(width = 200))\n\n\n\n\n\n\n\n\n\nPlotly Express\n\nfig = px.bar(diamonds, \n  x='cut', \n  y='cut',\n  labels={'cut': 'Cut', 'clarity': 'Clarity', 'count': 'Count'}, color='clarity')\n\n\n\n\nThe stacking is performed automatically by .mark_bar(). If you don’t want a stacked bar chart, you can use use the stack argument in alt.Y() one of three other options: \"identity\", \"dodge\" or \"fill\".\n\nposition = \"identity\" will place each object exactly where it falls in the context of the graph. This is not very useful for bars, because it overlaps them. To see that overlapping we either need to make the bars slightly transparent by setting alpha to a small value, or completely transparent by setting fill = NA.\n\n\n\nAltair\n\nchart_left = (alt.Chart(diamonds)\n        .encode(\n          x = \"cut\",\n          y = alt.Y(\"count()\", stack=None),\n          color = \"clarity\",\n          opacity = alt.value(1/5)\n          )\n        .mark_bar()\n        .properties(width = 200))      \n\n\n\n\n\n\n\n\n\nchart_right = (alt.Chart(diamonds)\n        .encode(\n          x = \"cut\",\n          y = alt.Y(\"count()\", stack=None),\n          stroke = \"clarity\",\n          color = alt.value(\"none\")\n          )\n        .mark_bar()\n        .properties(width = 200))\n\n\n\n\n\n\n\n\nPlotly Express\n\nfig = px.bar(diamonds, \n  x='cut', \n  y='cut',\n  labels={'cut': 'Cut', 'clarity': 'Clarity', 'count': 'Count'}, color='clarity',\n  opacity=1/5, barmode='group')\n\n\n\n\n\n\n\n\n\nfig = px.bar(diamonds, \n  x='cut',\n  y='cut', \n  labels={'cut': 'Cut', 'clarity': 'Clarity', 'count': 'Count'}, \n  color_discrete_sequence=['rgba(0,0,0,0)'],\n  color='clarity', barmode='group')\n\n\n\n\nposition = \"fill\" works like stacking, but makes each set of stacked bars the same height. This makes it easier to compare proportions across groups.\n\n\n\nAltair\n\nchart = (alt.Chart(diamonds)\n      .mark_bar()\n      .encode(\n        x = \"cut\",\n        y = alt.Y(\"count()\", stack='normalize'),\n        color = \"clarity\"\n        )\n      .mark_bar()\n      .properties(width = 200))\n\n\n\n\n\n\n\n\n\nPlotly Express\n\nfig = px.bar(diamonds,\n   x='cut', \n   y='cut',\n  labels={'cut': 'Cut', 'clarity': 'Clarity', 'count': 'Count'}, \n  color='clarity',\n  barmode='group', \n  facet_col='cut', \n  facet_col_wrap=3, \n  facet_row_spacing=0.05)\n\n\n\n\n\nPlacing overlapping objects directly beside one another is done by using the column encoding. This makes it easier to compare individual values with a common baseline.\n\n\n\nAltair\n\nchart = (alt.Chart(diamonds)\n      .encode(\n        x='clarity',\n        y=alt.Y('count()'),\n        color='clarity',\n        column='cut'\n        )\n      .mark_bar())\n\n\n\n\n\n\n\n\n\nPlotly Express\n\ndf_count = diamonds.groupby(['clarity', 'cut']).size().reset_index(name='count')\n\nfig = px.bar(df_count, \n             x='clarity', \n             y='count', \n             labels={'clarity': 'Clarity', 'count': 'Count'}, \n             color='clarity',\n             facet_col='cut', \n             facet_col_wrap=3, \n             facet_row_spacing=0.05)\n\n\n\n\nAltair does not have a simple way to add random jitter to points using an encoding or simple argument to alt.X() or alt.Y(). Altair can create a jittered point plot, also called a stripplot. However, it is not as straight forward.\nWe should note that Altair also does not provide piechart or donut chart marks as well."
  },
  {
    "objectID": "Course Materials/visualize.html#coordinate-systems-maps",
    "href": "Course Materials/visualize.html#coordinate-systems-maps",
    "title": "Data visualisation",
    "section": "Coordinate systems (maps)",
    "text": "Coordinate systems (maps)\nCoordinate systems are generally on an x and y axis in Altair. This coordinate system is the Cartesian coordinate system, where the x and y positions act independently to determine the location of each point. Maps are on a Cartesian coordinate system, but their behavior is different as they require a transformation or projection of the globe to an x and y Cartesian plane.\nVisualizing data on maps can get complicated quickly. Altair has a some mapping marks available for charts. The data formats for plotting with maps also get more involved. You can use the gpdvega package to help with spatial data in Altair. Many Python users us geopandas or Shapely to handle spatial data in Python. PennState has a nice list of spatial tools for Python."
  },
  {
    "objectID": "Course Materials/visualize.html#the-layered-grammar-of-graphics",
    "href": "Course Materials/visualize.html#the-layered-grammar-of-graphics",
    "title": "Data visualisation",
    "section": "The layered grammar of graphics",
    "text": "The layered grammar of graphics\nIn the previous sections, you learned much more than how to make scatterplots, bar charts, and boxplots. You learned a foundation that you can use to make any type of plot with Altair. To see this, let’s add position adjustments, stats, coordinate systems, and faceting to our code template:\n(alt.Chart(&lt;DATA&gt;)\n  .encoding(\n    \n    )\n  .facet(\n    column = &lt;FACET VARIABLE&gt;\n    )\n  .&lt;TRANFORM_FUNCTION&gt;()\n  .&lt;MARK_FUNCTION&gt;()\n  .properties(\n    width = ,\n    height = \n  ))\nAltair’s general template takes four main parameters - alt.Chart(), .mark_*(), .encode(), and .facet(). In practice, only need to supply the first three parameters to make a chart because Altair will provide useful defaults for everything except the data, the encodings, and the mark function.\nThe parameters in the template compose the grammar of graphics, a formal system for building plots. The grammar of graphics is based on the insight that you can uniquely describe any plot as a combination of a dataset, a mark, a set of encodings, a transformation, a position adjustment, and a faceting scheme.\nTo see how this works, consider how you could build a basic plot from scratch: you could start with a dataset and then transform it into the information that you want to display (with a stat).\nNext, you could choose a geometric object to represent each observation in the transformed data. You could then use the aesthetic properties of the geoms to represent variables in the data. You would map the values of each variable to the levels of an aesthetic.\nYou’d then select a coordinate system to place the geoms into. You’d use the location of the objects (which is itself an aesthetic property) to display the values of the x and y variables. At that point, you would have a complete graph, but you could further adjust the positions of the geoms within the coordinate system (a position adjustment) or split the graph into subplots (faceting). You could also extend the plot by adding one or more additional layers, where each additional layer uses a dataset, a geom, a set of mappings, a stat, and a position adjustment.\nYou could use this method to build any plot that you imagine. In other words, you can use the code template that you’ve learned in this chapter to build hundreds of thousands of unique plots."
  },
  {
    "objectID": "Course Materials/visualize.html#altairs-grammar-of-graphics",
    "href": "Course Materials/visualize.html#altairs-grammar-of-graphics",
    "title": "Data visualisation",
    "section": "Altair’s grammar of graphics",
    "text": "Altair’s grammar of graphics\nPlotnine is a Python implementation of ggplot2 in R’s grammar of graphics using matplotlib as the plotting backend. Altair’s implementation of the grammar of graphics is much like ggplot2 or plotnine at a high level. However, it uses the Vega-Lite grammar of graphics constructs and plotting backend.\nBelow are some useful links that will help you dig deeper into the Altair implementation of the grammar of graphics.\n\nAltair mark guidance\nAltair encoding guidance\nAltair facet guidance\nAltair tranformations guidance\nAltair chart customization\nAltair sorting barcharts\nAltair theme editing\nAltair Encoding data types"
  },
  {
    "objectID": "Projects/Archive/project_0_v0.html",
    "href": "Projects/Archive/project_0_v0.html",
    "title": "Project 0: Introduction",
    "section": "",
    "text": "Walkthrough\n\n\n\nBackground\n\n\n\n\n\n\nNote\n\n\n\nWe will complete six projects during the semester that each take about four days of class. On average, a student will spend 2 hours outside of class per hour in class to complete the assigned readings, submit any Canvas items, and complete the project (for a total of 8 hours per project). The instruction for each project will be structured into sections as written on this page.\nThis first Background section provides context for the project. Make sure you read the background carefully to see the big picture needs and purpose of the project.\n\n\nPython and VS Code are tools commonly used in the field of data science. During our first two days of class we will get VS Code prepped for data science programming. Completing Project 0 will set you up for success the rest of the semester.\n\n\nData\n\n\n\n\n\n\nNote\n\n\n\nEvery data science project should start with data, and our class projects are no different. Each project will have ‘Download’ and ‘Information’ links like the ones below.\n\n\nDownload: mpg data\nInformation: Data description\n\n\nReadings\n\n\n\n\n\n\nNote\n\n\n\nThe Readings section will contain links to reading assignments that are required for each project, as well as optional references. Remember that you are reading this material to build skills. Take the time to comprehend the readings and the skills contained within.\nWe recommend reading through the assigned material once for a general understanding before the first day of each project. You will reread and reference the material multiple times as you complete the project.\n\n\nThe readings listed below are required for the first two days of class.\n\nCourse Setup\nLearn about VS Code\nLearn about Plotly Express\nLearn about our Book Python for Data Science\nQuarto Instructional Template for DS\nPython for Data Science (P4DS): Introduction\n\n\nOptional References\n\nVS Code user interface\nReading Technical Documentation\n\n\n\n\nQuestions and Tasks:\n\n\n\n\n\n\nNote\n\n\n\nThis section lists the questions and tasks that need to be completed for the project. Your work on the project must be compiled into a report and submitted in Canvas by the weekend following the last day of material for the project.\n\n\n\nFinish the readings and be prepared with any questions to get your environment working smoothly (class for on-campus and Slack for online)\nIn VS Code, write a python script to create the example chart from section 3.2.2 of the textbook (part of the assigned readings). Note that you will be using Plotly Express to display the chart instead of Altair which is used in the book.\nYour final report should also include the markdown table created from the following (assuming you have mpg from question 2).\n\n(mpg\n  .head(5)\n  .filter([\"manufacturer\", \"model\",\"year\", \"hwy\"])\n)\n\n\nDeliverables:\n\n\n\n\n\n\nNote\n\n\n\nDeliverables are “the quantifiable goods or services that must be provided upon the completion of a project”. In this class the deliverable for each project is a HTML report created using Quarto. This final section will be the same for each project.\n\n\nUse this template to submit your Client Report. The template has two sections:\n\nA short elevator pitch that highlights key values or metrics from the results. Describing these key insights to interest or hook the reader to want to read more about your work. The writing style should be more technical with some creative elements. Do not summarize what you did.\nAnswers to the questions | tasks. Each answer should include a written description of your results, code cells with comments, charts, and/or tables.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Projects/project_0.html",
    "href": "Projects/project_0.html",
    "title": "Project 0: Introduction",
    "section": "",
    "text": "Walkthrough\n\n\n\nBackground\n\n\n\n\n\n\nNote\n\n\n\n\n\nWe will complete six projects during the semester that each take about two weeks (four days of class). On average, a student will spend 2 hours outside of class per hour in class to complete the assigned readings, submit any Canvas items, and complete the project (for a total of 8 hours per project). The instruction for each project will be structured into sections as written on this page.\nThis first Background section provides context for the project. Make sure you read the background carefully to see the big picture needs and purpose of the project.\n\n\n\nPython and VS Code are tools commonly used in the field of data science. During our first two days of class we will get VS Code prepped for data science programming. Completing Project 0 will set you up for success the rest of the semester.\n\n\nData\n\n\n\n\n\n\nNote\n\n\n\n\n\nEvery data science project should start with data, and our class projects are no different. Each project will have ‘Download’ and ‘Information’ links like the ones below.\n\n\n\nDownload: mpg data\nInformation: Data description\n\n\nReadings\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe Readings section will contain links to reading assignments that are required for each project, as well as optional references. Remember that you are reading this material to build skills. Take the time to comprehend the readings and the skills contained within.\nWe recommend reading through the assigned material once for a general understanding before the first day of each project. You will reread and reference the material multiple times as you complete the project.\n\n\n\nThe readings listed below are required for the first two days of class.\n\nCourse Setup\nLearn about our Book Python for Data Science\nPY4DS: First Steps\nPY4DS: CH1 Whole Game\nQuarto Instructional Template for DS \n\n\nOptional References\n\nLearn about VS Code\nLearn about Plotly Express\nVS Code user interface\nReading Technical Documentation\n\n\n\n\nQuestions and Tasks\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis section lists the questions and tasks that need to be completed for the project. Your work on the project must be compiled into a report, pushed to GitHub and a URL submitted in Canvas by the weekend following the last day of material for the project.\n\n\n\nIn the DS 250 folder of the Course Work Portfolio, edit the Project0.qmd quarto file to complete the following:\n\nFinish the readings, setup, and be prepared with any questions to get your environment working smoothly (class for on-campus and Slack for online)\n\nIn VS Code, create the example chart from section 3.2.2 of the textbook. (Note: you will be using Plotly Express to display the chart instead of Altair which is used in the book)\n\nYour final report should also include the table created from the following (assuming you have mpg from question 2)\n\n\n(mpg\n  .head(5)\n  .filter([\"manufacturer\", \"model\",\"year\", \"hwy\"])\n)\n\nYour portfolio should include your updated resume in markdown language as part of the Git and Github Setup using the resume.qmd file in the portfolio\n\n\n\nDeliverables:\n\n\n\n\n\n\nNote\n\n\n\n\n\nDeliverables are “the quantifiable goods or services that must be provided upon the completion of a project”. In this class the deliverable for each project is a GitHub published report created using Quarto files. This final section will be the same for each project.\n\n\n\nUse this template to submit your Client Report. The template has two sections:\n\nA short elevator pitch that highlights key values or metrics from the results. Describing these key insights to interest or hook the reader to want to read more about your work. The writing style should be more technical with some creative elements. Do not summarize what you did.\n\nAnswers to the questions | tasks. Each answer should include a written description of your results, code cells with comments, charts, and/or tables.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nYour report should be written in quarto markdown files and pushed to GitHub which will render it to HTML. Submit a URL of the rendered project in Canvas. (Do not submit the URL to the GitHub .qmd file)\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Projects",
      "Project 0: Introduction"
    ]
  },
  {
    "objectID": "Projects/project_2.html",
    "href": "Projects/project_2.html",
    "title": "Project 2: Late Flights & Missing Data (JSON)",
    "section": "",
    "text": "Walkthrough\n\n\n\nBackground\n\n\n\n\n\n\nNote\n\n\n\n\n\nWe will complete six projects during the semester that each take about two weeks (four days of class). On average, a student will spend 2 hours outside of class per hour in class to complete the assigned readings, submit any Canvas items, and complete the project (for a total of 8 hours per project). The instruction for each project will be structured into sections as written on this page.\nThis first Background section provides context for the project. Make sure you read the background carefully to see the big picture needs and purpose of the project.\n\n\n\nDelayed flights are not something most people look forward to. In the best case scenario you may only wait a few extra minutes for the plane to be cleaned. However, those few minutes can stretch into hours if a mechanical issue is discovered or a storm develops. Arriving hours late may result in you missing a connecting flight, job interview, or your best friend’s wedding.\nIn 2003 the Bureau of Transportation Statistics (BTS) began collecting data on the causes of delayed flights. The categories they use are Air Carrier, National Aviation System, Weather, Late-Arriving Aircraft, and Security. You can visit the BTS website to read definitions of these categories.\n\n\nClient Request\nThe JSON file for this project contains information on delays at 7 airports over 10 years. Your task is to clean the data, search for insights about flight delays, and communicate your results to the Client. The Client is a CEO of a flight booking app who is interested in the causes of flight delays and wants to know which airports have the worst delays. They also want to know the best month to fly if you want to avoid delays of any length.\n\n\nData\n\n\n\n\n\n\nNote\n\n\n\n\n\nEvery data science project should start with data, and our class projects are no different. Each project will have ‘Download’ and ‘Information’ links like the ones below.\n\n\n\nDownload: JSON File\nInformation: Data Description\nSubject Matter: Types of Delay\n\n\nReadings\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe Readings section will contain links to reading assignments that are required for each project, as well as optional references. Remember that you are reading this material to build skills. Take the time to comprehend the readings and the skills contained within.\nWe recommend reading through the assigned material once for a general understanding before the first day of each project. You will reread and reference the material multiple times as you complete the project.\n\n\n\n\nP4DS: CH4 Data Transformation\nP4DS: CH6 Tidy Data\nP4DS: CH11 Visualization\nP4DS: CH12 Layers\nP4DS: CH13 Exploratory Data Analysis\nP4DS: CH21 Missing Values\nP4DS: Ch25.3 JSON\nPython Data Science Handbook: Missing Data\nHandling Missing Data\nWikipedia Missing Data\n\n\nOptional References\n\nisin method\nwhere method\nnp.where method\nreplace method\nAn introduction to JSON (May need to open in ingognito to read.)\nThe key word in ‘Data Science’ is not Data…\nHow to Handle Missing Data (May need to open in ingognito to read.)\nLambda Function\n\n\n\n\nQuestions and Tasks (Core)\n\n\n\n\n\n\nNote\n\n\n\n\n\nThis section lists the questions and tasks that need to be completed for the project. Your work on the project must be compiled into a report, pushed to GitHub and a URL submitted in Canvas by the weekend following the last day of material for the project.\nThere are two types of questions: Core and Stretch. Core questions are required for each project. The course syllabus competencies requires specic a number of projects having all the Stretch questions achived based on your goals for the grade level you are seeking.\n\n\n\n\nFix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”). In your report include one record example (one row) from your new data, in the raw JSON format. Your example should display the “NaN” for at least one missing value.__\nWhich airport has the worst delays? Describe the metric you chose, and why you chose it to determine the “worst” airport. Your answer should include a summary table that lists (for each airport) the total number of flights, total number of delayed flights, proportion of delayed flights, and average delay time in hours.\nWhat is the best month to fly if you want to avoid delays of any length? Describe the metric you chose and why you chose it to calculate your answer. Include one chart to help support your answer, with the x-axis ordered by month. (To answer this question, you will need to remove any rows that are missing the Month variable.)\nAccording to the BTS website, the “Weather” category only accounts for severe weather delays. Mild weather delays are not counted in the “Weather” category, but are actually included in both the “NAS” and “Late-Arriving Aircraft” categories. Your job is to create a new column that calculates the total number of flights delayed by weather (both severe and mild). You will need to replace all the missing values in the Late Aircraft variable with the mean. Show your work by printing the first 5 rows of data in a table. Use these three rules for your calculations:\n\n100% of delayed flights in the Weather category are due to weather\n\n30% of all delayed flights in the Late-Arriving category are due to weather\n\nFrom April to August, 40% of delayed flights in the NAS category are due to weather. The rest of the months, the proportion rises to 65%\n\nUsing the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Describe what you learn from this graph.\n\n\n\nQuestions and Tasks (Stretch)\nHere is an example Stretch question(s) for this project. Your instructor may assign different Stretch question(s). You must comment in Canvas when submitting your project if you completed any of the Stretch questions.\n\nWhich delay is the worst delay? Create a similar analysis as above for Weahter Delay with: Carrier Delay and Security Delay. Compare the proportion of delay for each of the three categories in a Chart and a Table. Describe your results.\n\n\n\nDeliverables\nUse this template to submit your Client Report. The template has two sections:\n\nA short elevator pitch that highlights key values or metrics from the results. Describing these key insights to interest or hook the reader to want to read more about your work. The writing style should be more technical with some creative elements. Do not summarize what you did.\n\nAnswers to the questions | tasks. Each answer should include a written description of your results, code cells with comments, charts, and/or tables.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nYour report should be written in quarto markdown files and pushed to GitHub which will render it to HTML. Submit a URL of the rendered project in Canvas. (Do not submit the URL to the GitHub .qmd file)\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Projects",
      "Project 2: Late Flights and Missing Data (JSON)"
    ]
  },
  {
    "objectID": "Projects/project_4.html",
    "href": "Projects/project_4.html",
    "title": "Project 4: Can You Predict That?",
    "section": "",
    "text": "Walkthrough\n\n\n\nBackground\nThe clean air act of 1970 was the beginning of the end for the use of asbestos in home building. By 1976, the U.S. Environmental Protection Agency (EPA) was given authority to restrict the use of asbestos in paint. Homes built during and before this period are known to have materials with asbestos YOu can read more about this ban.\nThe state of Colorado has a large portion of their residential dwelling data that is missing the year built and they would like you to build a predictive model that can classify if a house is built pre 1980.\nColorado gave you home sales data for the city of Denver from 2013 on which to train your model. They said all the column names should be descriptive enough for your modeling and that they would like you to use the latest machine learning methods.\n\n\nClient Request\nThe Client is a state agency in Colorado that is responsible for the health and safety of its residents. They have a large portion of their residential dwelling data that is missing the year built and they would like you to build a predictive model that can classify if a house is built pre 1980.\n\n\nData\nDownload: dwellings_ml.csv (ml ready)\nOptional Data: dwellings_neighborhoods_ml.csv (ml ready)\nInformational Data: dwellings_denver.csv (not cleansed)\nInformation: Data description\n\n\nReadings\n\nMachine Learning Introduction\nA visual introduction to machine learning\nP4DS: CH22 Joins\nHow to choose a good evaluation metric for your Machine learning model\n\n\nOptional References\n\nDecision Tree Classification in Python\n\nBoosted algorithms in scikit-learn\nscikit-plot package\n\n\n\n\nQuestions and Tasks (Core)\n\nCreate 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.\nBuild a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.\nJustify your classification model by discussing the most important features selected by your model. This discussion should include a feature importance chart and a description of the features.\nDescribe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.\n\n\n\nQuestions and Tasks (Stretch)\nHere is an example Stretch question(s) for this project. Your instructor may assign different Stretch question(s). You must comment in Canvas when submitting your project if you completed any of the Stretch questions.\n\nRepeat the classification model using 3 different algorithms. Display their Feature Importance, and Decision Matrix. Explian the differences between the models and which one you would recommend to the Client.\nJoin the dwellings_neighborhoods_ml.csv data to the dwelling_ml.csv on the parcel column to create a new dataset. Duplicate the code for the stretch question above and update it to use this data. Explain the differences and if this changes the model you recomend to the Client.\nCan you build a model that predicts the year a house was built? Explain the model and the evaluation metrics you would use to determine if the model is good.\n\n\n\nDeliverables\nUse this template to submit your Client Report. The template has two sections:\n\nA short elevator pitch that highlights key values or metrics from the results. Describing these key insights to interest or hook the reader to want to read more about your work. The writing style should be more technical with some creative elements. Do not summarize what you did.\n\nAnswers to the questions | tasks. Each answer should include a written description of your results, code cells with comments, charts, and/or tables.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nYour report should be written in quarto markdown files and pushed to GitHub which will render it to HTML. Submit a URL of the rendered project in Canvas. (Do not submit the URL to the GitHub .qmd file)\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Projects",
      "Project 4: Can You Predict That?"
    ]
  },
  {
    "objectID": "Projects/project_6.html",
    "href": "Projects/project_6.html",
    "title": "Project 6: Git Your DS Portfolio Online",
    "section": "",
    "text": "Walkthrough\n\n\n\nBackground\nGitHub is an online platform where data scientists and developers can communicate and share work. It has also morphed into a tool to house all your work in a portfolio. Think about an Art student and how they have to develop their portfolio of various skills they have across the art classes. Similarly you will want to showcase you skillset across the Data Science skillsets.\nAs students, you will want to curate your creative work on GitHub using a program called Git. GitHub is the place to share your original work, not your homework assignments. The reading assignments will dive deaper into what to include in your portfolio and what not to include.\nMany people store their personal websites, blogs, and project websites on GitHub. Our textbook and course are hosted on GitHub, and you can see J. Hathaway’s or Ryan Hafen’s personal Data Science websites that are hosted on GitHub as well.\nFor this project, you will be making a public website that is a data science portfolio that will be hosted on GitHub. Your Resume (from project 0) will be one section of your portfolio/website. You should also post Data Science Society projects, personal projects, and any other data science related work you have done outside of class.\n\n\nData\nPortfolio: BYUI Data Science Portfolio\n\n\nReadings\n\nGit and GitHub for DS\nPull and Merge Forks on GitHub\nNew to Git and GitHub? This Essential Beginners Guide is for you\nGit vs. GitHub: What is the difference between them?\nUsing Version Control in VS Code\nGit in Visual Studio Code video\n\n\n\nPortfolio Resources\n\nHow to Modify a Quarto Website\nHow to Create a Compelling GitHub Portfolio\nHow to Create a Professional Portfolio on GitHub\nData Science Portfolios That Will Get You the Job\n4 Data Science Portfolio Projects You Need to Create\nExample 1 - Data Science Portfolio\nExample 2 - Data Science Portfolio\n\n\n\nQuestions and Tasks\n\nGit a Data Science Portfolio in GitHub (main page)\n\nUse the Portfolio Template on your Githhub root directory\n\nNavigate to the Data Science Portfolio repo in GitHub.\nClick the Green Button Use this template and select Create a new repository\n\nClick include all branches checkbox, this will include the gh-pages branch\n\nSelect yourself as the Owner\n\nName the repository as username.github.io where the username is your username on GitHub (Note: If the username part of the repository doesn’t exactly match your username, it won’t work, so make sure to get it right.)\nClick the Green Button Create repository\n\n\nCreate a new branch gh-pages if you forgot to check the include all branches box (skip otherwise)\n\nClick the Branch: main button then view all branches\n\nClick the New Branch button\n\nName the branch gh-pages and click the Green Button Create new branch\n\n\nModify Pages Settings for Build and deployment from main to gh-pages:\n\nClick the Settings tab\n\nScroll down to the Pages section in the left hand menu\n\nLocate the Build and deployment section and change Branch from main to gh-pages and leave the right side as /root\n\n\nClone the repository to your computer\n\nClick the &lt;&gt; Code menu\nClick the Green Button &lt;&gt; Code and select Open with GitHub Desktop\n\nClick the Button Open in Visual Studio Code \n\nUpdate the _quarto.yml file\n\nChange the title to your name\nChange the repo-rul to a brief description of your portfolio\nChange the page-footer left: to your name\nChange the page footer href: to your LinkedIn profile link\nScroll to the bottom and change the theme light: and/or dark: to another theme (optional)\n\nPush the changes to GitHub via GitHub Desktop\n\nMake sure your current repo in the top left is username.github.io\n\nType a commit message and click the Blue Button Commit to main\n\nClick the Blue Button Push origin\n\n\nConfirm the GitHub Actions are working\n\nNavigate to the repo in GitHub and click on the Actions tab\n\nConfirm the Update _quarto.yml is working by the yellow circle turning to a green check circle (Note: this can take 3-5min)\n\n\nFix the main page loading the ReadMe.md file\n\nRun quarto publish gh-pages in the terminal of VS Code\n\n\nGit your Resume in your Portfolio\n\nMove your resume from Project 0 into the Portfolio by replacing the resume.qmd file\nPush your results to GitHub with GitHub Desktop.\n\n\n\n\nDeliverables:\n\nComplete the questions\n\nSubmit a URL link to your portfolio.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Projects",
      "Project 6: Git Your Portfolio Online"
    ]
  },
  {
    "objectID": "Setup/Archive/git_github_setup_v0.html",
    "href": "Setup/Archive/git_github_setup_v0.html",
    "title": "Git and GitHub for DS",
    "section": "",
    "text": "Git is a distributed version control tool that can manage a development project’s source code history, while GitHub is a cloud based platform built around the Git tool. Git is a tool a developer installs locally on their computer, while GitHub is an online service that stores code pushed to it from computers running the Git tool. The key difference between Git and GitHub is that Git is an open-source tool developers install locally to manage source code, while GitHub is an online service to which developers who use Git can connect and upload or download resources.1"
  },
  {
    "objectID": "Setup/Archive/git_github_setup_v0.html#footnotes",
    "href": "Setup/Archive/git_github_setup_v0.html#footnotes",
    "title": "Git and GitHub for DS",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.theserverside.com/video/Git-vs-GitHub-What-is-the-difference-between-them#:~:text=The%20key%20difference%20between%20Git,and%20upload%20or%20download%20resources.↩︎\nhttps://www.theserverside.com/video/Git-vs-GitHub-What-is-the-difference-between-them#:~:text=The%20key%20difference%20between%20Git,and%20upload%20or%20download%20resources.↩︎\nhttps://www.howtogeek.com/180167/htg-explains-what-is-github-and-what-do-geeks-use-it-for/↩︎\nhttps://stackoverflow.com/questions/29971624/visual-studio-code-cannot-detect-installed-git↩︎"
  },
  {
    "objectID": "Setup/aws_setup.html",
    "href": "Setup/aws_setup.html",
    "title": "AWS Virtual Machine Setup",
    "section": "",
    "text": "Prerequisites\nIf you want to use a AWS Academy VM for this course, you will need to have an AWS Educate account. Please email the admin for with the subject line “AWS Educate Account Request” and include your full name and course code. You will receive an email from AWS Educate with instructions on how to create an account.\n\n\nAWS Virtual Machine Setup\nWatch and follow the two videos, and or follow along with the AWS Virtual Machine Creation Document. Copy paste code from the cell below.\n\n\n\n\nSetup AWS Documentation\n\nAWS Virtual Machine Creation\n\n\n\nCopy Paste Code Block\ngit clone https://github.com/byui-bwh/db-workstation-automation.git \n\ncd db-workstation-automation/\n\n. ./provision_vm.sh\n\n\nClone this GitHub Repository\ncd Desktop\ngit clone https://github.com/NicholasBoss/clarkstudents24.git\n\n\nInstalling GitHub Desktop\n\nOpen the Repo just cloned in Visual Studio Code\nNavigate through code to week 2 and open the root.py file\nRun the code in the root.py file by pressing the play button in the top right corner of the file\n\nWhen asked to run the GitHub desktop script answer y\nWhen asked to run the file setup script answer n\nClose the file\n\n\n\n\nInstalling Quarto CLI\n\nNavigete to the Quarto CLI website for Linux\n\nRun all the termainal commands on the site in the terminal in the AWS instance\n\n\n\n\nInstalling the Quarto VS Code Extension\n\nOpen Visual Studio Code\nClick on the Extensions icon in the left side bar\nSearch for Quarto in the search bar\nClick the Install button on the Quarto extension\nClose the Extensions tab\n\n\n\nInstall Python Libraries\n\nOpen the terminal in Visual Studio Code\nRun the following command to install the required Python libraries\n\npip install numpy pandas scikit-learn plotly.express nbformat nbclient pyyaml jupyter\n\n\nInstall Other VS Code Extensions\n\nJupyter\nLive Share\nPython\nPylance\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Setup",
      "AWS VM (Optional)"
    ]
  },
  {
    "objectID": "Setup/git_github_setup.html",
    "href": "Setup/git_github_setup.html",
    "title": "Git and GitHub for DS",
    "section": "",
    "text": "Note: Changes have been made to this course since the video was recorded. The video is still relevant and will be updated in the future to reflect the changes. For conflicts of the video and the text, the text is the most current.\n\n\n\nSetup Git and GitHub\n\nGitHub will be used for publishing all projects\n\nInstall git on your computer\n\nWindows Installation Note: Keep all settings as default during installation\nMac Installation\nNote: Mac users can install git using homebrew by running brew install git in the terminal. You will need to install homebrew first if you don’t have it\n\nCreate a GitHub Student Account with your BYUI-I email.\n\nUse an appropriate username. It will be the name of your public profile and website in Project 6.\n\nInstall GitHub Desktop\n\n\n\nGit a Course Work Portfolio in GitHub\n\nUse the Portfolio Template in Data Science GitHub repo\n\nNavigate to the Course Work Portfolio in GitHub\n\nClick the Green Button Use this template and select Create a new repository\n\n\nSelect byui-math-dept as the Owner You should have been added to byui-math-dept by your teacher if you dont see it ask them to add you. This Org uses SSO with BYUI, if you dont have your BYUI email in your GitHub account you need to add it (account -&gt; settings -&gt; emails -&gt; add email address)\n\n\nName the repository as your GitHub username + _ + your first semester and yearall lowercase (see example in image above)\n\nSelect Private as the type of Repo, then click Create Repository\n\n\nModify Pages Settings for Build and deployment to be main and /docs:\n\nClick the Settings tab\n\n\nScroll down to the Pages section in the left hand menu\n\n\nLocate the Build and deployment section and make sure the Source is Deploy from a branch, the Branch is main, and the folder is /docs. Dont forget to click Save\n\n\nClone the repository to your computer\n\nClick the &lt;&gt; Code menu\nClick the Green Button &lt;&gt; Code and select Open with GitHub Desktop\n\n\nClick the Button Open in Visual Studio Code \n\nIf it asks for a username and password, this is because your GitHub Desktop is not logged in to your GitHub account via SSO. Log out of your account in GitHub Desktop, be logged in in your browser to GitHub and make sure you can access the byui-math-dept org where you cloned your new portfolio. Then repete these instructions. It will log you back in to GitHub Desktop but this with with the SSO credentials\n\nUpdate the _quarto.yml file\n\nChange the title to your name\nChange the repo-rul to a brief description of your portfolio\nChange the page-footer left: to your name\nChange the page footer href: to your LinkedIn profile link\nScroll to the bottom and change the theme light: and/or dark: to another theme (optional)\n\nQuarto Render\n\nOpen the terminal in VS Code\nRun quarto render to build the site\n\nPush the changes to GitHub via GitHub Desktop\n\nMake sure you have the correct repo selected in the top left\n\n\nType a commit message and click the Blue Button Commit to main\n\n\nClick the Blue Button Push origin\n\n\n\nGit your Resume in your Portfolio (Learn Markdown)\n\nEdit the resume.qmd file in the Course Work Portfolio use markdown to to populate your resume\n\nPush your results to GitHub with GitHub Desktop.\n\n\n\n\nGitHub Portfolio Conversion to use Docs\n\nConvert your Course Work Portfolio to use Docs\n\nConvert your Portfolio to use the docs folder for the site\n\nCreate a new folder in your repository called docs\n\nUpdate the _quarto.yml file to point to the docs folder for the site\n\nAdd this code to line 3 output-dir: docs\n\n\nModify Pages Settings for Build and deployment to be main and /docs:\n\nClick the Settings tab\n\n\nScroll down to the Pages section in the left hand menu\n\n\nLocate the Build and deployment section and make sure the Source is Deploy from a branch, the Branch is main, and the folder is /docs. Dont forget to click Save\n\n\nQuarto Render\n\nOpen the terminal in VS Code\nRun quarto render to build the site\n\nPush the changes to GitHub via GitHub Desktop\n\nMake sure you have the correct repo selected in the top left\n\n\nType a commit message and click the Blue Button Commit to main\n\n\nClick the Blue Button Push origin\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Setup",
      "Git and Github"
    ]
  },
  {
    "objectID": "Setup/python_lib.html",
    "href": "Setup/python_lib.html",
    "title": "Python for Data Science",
    "section": "",
    "text": "Install Python Libraries\n\n\n\nInstalling and Importing Packages\nThe Apple Silicon is still more difficult to get installed. You can use the following links to get it installed - Link 1, Link 2, Link 3.\nWe can get packages installed for this course using one of the two methods below.\n\nUsing your interactive Python (Jupyter server)\nThis is the preferred install method for both PC and Mac:\n#%%\n# copy paste this into a python file in vs code and run this cell\nimport sys\n!{sys.executable} -m pip install numpy pandas scikit-learn plotly.express nbformat nbclient pyyaml\n\n\nUsing your terminal (alternative method)\n# default way\npip install numpy pandas scikit-learn plotly.express nbformat nbclient pyyaml\n\n\n\n\nLearn More about the Packages you Installed\nWe want to install the following three packages;\n\npandas\nnumpy\nplotly express\nscikit-learn.\n\n\nLearn More About Python\nPython\n\n\nContinue to Install VS Code\nInstall VS Code\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Setup",
      "Python Libraries"
    ]
  },
  {
    "objectID": "Setup/quarto_setup.html",
    "href": "Setup/quarto_setup.html",
    "title": "Quarto for Data Science",
    "section": "",
    "text": "Quarto\n\nQuarto is an open-source scientific and technical publishing system built on Pandoc. You can create dynamic content with Python, R, Julia, and Observable.\nWe use this perfect union of Jupyter Notebooks and RMarkdown for reporting on our projects. It leverages Markdown and Python code chunks to create dynamic HTML content.\n\n\nMarkdown\nMarkdown is a plain text formatting syntax aimed at making writing more accessible. The philosophy behind Markdown is that plain text documents should be readable without tags making a mess, but there should still be ways to add text modifiers like lists, bold, italics, etc. It is an alternative to WYSIWYG (what you see is what you get) editors, which use rich text that later gets converted to proper HTML.\n\n\nQuarto Basics\nYou will need to install the Quarto CLI and then go through the VS Code directions on using Quarto with Python.\n\nInstall Quarto CLI \nDownload the class instructional template. Open it in VS Code and press the Preview button. It should produce a HTML file with a Plotly Express Chart and a data table. (If it errors, it may be missing some libraries. Here is the code to install them.)\n\n\n\nIf you still can not Preview your .qmd template file. Run quarto check in your Terminal section of VS Code and copy paste the ouput in a DM to your teacher or TA.\n\n\n\n\nQuarto Preview Tip\nWhen clicking on the Preview Icon  in the top right of your .qmd file, some students experience the preview rendering their entire course website. If this is the case, you can fix it by only opening the project.qmd file you are working on in VS-Code instead of opening the entire course folder.\n\nContinue to Git and GitHub\nInstall Git and GitHub\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Setup",
      "Quarto"
    ]
  },
  {
    "objectID": "Setup/sql_setup.html",
    "href": "Setup/sql_setup.html",
    "title": "SQL for Data Science",
    "section": "",
    "text": "SQLITE Setup\nThere is nothing to download to setup SQLITE This SQLite Viewer VS Code extension will be helpful to explore the database\n\n\nDownloads\nDownload this sqlite db file Save it in the same place as the .py or .qmd file created in the next step\n\n\nTest Your Setup\nCopy the code below and test it in a .py file. If everything works you are all set\nimport pandas as pd \nimport numpy as np\nimport sqlite3\n\n# %%\n# careful to list your path to the file or save it in the same place as your .qmd or .py file\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)\n\nq = 'SELECT * FROM allstarfull LIMIT 5'\nresults = pd.read_sql_query(q,con)\n\nresults\nYou can see the list of tables available in the database\nq = '''\n    SELECT * \n    FROM sqlite_master \n    WHERE type='table'\n    '''\ntable = pd.read_sql_query(q,con)\ntable.filter(['name'])\n\n\n\n\n Back to top",
    "crumbs": [
      "Setup",
      "SQL"
    ]
  },
  {
    "objectID": "Skill Builders/git_github.html#before-you-start",
    "href": "Skill Builders/git_github.html#before-you-start",
    "title": "GitHub and Git",
    "section": "Before you start",
    "text": "Before you start\nMake sure you have gone through the tutorial on under course materials called Git: we assume that you have a connection to your data.\n\nComplete the Hello World GitHub Guide",
    "crumbs": [
      "Skill Builders",
      "Project 6: GitHub and Git"
    ]
  },
  {
    "objectID": "Skill Builders/json_missing.html",
    "href": "Skill Builders/json_missing.html",
    "title": "JSONs & Missing",
    "section": "",
    "text": "Link to json file\n\n\n\n\nRead in the json file as a pandas dataframe. After reading in the data, you’ll want to explore it and gain some intuition. Exploring data is a very important step — the more you know about your data the better! Answer the following questions to gain some insight into this dataset.\n\nHow many rows are there?\nHow many columns?\nWhat does a row represent in this dataset?\nWhat are the different ways missing values are encoded?\nHow many np.nan in each column?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n# Object/Categorical Columns\ndf.column_name.value_counts(dropna=False)\ndf.column_name.unique()\n\n# Numeric Columns\ndf.column_name.describe()\n\n# Counting missing values\ndf.isna().sum()  # Creates boolean dataframe and sums each column\n\n\n\n\n\n\n\nAfter learning different ways our data encodes missing values, now we will neatly manage them. There are many techniques we can use to handle missing values; for example, we can drop all rows that contain a missing value, impute with mean or median, or replace missing values with a new missing category. We will use some of these techniques in this exercise.\n\nshape_reported - replace missing values with missing string.\ndistance_reported - change -999 values to np.nan. (-999 is a typical way of encoding missing values.)\ndistance_reported - fill in missing values with the mean (imputation)\nwere_you_abducted - replace - string with missing string.\n\nThe first 10 rows of your data should look like this after completion of the above steps.\n\n\n\n\n\n\n\n\n\n\n\n\ncity\nshape_reported\ndistance_reported\nwere_you_abducted\nestimated_size\n\n\n\n\n0\nIthaca\nTRIANGLE\n8521.9\nyes\n5033.9\n\n\n1\nWillingboro\nOTHER\n7438.64\nno\n5781.03\n\n\n2\nHolyoke\nOVAL\n7438.64\nno\n697203\n\n\n3\nAbilene\nDISK\n7438.64\nno\n5384.61\n\n\n4\nNew York Worlds Fair\nLIGHT\n6615.78\nmissing\n3417.58\n\n\n5\nValley City\nDISK\n7438.64\nno\n4280.1\n\n\n6\nCrater Lake\nCIRCLE\n7377.89\nno\n528289\n\n\n7\nAlma\nDISK\n7438.64\nmissing\n4772.75\n\n\n8\nEklutna\nCIGAR\n5214.95\nno\n4534.03\n\n\n9\nHubbard\nCYLINDER\n8220.34\nmissing\n4653.72\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\ndf.column_name.replace(..., ..., inplace=True)\ndf.column_name.fillna(..., inplace=True)\n\n\n\n\n\n\n\nCreate a table that contains the following summary statistics. - median estimated size by shape - mean distance reported by shape - count of reports belonging to each shape\nYour table should look like this:\n\n\n\n\n\n\n\n\n\nshape_reported\nmedian_est_size\nmean_distance_reported\ngroup_count\n\n\n\n\nCIGAR\n5899.68\n6520.21\n3\n\n\nCIRCLE\n266002\n7408.26\n2\n\n\nCYLINDER\n4550.58\n8039.49\n2\n\n\nDISK\n4581.8\n7516.39\n16\n\n\nFIREBALL\n5407.22\n7097.78\n3\n\n\nFLASH\n6108.34\n7438.64\n1\n\n\nFORMATION\n5104.4\n8708.32\n2\n\n\nLIGHT\n3850.25\n7636.09\n2\n\n\nOTHER\n4699.4\n7473.98\n4\n\n\nOVAL\n4943.63\n7787.24\n4\n\n\nRECTANGLE\n3668.1\n6054.62\n2\n\n\nSPHERE\n5076.78\n7206.55\n6\n\n\nTRIANGLE\n5033.9\n8521.9\n1\n\n\nmissing\n250153\n7438.64\n2\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n(df.groupby(...)\n     .agg(...,\n          ...,\n          ...))\n\n\n\n\n\n\n\nThe cities listed below reported their estimated size in square inches, not square feet. Create a new column named estimated_size_sqft in the dataframe, that has all the estimated sizes reported as sqft. (Hint: divide by 144 to go from sqin -&gt; sqft)\n\nHolyoke\nCrater Lake\nLos Angeles\nSan Diego\nDallas\n\nThe head of your data should look like this.\n\n\n\n\n\n\n\n\n\n\n\n\n\ncity\nshape_reported\ndistance_reported\nwere_you_abducted\nestimated_size\nestimated_size_sqft\n\n\n\n\n0\nIthaca\nTRIANGLE\n8521.9\nyes\n5033.9\n5033.9\n\n\n1\nWillingboro\nOTHER\n7438.64\nno\n5781.03\n5781.03\n\n\n2\nHolyoke\nOVAL\n7438.64\nno\n697203\n4841.69\n\n\n3\nAbilene\nDISK\n7438.64\nno\n5384.61\n5384.61\n\n\n4\nNew York Worlds Fair\nLIGHT\n6615.78\nmissing\n3417.58\n3417.58\n\n\n5\nValley City\nDISK\n7438.64\nno\n4280.1\n4280.1\n\n\n6\nCrater Lake\nCIRCLE\n7377.89\nno\n528289\n3668.68\n\n\n7\nAlma\nDISK\n7438.64\nmissing\n4772.75\n4772.75\n\n\n8\nEklutna\nCIGAR\n5214.95\nno\n4534.03\n4534.03\n\n\n9\nHubbard\nCYLINDER\n8220.34\nmissing\n4653.72\n4653.72\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nnp.where(...,  # Condition\n         ...,  # If condition is true\n         ...)  # If condition is false\n\n\n\n\n\n\n\n\n\n\nAfter you have completed this skill builder with your team (or on your own) then compare your work to our script\n\n\n\n\n\nSee the script.",
    "crumbs": [
      "Skill Builders",
      "Project 2: JSON & Missing"
    ]
  },
  {
    "objectID": "Skill Builders/json_missing.html#skill-builder",
    "href": "Skill Builders/json_missing.html#skill-builder",
    "title": "JSONs & Missing",
    "section": "",
    "text": "Link to json file\n\n\n\n\nRead in the json file as a pandas dataframe. After reading in the data, you’ll want to explore it and gain some intuition. Exploring data is a very important step — the more you know about your data the better! Answer the following questions to gain some insight into this dataset.\n\nHow many rows are there?\nHow many columns?\nWhat does a row represent in this dataset?\nWhat are the different ways missing values are encoded?\nHow many np.nan in each column?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n# Object/Categorical Columns\ndf.column_name.value_counts(dropna=False)\ndf.column_name.unique()\n\n# Numeric Columns\ndf.column_name.describe()\n\n# Counting missing values\ndf.isna().sum()  # Creates boolean dataframe and sums each column\n\n\n\n\n\n\n\nAfter learning different ways our data encodes missing values, now we will neatly manage them. There are many techniques we can use to handle missing values; for example, we can drop all rows that contain a missing value, impute with mean or median, or replace missing values with a new missing category. We will use some of these techniques in this exercise.\n\nshape_reported - replace missing values with missing string.\ndistance_reported - change -999 values to np.nan. (-999 is a typical way of encoding missing values.)\ndistance_reported - fill in missing values with the mean (imputation)\nwere_you_abducted - replace - string with missing string.\n\nThe first 10 rows of your data should look like this after completion of the above steps.\n\n\n\n\n\n\n\n\n\n\n\n\ncity\nshape_reported\ndistance_reported\nwere_you_abducted\nestimated_size\n\n\n\n\n0\nIthaca\nTRIANGLE\n8521.9\nyes\n5033.9\n\n\n1\nWillingboro\nOTHER\n7438.64\nno\n5781.03\n\n\n2\nHolyoke\nOVAL\n7438.64\nno\n697203\n\n\n3\nAbilene\nDISK\n7438.64\nno\n5384.61\n\n\n4\nNew York Worlds Fair\nLIGHT\n6615.78\nmissing\n3417.58\n\n\n5\nValley City\nDISK\n7438.64\nno\n4280.1\n\n\n6\nCrater Lake\nCIRCLE\n7377.89\nno\n528289\n\n\n7\nAlma\nDISK\n7438.64\nmissing\n4772.75\n\n\n8\nEklutna\nCIGAR\n5214.95\nno\n4534.03\n\n\n9\nHubbard\nCYLINDER\n8220.34\nmissing\n4653.72\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\ndf.column_name.replace(..., ..., inplace=True)\ndf.column_name.fillna(..., inplace=True)\n\n\n\n\n\n\n\nCreate a table that contains the following summary statistics. - median estimated size by shape - mean distance reported by shape - count of reports belonging to each shape\nYour table should look like this:\n\n\n\n\n\n\n\n\n\nshape_reported\nmedian_est_size\nmean_distance_reported\ngroup_count\n\n\n\n\nCIGAR\n5899.68\n6520.21\n3\n\n\nCIRCLE\n266002\n7408.26\n2\n\n\nCYLINDER\n4550.58\n8039.49\n2\n\n\nDISK\n4581.8\n7516.39\n16\n\n\nFIREBALL\n5407.22\n7097.78\n3\n\n\nFLASH\n6108.34\n7438.64\n1\n\n\nFORMATION\n5104.4\n8708.32\n2\n\n\nLIGHT\n3850.25\n7636.09\n2\n\n\nOTHER\n4699.4\n7473.98\n4\n\n\nOVAL\n4943.63\n7787.24\n4\n\n\nRECTANGLE\n3668.1\n6054.62\n2\n\n\nSPHERE\n5076.78\n7206.55\n6\n\n\nTRIANGLE\n5033.9\n8521.9\n1\n\n\nmissing\n250153\n7438.64\n2\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n(df.groupby(...)\n     .agg(...,\n          ...,\n          ...))\n\n\n\n\n\n\n\nThe cities listed below reported their estimated size in square inches, not square feet. Create a new column named estimated_size_sqft in the dataframe, that has all the estimated sizes reported as sqft. (Hint: divide by 144 to go from sqin -&gt; sqft)\n\nHolyoke\nCrater Lake\nLos Angeles\nSan Diego\nDallas\n\nThe head of your data should look like this.\n\n\n\n\n\n\n\n\n\n\n\n\n\ncity\nshape_reported\ndistance_reported\nwere_you_abducted\nestimated_size\nestimated_size_sqft\n\n\n\n\n0\nIthaca\nTRIANGLE\n8521.9\nyes\n5033.9\n5033.9\n\n\n1\nWillingboro\nOTHER\n7438.64\nno\n5781.03\n5781.03\n\n\n2\nHolyoke\nOVAL\n7438.64\nno\n697203\n4841.69\n\n\n3\nAbilene\nDISK\n7438.64\nno\n5384.61\n5384.61\n\n\n4\nNew York Worlds Fair\nLIGHT\n6615.78\nmissing\n3417.58\n3417.58\n\n\n5\nValley City\nDISK\n7438.64\nno\n4280.1\n4280.1\n\n\n6\nCrater Lake\nCIRCLE\n7377.89\nno\n528289\n3668.68\n\n\n7\nAlma\nDISK\n7438.64\nmissing\n4772.75\n4772.75\n\n\n8\nEklutna\nCIGAR\n5214.95\nno\n4534.03\n4534.03\n\n\n9\nHubbard\nCYLINDER\n8220.34\nmissing\n4653.72\n4653.72\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nnp.where(...,  # Condition\n         ...,  # If condition is true\n         ...)  # If condition is false\n\n\n\n\n\n\n\n\n\n\nAfter you have completed this skill builder with your team (or on your own) then compare your work to our script\n\n\n\n\n\nSee the script.",
    "crumbs": [
      "Skill Builders",
      "Project 2: JSON & Missing"
    ]
  },
  {
    "objectID": "Skill Builders/munging.html#data",
    "href": "Skill Builders/munging.html#data",
    "title": "Munging Data",
    "section": "Data",
    "text": "Data\nLink to the data",
    "crumbs": [
      "Skill Builders",
      "Project 5: Munging Data"
    ]
  },
  {
    "objectID": "Skill Builders/munging.html#intro-to-cleaning-movies-data",
    "href": "Skill Builders/munging.html#intro-to-cleaning-movies-data",
    "title": "Munging Data",
    "section": "Intro to cleaning movies data",
    "text": "Intro to cleaning movies data\nThis skill builder focuses on munging (formatting) data into a machine learning ready dataset. We will be using an IMDB Ratings dataset. It contains columns that are categorical. Sklearn cannot handle columns that are strings, so we need to convert these into a numerical representation. We accomplish this by either one hot encoding, label encoding, or taking just one value of the range provided. There are many other ways to represent these columns as numbers, but they are beyond the scope of this course.\nOnce you’ve converted all columns to numeric, in an intelligent way, you will be asked to recreate a graph using plotly express. Here is the head of the data you will be working with. Enjoy!\n\n\n\n\n\n\n\n\n\n\n\nstar_rating\ncontent_rating\ngenre\nduration\nbox_office_rev\nmajor_hit\n\n\n\n\n9.3\nR\nCrime\n142\n€1924521976 - €1925521976\nno\n\n\n9.2\nR\nCrime\n175\n€177034987 - €178034987\nno\n\n\n9.1\nR\nCrime\n200\n€2617541398 - €2618541398\nno\n\n\n9\nPG-13\nAction\n152\n€996115723 - €997115723\nno\n\n\n8.9\nR\nCrime\n154\n€1172054364 - €1173054364\nno\n\n\n\n\n\nExercise 1\n\nGrab the high range value for each movie and put it into a new column called high_range_rev.\n\nMake sure the data type of this new column is numeric!!\n\nRemove the box_office_rev column from the dataset.\n\nThe .str.split() and .astype() methods might be of use! Also, to get the euro sign just copy it from here, €, and put it in your code.\nThe first 5 rows of the resulting dataframe should look like this\n\n\n\n\n\n\n\n\n\n\n\nstar_rating\ncontent_rating\ngenre\nduration\nmajor_hit\nhigh_range_rev\n\n\n\n\n9.3\nR\nCrime\n142\nno\n2345444803\n\n\n9.2\nR\nCrime\n175\nno\n2182412593\n\n\n9.1\nR\nCrime\n200\nno\n1604872807\n\n\n9\nPG-13\nAction\n152\nno\n284317976\n\n\n8.9\nR\nCrime\n154\nyes\n1791932201\n\n\n\n\n\n\nExercise 2\nConvert the major_hit column to 1/0’s. yes -&gt; 1 and no -&gt; 0. Again, there are several ways to accomplish this. Using our old friend np.where is probably the easiest though.\nThe first 5 rows of the resulting dataframe should like this\n\n\n\n\n\n\n\n\n\n\n\nstar_rating\ncontent_rating\ngenre\nduration\nmajor_hit\nhigh_range_rev\n\n\n\n\n9.3\nR\nCrime\n142\n0\n1925521976\n\n\n9.2\nR\nCrime\n175\n0\n178034987\n\n\n9.1\nR\nCrime\n200\n0\n2618541398\n\n\n9\nPG-13\nAction\n152\n0\n997115723\n\n\n8.9\nR\nCrime\n154\n0\n1173054364\n\n\n\n\n\n\nExercise 3\nConvert the content_rating column using label encoding. We’re using label encoding in this case because the movie ratings already have a natural ordering to them. We will replace each rating with a number in it’s natural ascending order.\nTo be more specific, here is how we will do it.\n\nG: 0\nPG: 1\nPG-13: 2\nR: 3\n\nA dictionary and the .map() method could be useful for this exercise. There are other ways of tackling this problem though. Be creative!\nThe first 5 rows of the resulting dataframe should look like\n\n\n\n\n\n\n\n\n\n\n\nstar_rating\ncontent_rating\ngenre\nduration\nmajor_hit\nhigh_range_rev\n\n\n\n\n9.3\n3\nCrime\n142\n0\n1925521976\n\n\n9.2\n3\nCrime\n175\n0\n178034987\n\n\n9.1\n3\nCrime\n200\n0\n2618541398\n\n\n9\n2\nAction\n152\n0\n997115723\n\n\n8.9\n3\nCrime\n154\n0\n1173054364\n\n\n\n\n\n\nExercise 4\nThe last column that we need to take care of is genre. We will use one hot encoding for this. Make sure to ONLY one hot encode the genre column!\nA useful function for one hot encoding is pd.get_dummies(). I recommend checking out the documentation.\nThe resulting dataframe should look like the following example; don’t worry if your high_range_rev column turned into scientific notation—Pandas does this sometimes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstar_rating\ncontent_rating\nduration\nmajor_hit\nhigh_range_rev\ngenre_Action\ngenre_Adventure\ngenre_Animation\ngenre_Biography\ngenre_Comedy\ngenre_Crime\ngenre_Drama\ngenre_Family\ngenre_Fantasy\ngenre_Horror\ngenre_Mystery\ngenre_Sci-Fi\ngenre_Thriller\ngenre_Western\n\n\n\n\n0\n9.3\n3\n142\n0\n1.92552e+09\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n9.2\n3\n175\n0\n1.78035e+08\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n9.1\n3\n200\n0\n2.61854e+09\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n9\n2\n152\n0\n9.97116e+08\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n8.9\n3\n154\n0\n1.17305e+09\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\nExercise 5\nRecreate this graph as best you can. You’ll need to use the original data that specifies the actual rating.\n\n\n\n\n\n\n\n\nAfter you have completed this skill builder with your team (or on your own) then compare your work to our script\n\n\n\n\n\nSee the script.",
    "crumbs": [
      "Skill Builders",
      "Project 5: Munging Data"
    ]
  },
  {
    "objectID": "Skill Builders/pandas_plotly.html",
    "href": "Skill Builders/pandas_plotly.html",
    "title": "Pandas and Plotly",
    "section": "",
    "text": "For this skill builder, we are exploring some important functions in the package of pandas and plotly-express. DS programming requires a lot of data wrangling. Using the proper functions, we can create concise and comprehensive codes. You should be exposed to a few functions through the readings this week.\nYou may want to at least scan the readings before beginning this task since this serves as an assessment of your understanding of the assigned readings. This should be able to be finished within 60 minutes. You should work through it on your own or in a group based in your professors instruction.",
    "crumbs": [
      "Skill Builders",
      "Project 1: Pandas and Plotly"
    ]
  },
  {
    "objectID": "Skill Builders/pandas_plotly.html#skill-builder",
    "href": "Skill Builders/pandas_plotly.html#skill-builder",
    "title": "Pandas and Plotly",
    "section": "",
    "text": "For this skill builder, we are exploring some important functions in the package of pandas and plotly-express. DS programming requires a lot of data wrangling. Using the proper functions, we can create concise and comprehensive codes. You should be exposed to a few functions through the readings this week.\nYou may want to at least scan the readings before beginning this task since this serves as an assessment of your understanding of the assigned readings. This should be able to be finished within 60 minutes. You should work through it on your own or in a group based in your professors instruction.",
    "crumbs": [
      "Skill Builders",
      "Project 1: Pandas and Plotly"
    ]
  },
  {
    "objectID": "Skill Builders/pandas_plotly.html#data-import",
    "href": "Skill Builders/pandas_plotly.html#data-import",
    "title": "Pandas and Plotly",
    "section": "Data Import",
    "text": "Data Import\nRun the following code to import the data we need for this skill builder:\n\n# package import\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\n\n# data import\nurl = 'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/AER/Guns.csv'\ndf = pd.read_csv(url)\nMake sure the variable df is correctly assigned in your environment and finish the following exercises. You can read the documentation of the data on this page - https://vincentarelbundock.github.io/Rdatasets/doc/AER/Guns.html\n\n\nExercise 1\nOne of the first things we can do to a freshly imported data is to check its columns. This will help us understand the basic structure of the dataframe(table).\n\nUsing one line of code, select all the columns in dat, assign it to a variable called col_list.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nEvery dataframe has an attribute “columns”.\nAccessing this attribute will give you a list of all column names\n\n\n\nWe often want to know the dimension of a dataframe. How many columns are in the dataset? How many rows are in the dataset?\n\nUsing one line of code, show the number of columns and rows in df.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nEvery dataframe has an attribute “shape”.\nAccessing this attribute will give you the dimension of a datafarme\n\n\n\nNow run df.head(). It will print out the first 5 rows of data in df.\n\nJust from looking at the output, what column(s) seems to be redundant with the row number?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThere is one column that serves as nothing but a row counter, that columns is redundant.\n\n\n\n\n\n\nExercise 2\nAfter a brief investigation of the data, we will clean up the data. By cleaning up, we are trying to filter down df so this only holds data we need. We will first get rid of the extra column we found in the previous excercise.\n\nUsing one line of code, drop the redundant column using the variable col_list (created in excercise 1)\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse drop().\nUnderstand what “axis” is as a parameter of drop().\nYour function should looks like this:\ndf.drop([col_list[_]], axis = _)\nfill the “_“’s with the correct values and assign the output to df.\n\n\n\nDon’t forget to save the changes in df. Run df.head() to make sure the column is dropped in df.\n\n\n\nExercise 3\nWe have filtered df vertically by dropping a column. Now we will try to filter df horizontally, meaning we will get rid of some the rows.\nWe can do that by applying a condition to df. A condition is an expression that can be evaluated as True/False. For example, 8 &gt; 5 is an expression that evaluates to be True. This is trivial because 8 will always be greater than 5.\nRun the code below:\n\nwhat is the difference between exp1 and exp2?\n\nexp1 = 8 &gt; 5\nexp2 = df.violent &lt; 300\n\n\n\n\n\n\nHint\n\n\n\n\n\nTry type() on else variable OR calling else variable.\n\n\n\nRun ths code below:\n\nBy putting df.violent &lt; 300, and the violent column from df into a dataframe, what is the relationship between the two columns?\n\nexp = pd.DataFrame({\"df.violent &lt; 300\" : exp2,\n                    \"violent value from dat\" : df.violent})\n\nexp\n\n\n\n\n\n\nHint\n\n\n\n\n\nTry computing df.violent[n] &lt; 300 and (df.violent &lt; 300)[n] where n is less than the number of row. The two expressions will always be the same as long as n is less than the number of rows.\n\n\n\n\nUsing query()to filter down the df so that it only contains the data for idaho\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nquery() takes in expressions and filters down data.\n\n\n\nDon’t forget to save the changes in df. Run df.shape() to make sure the there are 23 rows and 13 columns.\n\n\n\nExercise 4\nBesides filtering, we can manipulate the data by adding new data to it. By adding a new column to the data, we assign a new value to each row.\n\nUsing assign(), create a new column that show the ratio between murder rate and violent rate.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nUse assign()\nYou see get the ratio by computing this code:\ndf.murder/df.violent\n\n\n\n\n\n\nExercise 5\n\nCreate a scatter plot that shows the relationship between murder rate and violent rate for the state of Idaho. Your chart should show murder rate as the x-axis, violent as the y-axis.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nCan you mimic this plot while using Plotly-Express?\nhttps://altair-viz.github.io/gallery/scatter_tooltips.html https://plotly.com/python/line-and-scatter/",
    "crumbs": [
      "Skill Builders",
      "Project 1: Pandas and Plotly"
    ]
  },
  {
    "objectID": "Skill Builders/pandas_plotly.html#because-youre-extra",
    "href": "Skill Builders/pandas_plotly.html#because-youre-extra",
    "title": "Pandas and Plotly",
    "section": "Because You’re Extra",
    "text": "Because You’re Extra\n\nExercise 6\n\nUsing a line of code, filter down the data set so that it only shows the data in years between 1993 and 1997.\n\n\n\n\nExercise 7\n\nCreate a line chart that show prisoners numbers for the state of Idaho, Utah, and Oregon.\n\nYour chart should show year as the x-axis, prisoner as the y-axis, states as different colours, along with an appropriate title.\n\n\n\nExercise 8\n\nWithout using query(), finshed the data wrangling in question 2,5 and 6.\n\n\n\n\n\n\n\n\nAfter you have completed this skill builder with your team (or on your own) then compare your work to our script\n\n\n\n\n\nSee the script.",
    "crumbs": [
      "Skill Builders",
      "Project 1: Pandas and Plotly"
    ]
  },
  {
    "objectID": "Syllabus/competency.html",
    "href": "Syllabus/competency.html",
    "title": "DS 250 Competency",
    "section": "",
    "text": "We need skills not grades! Shifting Attention\n\n\nCompetency scale\nYou must complete all competency items at the level detailed to achieve the listed grade. You can request half-step adjustments if you fall slightly short on some elements and over on others.\nYou will need to provide a detailed description in your Course Goals Letter of the items you completed to support your grade request. The course goals letter is a reflection on your efforts and the competencies they align with as well as your reflection on achieving your goals for this course.\n\nExample Course Goals Letter (End):\nHere is my assessment of my efforts and the competencie they aligh with as well as my reflection on achieving my goals for this course:\n\nProjects 29 points (B-)\nProject Stretchs 1 (C)\nMid-project checkpoints 4 (B+)\nMethods & Calculations checkpoints 4 (C+)\nDS Community 3 (A)\nCoding challenge 3 (A/B)\nThe over-under of my work is a B/B+. I am requesting a B+ for the course.\n\n\nLeader (A)Supporter (B)Listener (C)Asleep (D)\n\n\n\nLeader (A)\n\n\n\n\n\n\n\n\nElement\nRequirement\nDescription\n\n\n\n\nProjects\n34 Points\n5 points per project\n\n\nProject Stretchs\nAt least 3\n3 projects all stretches\n\n\nMid-project checkpoints\n5 completed\nFull credit (online only)\n\n\nMethods & Calculations checkpoints\n6 completed\nAll 6 @ 100% Full Points\n\n\nDS Community\nAt least 3\n–\n\n\nCourse Goal Letter (End)\nsubmission\n–\n\n\nCoding challenge\nAt least 3\nScore is out of 4\n\n\n\n\n\n\n\nSupporter (B)\n\n\n\n\n\n\n\n\nElement\nRequirement\nDescription\n\n\n\n\nProjects\n29 Points\n5 points per project\n\n\nProject Stretchs\nAt least 2\n2 projects all stretches\n\n\nMid-project checkpoints\n3 completed\nFull credit (online only)\n\n\nMethods & Calculations checkpoints\n5 completed\n5 @ 100% Full Points\n\n\nDS Community\nAt least 2\n–\n\n\nCourse Goal Letter (End)\nsubmission\n–\n\n\nCoding challenge\nAt least 3\nScore is out of 4\n\n\n\n\n\n\n\nListener (C)\n\n\n\n\n\n\n\n\nElement\nRequirement\nDescription\n\n\n\n\nProjects\n24 Points\n5 points per project\n\n\nProject Stretchs\nAt least 1\n1 projects all stretches\n\n\nMid-project checkpoints\n3 completed\nFull credit (online only)\n\n\nMethods & Calculations checkpoints\n3 completed\n3 @ 100% Full Points\n\n\nDS Community\nAt least 1\n–\n\n\nCourse Goal Letter (End)\nsubmission\n–\n\n\nCoding challenge\nAt least 2\nScore is out of 4\n\n\n\n\n\n\n\nAsleep (D)\n\n\n\n\n\n\n\n\nElement\nRequirement\nDescription\n\n\n\n\nProjects\n14 Points\n5 points per project\n\n\nProject Stretchs\nNone\n0 projects all stretches\n\n\nMid-project checkpoints\n1 completed\nFull credit (online only)\n\n\nMethods & Calculations checkpoints\n2 completed\n2 @ 100% Full Points\n\n\nDS Community\nNone\n–\n\n\nCourse Goal Letter (End)\nNone\n–\n\n\nCoding challenge\nNone\nScore is out of 4\n\n\n\n\n\n\n\n\n\n\nCompetency elements\n\nProjectsQuiz CheckpointsCheckpoints Mid-Project (online only)DS CommunityCourse Goals LetterChallenge\n\n\n\nProjects (Questions|Tasks)\nEach of the 7 projects is worth 5 points. There is a draft submission due Wednesday (W) of the 2nd week of the project and the final draft due the following Saturday (S). This gives you 2 attempts to get full points on the project. There are 6 two-week projects (P0-P5) and 1 one-week project (P6). Typically no resubmissions are allowed once the project closes in Canvas. Make the changes and take advantage of the resubmit within this window.\nProjects 1-5 will have stretch questions. You must complete all the core qeustions first before attempting the stretch questions. You must complete all the stretch questions accurately for the project to count toward the competencies. The number in the competencies relating to Project Stretches is the number of projects with all the stretch questions completed.\n\nGrading Details\n\n1 point: Submission\n3 points: Submission of a good faith attempt with a statement of work quality.\n4 points: High-quality work that addresses each of the Questions and Tasks and a comment in Canvas of your statement of work quality.\n5 points: Addressed issues and completion of resubmission (if needed).\n\n\n\n\n\n\nCheckpoints (methods and calculations)\nThese Methods and Calculation Quizzes are in Canvas and they open when the project starts. They have unlimited attempts and remain open until the end of the semester. You must get a 100% on these quizzes for them to count toward the competencies.\n\nExamples\n\nFact-Finding Questions (Calculate descriptive summaries): Fact-finding questions help you with calculations that build into the Questions and Tasks of the project. These questions have clearly defined answers using Python calculations. You should expect 2-3 problems.\n\n\nExample: Using the top 10 airports in size, what is the average size?\nExample: What proportion of flights are delayed at the largest airport?\n\n\nHow the code works questions (Explaining the tools): This part could have direct answer questions or open-ended questions.\n\n\nExample (direct): What is the recommended function for arranging your data by a variable? What are the outputs after using &lt;FUNCTION&gt;?\nExample (open): Your client has shown some confusion about NumPy’s ‘nan’ handling in Python. Help them understand by answering the question, ‘How is missing data handled in Pandas?’\n\n\n\n\n\n\nCheckpoints (Mid-project status)\nThe mid-project checkpoint has a few questions. It opens the first day of the project and closes on the first Saturday of a 2 week project. It has the following questions.\n\nExamples\n\nThroughout this project, you have worked on a code. Record a video showing your code that is no more than 1 minute. This video must include: &gt; How long have you worked on this code? &gt; What is your code designed to do? &gt; What are, if any, the issues you’re facing? &gt; What questions or tasks have you checked off?\nSubmit your 1 minute video. &gt; You may share any additional notes with your teacher using the Canvas comment feature.\n\n\n\n\n\n\nData Science Community\nTo earn credit for the DS Community element you must complete tasks from the list below. At the end of the semester, you will be asked to report on how many tasks you completed and what you learned from them. See the Competency Scale above to determine how many you need to complete based on the grade you want.\n\nAttend Data Science Society at least once.\nSign up for an email newsletter that will teach you more about data science. Data Science Weekly or Data Elixir are good options.\nListen to a podcast episode about data science. Build a Career in Data Science has some excellent episodes.\nWatch a professional presentation on YouTube about data science. Be prepared to share the link and a summary of the video.\nReach out to someone who works in a data-related field and ask them for 15 minutes of their time. Use this time to conduct an “informational interview” and learn more about their responsibilities and career path.\nResearch and apply to at least 5 data-related jobs or internships.\n\n\n\n\n\nFinishing the semester\nSubmit a Course Goals Letter (End) that includes what you have learned from this class, the next data science course you plan on taking, and the final grade that you are requesting based on the work you have submitted compared to the competencies above.\n\n\n\n\nCoding challenge\nWe will have a timed (60 min) coding challenge on the ultimate or penultimate day of class. This is not a traditional exam and is similar to the projects all semester in size and scope but is accumulative. It will cover the general techniques that we have been practicing throughout the course. You will rely on your code from the projects and the methods and calculations checkpoints to complete the challenge.\nWe expect to have a few practice challenges throughout the semester. We will score the coding challenge on a four-point scale.\n\n1 point: At least you tried.\n2 points: You have learned some items from the course, but your work in the coding challenge is deficient.\n3 points: Your submission uses proper coding techniques and addresses the objective.\n4 points: Exceptional work. Your code can be used as a solution to share with others.\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Syllabus",
      "Competency"
    ]
  },
  {
    "objectID": "Workbooks/wb0.html",
    "href": "Workbooks/wb0.html",
    "title": "Project 0 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack.\n\n\n\n\n\nCourse Setup ::: {.callout-note collapse=“true”} ## Note on the Intro Chapter in Python for DataScience\nDon’t follow the setup for pyton in the book use the Course Setup instead\nTesting running the instructional template to make sure your setup is correct :::\n\n\n\n\n\n\nLearn about Plotly Express ## Note on the Data Visualization Chapter in Python for DataScience\nWe will not be using Altair as the book does, we will use Plotly Express instead\nAll Altair chart will have a Plotly alternative :::",
    "crumbs": [
      "Workbooks",
      "Project 0"
    ]
  },
  {
    "objectID": "Workbooks/wb0.html#tutoring-lab-info",
    "href": "Workbooks/wb0.html#tutoring-lab-info",
    "title": "Project 0 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack.",
    "crumbs": [
      "Workbooks",
      "Project 0"
    ]
  },
  {
    "objectID": "Workbooks/wb0.html#setup-python-vs-code-and-quarto",
    "href": "Workbooks/wb0.html#setup-python-vs-code-and-quarto",
    "title": "Project 0 Workbook",
    "section": "",
    "text": "Course Setup ::: {.callout-note collapse=“true”} ## Note on the Intro Chapter in Python for DataScience\nDon’t follow the setup for pyton in the book use the Course Setup instead\nTesting running the instructional template to make sure your setup is correct :::",
    "crumbs": [
      "Workbooks",
      "Project 0"
    ]
  },
  {
    "objectID": "Workbooks/wb0.html#learn-about-plotly-express",
    "href": "Workbooks/wb0.html#learn-about-plotly-express",
    "title": "Project 0 Workbook",
    "section": "",
    "text": "Learn about Plotly Express ## Note on the Data Visualization Chapter in Python for DataScience\nWe will not be using Altair as the book does, we will use Plotly Express instead\nAll Altair chart will have a Plotly alternative :::",
    "crumbs": [
      "Workbooks",
      "Project 0"
    ]
  },
  {
    "objectID": "Workbooks/wb2(0).html",
    "href": "Workbooks/wb2(0).html",
    "title": "Project 2 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack."
  },
  {
    "objectID": "Workbooks/wb2(0).html#tutoring-lab-info",
    "href": "Workbooks/wb2(0).html#tutoring-lab-info",
    "title": "Project 2 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack."
  },
  {
    "objectID": "Workbooks/wb2(0).html#text-basics",
    "href": "Workbooks/wb2(0).html#text-basics",
    "title": "Project 2 Workbook",
    "section": "Text Basics",
    "text": "Text Basics\n\nHorizontal Lines\nAdd horizontal lines with either three ---, ***, or ___ But you also need blank lines above and below them\n\n\n\n\n\n\nExpand To See The Results\n\n\n\n\n\n\n\n\n\n\n\n\nHeaders\n# Level 1 Header\n## Level 2 Header\n### Level 3 Header\n#### Level 4 Header\n##### Level 5 Header\n###### Level 6 Header\nNote: only top 3 Levels of Headers will automatically generate a table of contents. Also Level 2 will automatically add a line underneath it.\n\n\n\n\n\n\nExpand To See The Results\n\n\n\n\n\nLevel 1 Header\n\nLevel 2 Header\n\nLevel 3 Header\n\nLevel 4 Header\n\nLevel 5 Header\n\nLevel 6 Header\n\n\n\n\n\n\n\n\n\n\nItalics and Bold\n_italics_ use one `_`\nyou can also use _mid_ sentence\n\n__bold__ use two `__`\nyou can also use __mid__ sentence\n\n\n\n\n\n\nExpand To See The Results\n\n\n\n\n\nitalics use one _ you can also use mid sentence\nbold use two __ you can also use mid sentence\n\n\n\n\n\nBullet Items\n- Bulleted items\n  - Indented bulleted items\n  - You can have as many as you want\n    - Really as many as you want\n      - I knew you wanted one more\n\n\n\n\n\n\nExpand To See The Results\n\n\n\n\n\n\nBulleted items\n\nIndented bulleted items\nYou can have as many as you want\n\nReally as many as you want\n\nI knew you wanted one more bullet\n\n\n\n\n\n\n\n\n\nNumbered Items\n1. Numbered items\n1. Numbered items continued\n1. Dont worry these will iterate\n1. Keep using 1. each time\n\n\n\n\n\n\nExpand To See The Results\n\n\n\n\n\n\nNumbered items\nNumbered items continued\nDont worry these will iterate\nKeep using 1. each time"
  },
  {
    "objectID": "Workbooks/wb2(0).html#level-2-header",
    "href": "Workbooks/wb2(0).html#level-2-header",
    "title": "Project 2 Workbook",
    "section": "Level 2 Header",
    "text": "Level 2 Header\n\nLevel 3 Header\n\nLevel 4 Header\n\nLevel 5 Header\n\nLevel 6 Header"
  },
  {
    "objectID": "Workbooks/wb2(0).html#pandas-dataframe-df",
    "href": "Workbooks/wb2(0).html#pandas-dataframe-df",
    "title": "Project 2 Workbook",
    "section": "Pandas DataFrame (df)",
    "text": "Pandas DataFrame (df)\n\n\n\n\n\n\nExpand To See Links to Chapter in the book Python for DataScience\n\n\n\n\n\n\nChapter on Pandas: DataFrames\n\n\n\n\nWhat is a pandas dataFrame? We can read the official documentation. I also like the video in this tutorial.\nUse the Import Packages and Load Data for the Code that follows."
  },
  {
    "objectID": "Workbooks/wb2(0).html#import-packages",
    "href": "Workbooks/wb2(0).html#import-packages",
    "title": "Project 2 Workbook",
    "section": "Import Packages",
    "text": "Import Packages\nimport `library` as `alias`\n\n\nImport Libraries\n\n#| label: libraries\n#| include: false\nimport pandas as pd\nimport altair as alt\nimport numpy as np\n\nfrom IPython.display import Markdown\nfrom IPython.display import display\nfrom tabulate import tabulate"
  },
  {
    "objectID": "Workbooks/wb2(0).html#load-data",
    "href": "Workbooks/wb2(0).html#load-data",
    "title": "Project 2 Workbook",
    "section": "Load Data",
    "text": "Load Data\ndata = pd.read_csv(`url` or `file_path`)\n\n\nLoad Data\n\n#| label: project data\n#| code-summary: Read and format project data\n# Include and execute your code here\nurl = \"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\"\ndata = pd.read_csv(url)\n\nData Frames come with attributes and built-in functions that can help us get a feel for our data.\nRun the code below one at a time (or use other functions of your choice) to explore the names data. What do you learn?\n\n\n.columns\n\ndata.columns\n\n\n\n.shape\n\ndata.shape\n\n\n\n.size\n\ndata.size\n\n\n\n.head()\n\ndata.head()\n\n\n\n.describe()\n\ndata.describe()"
  },
  {
    "objectID": "Workbooks/wb2(0).html#pandas-data-transformation",
    "href": "Workbooks/wb2(0).html#pandas-data-transformation",
    "title": "Project 2 Workbook",
    "section": "Pandas Data Transformation",
    "text": "Pandas Data Transformation\n\n\n\n\n\n\nExpand To See Links to Chapter in the book Python for DataScience\n\n\n\n\n\n\nChapter on Pandas: Transformations"
  },
  {
    "objectID": "Workbooks/wb2(0).html#prove-activity---explore-pandas-and-the-data",
    "href": "Workbooks/wb2(0).html#prove-activity---explore-pandas-and-the-data",
    "title": "Project 2 Workbook",
    "section": "Prove Activity - Explore Pandas and the Data",
    "text": "Prove Activity - Explore Pandas and the Data\n1. How many .unique() names does the names dataframe contain?  Work with a partner to find the answer. You might want to look at this pandas cheat sheet.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nPull the name column out as a series\nUse the pandas unique function pd.unique()\nFind the size of the series\n\n\n\n\n\n\n\n\n\n\nExpand To See An Example Solution\n\n\n\n\n\npd.unique(data.name)\n\n\n\n2. What is the range of years in the names dataframe?  Again, work with a partner and use the pandas cheat sheet.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\nPull the year column out as a series\nFind the max\nFind the min\n\n\n\n\n\n\n\n\n\n\nExpand To See An Example Solution\n\n\n\n\n\npd.unique(data.year).min()\npd.unique(data.year).max()\npd.unique(data.year).size\n\n\n\n3. How would you .query() for the name John? \n\n\n\n\n\n\nHint\n\n\n\n\n\n\npd.query() the name when it is equal to == John\n\n\n\n\n\n\n\n\n\n\nExpand To See An Example Solution\n\n\n\n\n\ndata.query('name == \"John\"')\npd.unique(data.query('name == \"John\"'))\n\n\n\n4. What is the range of years in the names df for the name John? \n\n\n\n\n\n\nHint\n\n\n\n\n\n\nUse your combined knowledge to .query()\nand to find the .max() and .min()\nand .size() for a particular name\n\n\n\n\n\n\n\n\n\n\nExpand To See An Example Solution\n\n\n\n\n\npd.unique(data.query('name == \"John\"').year).min()\npd.unique(data.query('name == \"John\"').year).max()\npd.unique(data.query('name == \"John\"').year).size\n\n\n\n5. How would you query for multiple names in a df? \n\n\n\n\n\n\nHint\n\n\n\n\n\n\nSimilar to what you did above for a single name\nNow one way is to use the | OR bar\nAnother way is to use in a [] List\n\n\n\n\n\n\n\n\n\n\nExpand To See An Example Solution\n\n\n\n\n\n#note you can not use pd.unique() with multiple names\ndata.query('name == \"John\" | name == \"Peter\"')\ndata.query('name in [\"John\",\"Peter\",\"Bob\"]')"
  },
  {
    "objectID": "Workbooks/wb2(0).html#size-of-chart",
    "href": "Workbooks/wb2(0).html#size-of-chart",
    "title": "Project 2 Workbook",
    "section": "Size of chart",
    "text": "Size of chart\n\n\nWidth and Height\n\n.properties(\n    width=600,\n    height=150,\n)"
  },
  {
    "objectID": "Workbooks/wb2(0).html#title-and-subtitle",
    "href": "Workbooks/wb2(0).html#title-and-subtitle",
    "title": "Project 2 Workbook",
    "section": "Title and subtitle",
    "text": "Title and subtitle\n\n\nTitle\n\n.properties(\n    title=\"Bump Chart for Stock Prices\",,\n)\n\n\n\nTitle w/ Subtitle 1\n\n.properties(\n    title={\n      \"text\": [\"First line of title\", \"Second line of title\"], \n      \"subtitle\": [\"Cool first line of subtitle\", \"Even cooler second line wow dang\"],\n      \"color\": \"red\",\n      \"subtitleColor\": \"green\"\n    }\n)\n\n\n\nTitle w/ Subtitle 2\n\ntitle=alt.Title(\n       \"Iowa's green energy boom\",\n       subtitle=\"A growing share of the state's energy has come from renewable sources\"\n   )\n\n\n\n\n\n\n\nExpand For Links to Additional Examples\n\n\n\n\n\n\nExample 1\nExample 2"
  },
  {
    "objectID": "Workbooks/wb2(0).html#size-and-color-of-bars",
    "href": "Workbooks/wb2(0).html#size-and-color-of-bars",
    "title": "Project 2 Workbook",
    "section": "Size and color of bars",
    "text": "Size and color of bars\n\n\nBar Chart Size\n\nalt.Chart(data).mark_bar(size=30)\n\n\n\nBar Chart Size and Width\n\nalt.Chart(data).mark_bar(size=30).encode(\n    x='name:O',\n    y='value:Q'\n).properties(width=200)\n\n\n\nBar Chart Size and Step\n\nalt.Chart(data).mark_bar(size=30).encode(\n    x='name:N',\n    y='value:Q'\n).properties(width=alt.Step(100))"
  },
  {
    "objectID": "Workbooks/wb2(0).html#axis-formatting",
    "href": "Workbooks/wb2(0).html#axis-formatting",
    "title": "Project 2 Workbook",
    "section": "Axis formatting",
    "text": "Axis formatting\n\n\nAxis Titles\n\naxis = alt.Axis(format = 'd', title = \"Year\")\naxis = alt.Axis(title = \"Children with Name\")\n\n\n\nAxis Scale removing Zero\n\nalt.X('Acceleration:Q').scale(zero=False)\n\n\n\nAxis Domain Sizing\n\n.scale(domain=(5, 20))"
  },
  {
    "objectID": "Workbooks/wb2(0).html#reference-marks",
    "href": "Workbooks/wb2(0).html#reference-marks",
    "title": "Project 2 Workbook",
    "section": "Reference marks",
    "text": "Reference marks\n\n\nVerticle Reference Line with Color\n\nline = alt.Chart(line_df).mark_rule(color=\"red\").encode(x = \"year\")\nchart + line\n\n\n\n\n\n\n\nExpand For Links to Additional Examples\n\n\n\n\n\n\nExample 1"
  },
  {
    "objectID": "Workbooks/wb2(0).html#reference-text",
    "href": "Workbooks/wb2(0).html#reference-text",
    "title": "Project 2 Workbook",
    "section": "Reference text",
    "text": "Reference text\n\n\nVerticle Reference Line with Text\n\ndat_label = pd.DataFrame({'x': [1976], 'y': [50000], 'label': ['October 6, 1976']})\nchart + line + text\n\n\n\n\n\n\n\nExpand For Links to Additional Examples\n\n\n\n\n\n\nExample 1\nExample 2\nExample 3"
  },
  {
    "objectID": "Workbooks/wb2(0).html#data-labels",
    "href": "Workbooks/wb2(0).html#data-labels",
    "title": "Project 2 Workbook",
    "section": "Data Labels",
    "text": "Data Labels\n\n\nData Labels\n\nsource = data.wheat()\n\nbase = alt.Chart(source).encode(\n    x='wheat',\n    y=\"year:O\",\n    text='wheat'\n)\nbase.mark_bar() + base.mark_text(align='left', dx=2)"
  },
  {
    "objectID": "Workbooks/wb2(0).html#horizontal-datum-line",
    "href": "Workbooks/wb2(0).html#horizontal-datum-line",
    "title": "Project 2 Workbook",
    "section": "Horizontal Datum Line",
    "text": "Horizontal Datum Line\n\n\nHorizontal Reference Line\n\nalt.Chart().mark_rule().encode(y=alt.datum(1))"
  },
  {
    "objectID": "Workbooks/wb2(0).html#additional-resources",
    "href": "Workbooks/wb2(0).html#additional-resources",
    "title": "Project 2 Workbook",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nLearn more about Customizing Altair Visualizations\nAltair Example Gallery"
  },
  {
    "objectID": "Workbooks/wb2(0).html#altair-videos",
    "href": "Workbooks/wb2(0).html#altair-videos",
    "title": "Project 2 Workbook",
    "section": "Altair videos",
    "text": "Altair videos\n\nWhat is Altair?\nAltair’s Visualization Grammar\nAltair’s Data Types"
  },
  {
    "objectID": "Workbooks/wb3.html",
    "href": "Workbooks/wb3.html",
    "title": "Project 3 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack.",
    "crumbs": [
      "Workbooks",
      "Project 3"
    ]
  },
  {
    "objectID": "Workbooks/wb3.html#tutoring-lab-info",
    "href": "Workbooks/wb3.html#tutoring-lab-info",
    "title": "Project 3 Workbook",
    "section": "",
    "text": "The data science lab is a resource you can use in person, online, and in Slack.",
    "crumbs": [
      "Workbooks",
      "Project 3"
    ]
  },
  {
    "objectID": "Workbooks/wb5.html",
    "href": "Workbooks/wb5.html",
    "title": "Project 5 Workbook",
    "section": "",
    "text": "Project 5 WorkBook\nUnder Construction\n\n\n\n\n Back to top",
    "crumbs": [
      "Workbooks",
      "Project 5"
    ]
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Book Time with Us",
    "section": "",
    "text": "Book an office hour slot with your teacher.  Find office hours for all the Data Science Faculty Faculty.\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DS 250: Data Science Programming",
    "section": "",
    "text": "Use the top menue to navigate to the main sections of the course\nOnce in a main section switch to navigating using the left side bar\n\n\n\nThis section contains all the needed info to get your computer setup for the course\n\n\n\nProjects contains all the course projects for the semester\n\n\n\nAdditional information on specific topics can be found here and usesd as a resourse. This can expand upon the initial setup for the course and build on that\n\n\n\nThese are under development. A workbook is designed to mirror the code in the reading from each project with links back to the book. If you learn from a more hands on approach starting with the workbook coudl be a good approach\n\n\n\nYou can find your professor, see their calendar, and book time to meet one-on-one\n\n\n\nThe syllabus and course competencies are located here. Take time to review them as they contain all the info to help you get the grade you want out of the course\n\n\n\nProfessors may use these as group coding activities in class\n\n\n\nLook here for common questions on the course"
  },
  {
    "objectID": "index.html#how-to-navigate-this-course",
    "href": "index.html#how-to-navigate-this-course",
    "title": "DS 250: Data Science Programming",
    "section": "",
    "text": "Use the top menue to navigate to the main sections of the course\nOnce in a main section switch to navigating using the left side bar\n\n\n\nThis section contains all the needed info to get your computer setup for the course\n\n\n\nProjects contains all the course projects for the semester\n\n\n\nAdditional information on specific topics can be found here and usesd as a resourse. This can expand upon the initial setup for the course and build on that\n\n\n\nThese are under development. A workbook is designed to mirror the code in the reading from each project with links back to the book. If you learn from a more hands on approach starting with the workbook coudl be a good approach\n\n\n\nYou can find your professor, see their calendar, and book time to meet one-on-one\n\n\n\nThe syllabus and course competencies are located here. Take time to review them as they contain all the info to help you get the grade you want out of the course\n\n\n\nProfessors may use these as group coding activities in class\n\n\n\nLook here for common questions on the course"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "We will be relying on a few resources for this course. You will find the pertinant readings attached to each of the projects. Those readings will be culled from:\n\nPython for Data Science: A port of R for Data Science using the Python packages pandas and Altair.\npandas User Guide\nPlotly-Express User Guide\nscikit-learn learn User Guide\nscikit-learn Tutorials\nPython Data Science Handbook\nA Whirlwind Tour of Python\nSQL\n\nWes McKinney’s pandas code for his book Python for Data Analysis is a useful reference as well: https://github.com/wesm/pydata-book\n\n\n\n Back to top",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "skill_builders.html",
    "href": "skill_builders.html",
    "title": "Skill Builders",
    "section": "",
    "text": "These short activites are provided for you to gain some additional skills to help with the class projects.\n\n\n\n Back to top",
    "crumbs": [
      "Skill Builders"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "About this site\n\n\n\n Back to top"
  }
]